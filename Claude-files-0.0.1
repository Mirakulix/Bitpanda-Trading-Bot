# FastAPI Endpoints Specification
# ai-trading-bot/api/app/routers/

"""
Complete API Endpoint Definition for AI Trading Bot
FastAPI + Pydantic Models + Security
"""

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import HTTPBearer
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
from enum import Enum

# ================================
# PYDANTIC MODELS & SCHEMAS
# ================================

class AssetType(str, Enum):
    STOCK = "stock"
    CRYPTO = "crypto"
    ETF = "etf"
    COMMODITY = "commodity"
    FOREX = "forex"

class OrderType(str, Enum):
    BUY = "buy"
    SELL = "sell"
    STOP_LOSS = "stop_loss"
    TAKE_PROFIT = "take_profit"

class OrderStatus(str, Enum):
    PENDING = "pending"
    EXECUTED = "executed"
    CANCELLED = "cancelled"
    FAILED = "failed"

class AnalysisType(str, Enum):
    FUNDAMENTAL = "fundamental"
    TECHNICAL = "technical" 
    SENTIMENT = "sentiment"
    CONSENSUS = "consensus"

# Request Models
class LoginRequest(BaseModel):
    username: str = Field(..., min_length=3, max_length=50)
    password: str = Field(..., min_length=8)

class CreateOrderRequest(BaseModel):
    symbol: str = Field(..., example="BTC")
    order_type: OrderType
    quantity: float = Field(..., gt=0)
    price: Optional[float] = None
    stop_loss: Optional[float] = None
    take_profit: Optional[float] = None

class AnalysisRequest(BaseModel):
    symbols: List[str] = Field(..., example=["BTC", "ETH", "AAPL"])
    timeframe: str = Field("1h", example="1h")
    analysis_types: List[AnalysisType] = [AnalysisType.CONSENSUS]

# Response Models
class UserProfile(BaseModel):
    user_id: str
    username: str
    email: str
    created_at: datetime
    portfolio_value: float
    total_profit_loss: float

class Position(BaseModel):
    position_id: str
    symbol: str
    asset_type: AssetType
    quantity: float
    avg_buy_price: float
    current_price: float
    unrealized_pnl: float
    created_at: datetime

class Order(BaseModel):
    order_id: str
    symbol: str
    order_type: OrderType
    quantity: float
    price: float
    status: OrderStatus
    created_at: datetime
    executed_at: Optional[datetime] = None

class AIAnalysis(BaseModel):
    analysis_id: str
    symbol: str
    analysis_type: AnalysisType
    confidence_score: float = Field(..., ge=0, le=1)
    recommendation: str  # "BUY", "SELL", "HOLD"
    reasoning: str
    key_indicators: Dict[str, Any]
    created_at: datetime

class MarketData(BaseModel):
    symbol: str
    price: float
    volume_24h: float
    change_24h: float
    change_percent_24h: float
    market_cap: Optional[float] = None
    timestamp: datetime

class PortfolioSummary(BaseModel):
    total_value: float
    total_invested: float
    total_profit_loss: float
    total_profit_loss_percent: float
    positions_count: int
    active_orders_count: int
    last_updated: datetime

# ================================
# AUTHENTICATION & SECURITY
# ================================

security = HTTPBearer()

async def get_current_user(token: str = Depends(security)):
    """JWT Token validation"""
    # Implementation will verify JWT token
    pass

# ================================
# API ROUTERS
# ================================

# Authentication Router
auth_router = APIRouter(prefix="/auth", tags=["Authentication"])

@auth_router.post("/login", response_model=Dict[str, str])
async def login(request: LoginRequest):
    """User login with JWT token generation"""
    return {"access_token": "jwt_token", "token_type": "bearer"}

@auth_router.post("/logout")
async def logout(current_user=Depends(get_current_user)):
    """User logout - invalidate token"""
    return {"message": "Successfully logged out"}

@auth_router.get("/profile", response_model=UserProfile)
async def get_profile(current_user=Depends(get_current_user)):
    """Get current user profile"""
    pass

# Portfolio Router
portfolio_router = APIRouter(prefix="/portfolio", tags=["Portfolio"])

@portfolio_router.get("/summary", response_model=PortfolioSummary)
async def get_portfolio_summary(current_user=Depends(get_current_user)):
    """Get portfolio overview with P&L"""
    pass

@portfolio_router.get("/positions", response_model=List[Position])
async def get_positions(current_user=Depends(get_current_user)):
    """Get all active positions"""
    pass

@portfolio_router.get("/positions/{symbol}", response_model=Position)
async def get_position(symbol: str, current_user=Depends(get_current_user)):
    """Get specific position by symbol"""
    pass

@portfolio_router.get("/history")
async def get_portfolio_history(
    days: int = 30,
    current_user=Depends(get_current_user)
):
    """Get portfolio value history for charting"""
    pass

# Trading Router
trading_router = APIRouter(prefix="/trading", tags=["Trading"])

@trading_router.post("/orders", response_model=Order)
async def create_order(
    request: CreateOrderRequest,
    current_user=Depends(get_current_user)
):
    """Create new buy/sell order"""
    pass

@trading_router.get("/orders", response_model=List[Order])
async def get_orders(
    status: Optional[OrderStatus] = None,
    limit: int = 50,
    current_user=Depends(get_current_user)
):
    """Get order history with filtering"""
    pass

@trading_router.delete("/orders/{order_id}")
async def cancel_order(
    order_id: str,
    current_user=Depends(get_current_user)
):
    """Cancel pending order"""
    pass

@trading_router.get("/orders/{order_id}", response_model=Order)
async def get_order(
    order_id: str,
    current_user=Depends(get_current_user)
):
    """Get specific order details"""
    pass

# Market Data Router
market_router = APIRouter(prefix="/market", tags=["Market Data"])

@market_router.get("/prices", response_model=List[MarketData])
async def get_market_prices(symbols: List[str]):
    """Get current market prices for symbols"""
    pass

@market_router.get("/prices/{symbol}", response_model=MarketData)
async def get_price(symbol: str):
    """Get current price for specific symbol"""
    pass

@market_router.get("/chart/{symbol}")
async def get_chart_data(
    symbol: str,
    timeframe: str = "1h",
    limit: int = 100
):
    """Get OHLCV data for charting"""
    pass

@market_router.get("/trending")
async def get_trending_assets():
    """Get trending/top moving assets"""
    pass

# AI Analysis Router
ai_router = APIRouter(prefix="/ai", tags=["AI Analysis"])

@ai_router.post("/analyze", response_model=List[AIAnalysis])
async def analyze_symbols(
    request: AnalysisRequest,
    current_user=Depends(get_current_user)
):
    """Trigger AI analysis for symbols"""
    pass

@ai_router.get("/analysis/{symbol}", response_model=List[AIAnalysis])
async def get_analysis(
    symbol: str,
    analysis_type: Optional[AnalysisType] = None,
    limit: int = 10
):
    """Get latest AI analysis for symbol"""
    pass

@ai_router.get("/consensus/{symbol}")
async def get_ai_consensus(symbol: str):
    """Get multi-AI consensus with confidence scores"""
    pass

@ai_router.get("/sentiment")
async def get_market_sentiment():
    """Get overall market sentiment from social media"""
    pass

@ai_router.post("/backtest")
async def run_backtest(
    strategy_params: Dict[str, Any],
    current_user=Depends(get_current_user)
):
    """Run strategy backtesting"""
    pass

# Risk Management Router
risk_router = APIRouter(prefix="/risk", tags=["Risk Management"])

@risk_router.get("/metrics")
async def get_risk_metrics(current_user=Depends(get_current_user)):
    """Get portfolio risk metrics (VaR, Sharpe, etc.)"""
    pass

@risk_router.get("/alerts", response_model=List[Dict])
async def get_risk_alerts(current_user=Depends(get_current_user)):
    """Get active risk alerts"""
    pass

@risk_router.post("/stop-loss/{symbol}")
async def set_stop_loss(
    symbol: str,
    stop_price: float,
    current_user=Depends(get_current_user)
):
    """Set/update stop loss for position"""
    pass

# Settings Router
settings_router = APIRouter(prefix="/settings", tags=["Settings"])

@settings_router.get("/trading")
async def get_trading_settings(current_user=Depends(get_current_user)):
    """Get trading configuration"""
    pass

@settings_router.put("/trading")
async def update_trading_settings(
    settings: Dict[str, Any],
    current_user=Depends(get_current_user)
):
    """Update trading configuration"""
    pass

@settings_router.get("/notifications")
async def get_notification_settings(current_user=Depends(get_current_user)):
    """Get notification preferences"""
    pass

# WebSocket Router for Real-time Updates
ws_router = APIRouter(prefix="/ws", tags=["WebSocket"])

@ws_router.websocket("/portfolio/{user_id}")
async def portfolio_websocket(websocket, user_id: str):
    """Real-time portfolio updates"""
    pass

@ws_router.websocket("/market")
async def market_websocket(websocket):
    """Real-time market data stream"""
    pass

@ws_router.websocket("/orders/{user_id}")
async def orders_websocket(websocket, user_id: str):
    """Real-time order status updates"""
    pass

# ================================
# MAIN APP SETUP
# ================================

"""
FastAPI App Registration:

app = FastAPI(title="AI Trading Bot API", version="1.0.0")

app.include_router(auth_router)
app.include_router(portfolio_router)
app.include_router(trading_router)
app.include_router(market_router)
app.include_router(ai_router)
app.include_router(risk_router)
app.include_router(settings_router)
app.include_router(ws_router)

# Middleware for CORS, rate limiting, etc.
# Security middleware for JWT validation
# Error handlers for custom exceptions
"""

# ================================
# HTTP STATUS CODES REFERENCE
# ================================

"""
Standard HTTP Response Codes:

200 OK - Successful GET requests
201 Created - Successful POST requests (new resource)
204 No Content - Successful DELETE requests
400 Bad Request - Invalid request data
401 Unauthorized - Missing/invalid authentication
403 Forbidden - Insufficient permissions
404 Not Found - Resource not found
409 Conflict - Resource conflict (duplicate order)
422 Unprocessable Entity - Validation errors
429 Too Many Requests - Rate limit exceeded
500 Internal Server Error - Server errors
503 Service Unavailable - External API down
"""


-- Database Schema for AI Trading Bot
-- PostgreSQL + TimescaleDB for time-series data
-- SQLAlchemy ORM Models + Alembic Migrations

-- ================================
-- CORE TABLES (PostgreSQL)
-- ================================

-- Users table
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    is_active BOOLEAN DEFAULT true,
    is_verified BOOLEAN DEFAULT false,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    last_login TIMESTAMPTZ,
    
    -- Trading Configuration
    risk_tolerance DECIMAL(3,2) DEFAULT 0.05, -- 5% max risk per trade
    max_portfolio_risk DECIMAL(3,2) DEFAULT 0.15, -- 15% max drawdown
    auto_trading_enabled BOOLEAN DEFAULT false,
    
    -- Austrian Tax Settings
    tax_residence VARCHAR(2) DEFAULT 'AT',
    tax_id VARCHAR(50),
    
    CONSTRAINT valid_risk_tolerance CHECK (risk_tolerance BETWEEN 0 AND 1),
    CONSTRAINT valid_portfolio_risk CHECK (max_portfolio_risk BETWEEN 0 AND 1)
);

-- User portfolios
CREATE TABLE portfolios (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    name VARCHAR(100) NOT NULL DEFAULT 'Main Portfolio',
    initial_balance DECIMAL(15,2) NOT NULL,
    current_balance DECIMAL(15,2) NOT NULL,
    total_invested DECIMAL(15,2) DEFAULT 0,
    total_profit_loss DECIMAL(15,2) DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    
    UNIQUE(user_id, name)
);

-- Asset types and symbols
CREATE TABLE assets (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    symbol VARCHAR(20) UNIQUE NOT NULL, -- BTC, AAPL, etc.
    name VARCHAR(100) NOT NULL,
    asset_type VARCHAR(20) NOT NULL, -- crypto, stock, etf, commodity
    exchange VARCHAR(50), -- binance, nasdaq, etc.
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    
    -- Asset metadata
    sector VARCHAR(50),
    market_cap BIGINT,
    
    CONSTRAINT valid_asset_type CHECK (asset_type IN ('crypto', 'stock', 'etf', 'commodity', 'forex'))
);

-- Trading positions
CREATE TABLE positions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    portfolio_id UUID NOT NULL REFERENCES portfolios(id) ON DELETE CASCADE,
    asset_id UUID NOT NULL REFERENCES assets(id),
    
    -- Position details
    quantity DECIMAL(20,8) NOT NULL,
    avg_buy_price DECIMAL(15,8) NOT NULL,
    current_price DECIMAL(15,8),
    
    -- P&L calculation
    unrealized_pnl DECIMAL(15,2) DEFAULT 0,
    realized_pnl DECIMAL(15,2) DEFAULT 0,
    
    -- Position management
    status VARCHAR(20) DEFAULT 'open', -- open, closed
    opened_at TIMESTAMPTZ DEFAULT NOW(),
    closed_at TIMESTAMPTZ,
    
    -- Risk management
    stop_loss_price DECIMAL(15,8),
    take_profit_price DECIMAL(15,8),
    
    CONSTRAINT valid_status CHECK (status IN ('open', 'closed', 'partial')),
    CONSTRAINT positive_quantity CHECK (quantity > 0),
    CONSTRAINT positive_price CHECK (avg_buy_price > 0)
);

-- Trading orders
CREATE TABLE orders (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    portfolio_id UUID NOT NULL REFERENCES portfolios(id) ON DELETE CASCADE,
    position_id UUID REFERENCES positions(id), -- NULL for opening positions
    asset_id UUID NOT NULL REFERENCES assets(id),
    
    -- Order details
    order_type VARCHAR(20) NOT NULL, -- buy, sell, stop_loss, take_profit
    quantity DECIMAL(20,8) NOT NULL,
    price DECIMAL(15,8),
    stop_price DECIMAL(15,8), -- for stop orders
    
    -- Order execution
    status VARCHAR(20) DEFAULT 'pending', -- pending, executed, cancelled, failed
    executed_quantity DECIMAL(20,8) DEFAULT 0,
    executed_price DECIMAL(15,8),
    
    -- Timestamps
    created_at TIMESTAMPTZ DEFAULT NOW(),
    executed_at TIMESTAMPTZ,
    cancelled_at TIMESTAMPTZ,
    
    -- External reference
    external_order_id VARCHAR(100), -- Bitpanda order ID
    
    -- Fees
    fee_amount DECIMAL(15,8) DEFAULT 0,
    fee_currency VARCHAR(10),
    
    CONSTRAINT valid_order_type CHECK (order_type IN ('buy', 'sell', 'stop_loss', 'take_profit', 'market', 'limit')),
    CONSTRAINT valid_order_status CHECK (status IN ('pending', 'executed', 'cancelled', 'failed', 'partial')),
    CONSTRAINT positive_quantity CHECK (quantity > 0)
);

-- AI Analysis results
CREATE TABLE ai_analyses (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    asset_id UUID NOT NULL REFERENCES assets(id),
    
    -- Analysis details
    analysis_type VARCHAR(20) NOT NULL, -- fundamental, technical, sentiment, consensus
    ai_model VARCHAR(50) NOT NULL, -- gpt-4.1, deepseek-r1, gemini, etc.
    
    -- Results
    recommendation VARCHAR(10) NOT NULL, -- BUY, SELL, HOLD
    confidence_score DECIMAL(5,4) NOT NULL, -- 0.0000 to 1.0000
    target_price DECIMAL(15,8),
    reasoning TEXT,
    
    -- Key indicators (JSON)
    indicators JSONB,
    
    -- Metadata
    created_at TIMESTAMPTZ DEFAULT NOW(),
    expires_at TIMESTAMPTZ, -- when analysis becomes stale
    
    CONSTRAINT valid_analysis_type CHECK (analysis_type IN ('fundamental', 'technical', 'sentiment', 'consensus')),
    CONSTRAINT valid_recommendation CHECK (recommendation IN ('BUY', 'SELL', 'HOLD')),
    CONSTRAINT valid_confidence CHECK (confidence_score BETWEEN 0 AND 1)
);

-- Risk alerts
CREATE TABLE risk_alerts (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    portfolio_id UUID REFERENCES portfolios(id) ON DELETE CASCADE,
    
    -- Alert details
    alert_type VARCHAR(30) NOT NULL, -- drawdown, concentration, volatility
    severity VARCHAR(10) NOT NULL, -- low, medium, high, critical
    message TEXT NOT NULL,
    
    -- Alert data
    current_value DECIMAL(15,8),
    threshold_value DECIMAL(15,8),
    
    -- Status
    is_active BOOLEAN DEFAULT true,
    acknowledged_at TIMESTAMPTZ,
    resolved_at TIMESTAMPTZ,
    
    created_at TIMESTAMPTZ DEFAULT NOW(),
    
    CONSTRAINT valid_alert_type CHECK (alert_type IN ('drawdown', 'concentration', 'volatility', 'stop_loss', 'margin_call')),
    CONSTRAINT valid_severity CHECK (severity IN ('low', 'medium', 'high', 'critical'))
);

-- System configuration
CREATE TABLE system_config (
    key VARCHAR(100) PRIMARY KEY,
    value TEXT NOT NULL,
    description TEXT,
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    updated_by UUID REFERENCES users(id)
);

-- ================================
-- TIME-SERIES TABLES (TimescaleDB)
-- ================================

-- Market data (OHLCV)
CREATE TABLE market_data (
    time TIMESTAMPTZ NOT NULL,
    asset_id UUID NOT NULL REFERENCES assets(id),
    
    -- OHLCV data
    open_price DECIMAL(15,8) NOT NULL,
    high_price DECIMAL(15,8) NOT NULL,
    low_price DECIMAL(15,8) NOT NULL,
    close_price DECIMAL(15,8) NOT NULL,
    volume DECIMAL(20,8) NOT NULL,
    
    -- Additional metrics
    volume_quote DECIMAL(20,8), -- volume in quote currency
    trades_count INTEGER,
    
    -- Timeframe
    timeframe VARCHAR(10) NOT NULL, -- 1m, 5m, 1h, 1d, etc.
    
    PRIMARY KEY (time, asset_id, timeframe)
);

-- Convert to hypertable for time-series optimization
SELECT create_hypertable('market_data', 'time');

-- Portfolio value history
CREATE TABLE portfolio_history (
    time TIMESTAMPTZ NOT NULL,
    portfolio_id UUID NOT NULL REFERENCES portfolios(id) ON DELETE CASCADE,
    
    -- Portfolio metrics
    total_value DECIMAL(15,2) NOT NULL,
    cash_balance DECIMAL(15,2) NOT NULL,
    invested_value DECIMAL(15,2) NOT NULL,
    unrealized_pnl DECIMAL(15,2) NOT NULL,
    realized_pnl DECIMAL(15,2) NOT NULL,
    
    -- Performance metrics
    daily_return DECIMAL(8,6),
    sharpe_ratio DECIMAL(8,4),
    max_drawdown DECIMAL(8,4),
    
    PRIMARY KEY (time, portfolio_id)
);

SELECT create_hypertable('portfolio_history', 'time');

-- Real-time price updates
CREATE TABLE price_updates (
    time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    asset_id UUID NOT NULL REFERENCES assets(id),
    
    price DECIMAL(15,8) NOT NULL,
    volume_24h DECIMAL(20,8),
    change_24h DECIMAL(15,8),
    change_percent_24h DECIMAL(8,4),
    
    -- Market data
    market_cap BIGINT,
    rank INTEGER,
    
    PRIMARY KEY (time, asset_id)
);

SELECT create_hypertable('price_updates', 'time');

-- Social sentiment data
CREATE TABLE sentiment_data (
    time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    asset_id UUID NOT NULL REFERENCES assets(id),
    
    -- Sentiment scores (-1 to 1)
    twitter_sentiment DECIMAL(5,4),
    reddit_sentiment DECIMAL(5,4),
    news_sentiment DECIMAL(5,4),
    overall_sentiment DECIMAL(5,4),
    
    -- Volume metrics
    twitter_mentions INTEGER DEFAULT 0,
    reddit_mentions INTEGER DEFAULT 0,
    news_articles INTEGER DEFAULT 0,
    
    -- Fear & Greed Index
    fear_greed_index INTEGER, -- 0-100
    
    PRIMARY KEY (time, asset_id)
);

SELECT create_hypertable('sentiment_data', 'time');

-- System metrics
CREATE TABLE system_metrics (
    time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    metric_name VARCHAR(100) NOT NULL,
    
    -- Metric values
    value DECIMAL(15,8),
    string_value TEXT,
    
    -- Labels (JSON for flexibility)
    labels JSONB,
    
    PRIMARY KEY (time, metric_name)
);

SELECT create_hypertable('system_metrics', 'time');

-- ================================
-- INDEXES FOR PERFORMANCE
-- ================================

-- Users indexes
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_username ON users(username);
CREATE INDEX idx_users_active ON users(is_active) WHERE is_active = true;

-- Portfolios indexes
CREATE INDEX idx_portfolios_user_id ON portfolios(user_id);
CREATE INDEX idx_portfolios_updated_at ON portfolios(updated_at);

-- Assets indexes
CREATE INDEX idx_assets_symbol ON assets(symbol);
CREATE INDEX idx_assets_type ON assets(asset_type);
CREATE INDEX idx_assets_active ON assets(is_active) WHERE is_active = true;

-- Positions indexes
CREATE INDEX idx_positions_portfolio_id ON positions(portfolio_id);
CREATE INDEX idx_positions_asset_id ON positions(asset_id);
CREATE INDEX idx_positions_status ON positions(status);
CREATE INDEX idx_positions_opened_at ON positions(opened_at);

-- Orders indexes
CREATE INDEX idx_orders_portfolio_id ON orders(portfolio_id);
CREATE INDEX idx_orders_asset_id ON orders(asset_id);
CREATE INDEX idx_orders_status ON orders(status);
CREATE INDEX idx_orders_created_at ON orders(created_at);
CREATE INDEX idx_orders_external_id ON orders(external_order_id);

-- AI Analysis indexes
CREATE INDEX idx_ai_analyses_asset_id ON ai_analyses(asset_id);
CREATE INDEX idx_ai_analyses_type ON ai_analyses(analysis_type);
CREATE INDEX idx_ai_analyses_created_at ON ai_analyses(created_at);
CREATE INDEX idx_ai_analyses_expires_at ON ai_analyses(expires_at);

-- Time-series indexes (automatically created by TimescaleDB)
CREATE INDEX idx_market_data_asset_timeframe ON market_data(asset_id, timeframe, time DESC);
CREATE INDEX idx_portfolio_history_portfolio ON portfolio_history(portfolio_id, time DESC);
CREATE INDEX idx_price_updates_asset ON price_updates(asset_id, time DESC);
CREATE INDEX idx_sentiment_data_asset ON sentiment_data(asset_id, time DESC);

-- ================================
-- VIEWS FOR COMMON QUERIES
-- ================================

-- Current portfolio view
CREATE VIEW current_portfolio_view AS
SELECT 
    p.id as portfolio_id,
    p.user_id,
    p.name as portfolio_name,
    p.current_balance,
    COUNT(pos.id) as positions_count,
    COALESCE(SUM(pos.quantity * pos.current_price), 0) as total_invested_value,
    COALESCE(SUM(pos.unrealized_pnl), 0) as total_unrealized_pnl,
    (p.current_balance + COALESCE(SUM(pos.quantity * pos.current_price), 0)) as total_portfolio_value
FROM portfolios p
LEFT JOIN positions pos ON p.id = pos.portfolio_id AND pos.status = 'open'
GROUP BY p.id, p.user_id, p.name, p.current_balance;

-- Latest AI consensus view
CREATE VIEW latest_ai_consensus AS
SELECT DISTINCT ON (asset_id)
    asset_id,
    a.symbol,
    recommendation,
    confidence_score,
    reasoning,
    created_at
FROM ai_analyses aa
JOIN assets a ON aa.asset_id = a.id
WHERE analysis_type = 'consensus'
    AND expires_at > NOW()
ORDER BY asset_id, created_at DESC;

-- Active risk alerts view
CREATE VIEW active_risk_alerts AS
SELECT 
    ra.*,
    u.username,
    p.name as portfolio_name
FROM risk_alerts ra
JOIN users u ON ra.user_id = u.id
LEFT JOIN portfolios p ON ra.portfolio_id = p.id
WHERE ra.is_active = true
    AND ra.resolved_at IS NULL
ORDER BY ra.severity DESC, ra.created_at DESC;

-- ================================
-- FUNCTIONS FOR CALCULATED FIELDS
-- ================================

-- Calculate portfolio performance
CREATE OR REPLACE FUNCTION calculate_portfolio_performance(
    p_portfolio_id UUID,
    p_start_date TIMESTAMPTZ DEFAULT NOW() - INTERVAL '30 days'
)
RETURNS TABLE(
    total_return DECIMAL(8,4),
    annualized_return DECIMAL(8,4),
    sharpe_ratio DECIMAL(8,4),
    max_drawdown DECIMAL(8,4),
    volatility DECIMAL(8,4)
) AS $$
BEGIN
    -- Implementation for portfolio performance calculation
    -- Using portfolio_history data
    RETURN QUERY
    SELECT 
        0.0::DECIMAL(8,4) as total_return,
        0.0::DECIMAL(8,4) as annualized_return,
        0.0::DECIMAL(8,4) as sharpe_ratio,
        0.0::DECIMAL(8,4) as max_drawdown,
        0.0::DECIMAL(8,4) as volatility;
END;
$$ LANGUAGE plpgsql;

-- Update current prices for positions
CREATE OR REPLACE FUNCTION update_position_prices()
RETURNS TRIGGER AS $$
BEGIN
    -- Update unrealized P&L when current_price changes
    NEW.unrealized_pnl = (NEW.current_price - NEW.avg_buy_price) * NEW.quantity;
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER update_position_pnl
    BEFORE UPDATE OF current_price ON positions
    FOR EACH ROW
    EXECUTE FUNCTION update_position_prices();

-- ================================
-- DATA RETENTION POLICIES
-- ================================

-- Keep detailed market data for 1 year, then aggregate
SELECT add_retention_policy('market_data', INTERVAL '1 year');

-- Keep sentiment data for 6 months
SELECT add_retention_policy('sentiment_data', INTERVAL '6 months');

-- Keep system metrics for 3 months
SELECT add_retention_policy('system_metrics', INTERVAL '3 months');

-- Portfolio history is kept indefinitely for compliance

-- ================================
-- SAMPLE DATA INSERTS
-- ================================

-- Insert sample assets
INSERT INTO assets (symbol, name, asset_type, exchange) VALUES
('BTC', 'Bitcoin', 'crypto', 'bitpanda'),
('ETH', 'Ethereum', 'crypto', 'bitpanda'),
('AAPL', 'Apple Inc.', 'stock', 'nasdaq'),
('GOOGL', 'Alphabet Inc.', 'stock', 'nasdaq'),
('SPY', 'SPDR S&P 500 ETF', 'etf', 'nyse'),
('GOLD', 'Gold', 'commodity', 'lbma');

-- Insert system configuration
INSERT INTO system_config (key, value, description) VALUES
('max_daily_trades', '50', 'Maximum trades per day per user'),
('min_trade_amount', '10.00', 'Minimum trade amount in EUR'),
('max_position_size', '0.20', 'Maximum position size as % of portfolio'),
('stop_loss_default', '0.05', 'Default stop loss percentage'),
('take_profit_default', '0.15', 'Default take profit percentage');

-- ================================
-- DATABASE PERMISSIONS
-- ================================

-- Create roles for application
CREATE ROLE trading_bot_app;
CREATE ROLE trading_bot_readonly;

-- Grant permissions
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO trading_bot_app;
GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO trading_bot_app;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO trading_bot_readonly;

-- Row Level Security (future enhancement)
-- ALTER TABLE portfolios ENABLE ROW LEVEL SECURITY;
-- CREATE POLICY portfolio_isolation ON portfolios FOR ALL TO trading_bot_app 
-- USING (user_id = current_setting('app.current_user_id')::UUID);

# Service Dependencies & Infrastructure Configuration
# ai-trading-bot/infrastructure/

# ================================
# DOCKER-COMPOSE.YML - LOKALE ENTWICKLUNG
# ================================
---
version: '3.8'

services:
  # ===================
  # DATABASES
  # ===================
  postgres:
    image: timescale/timescaledb:2.11.0-pg15
    container_name: trading_postgres
    environment:
      POSTGRES_DB: trading_bot
      POSTGRES_USER: trading_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U trading_user -d trading_bot"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7.0-alpine
    container_name: trading_redis
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  # ===================
  # MESSAGE BROKER
  # ===================
  rabbitmq:
    image: rabbitmq:3.11-management-alpine
    container_name: trading_rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD}
      RABBITMQ_DEFAULT_VHOST: trading
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"  # Management UI
    restart: unless-stopped
    healthcheck:
      test: rabbitmq-diagnostics -q ping
      interval: 10s
      timeout: 5s
      retries: 5

  # ===================
  # CORE SERVICES
  # ===================
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
      target: development
    container_name: trading_api
    environment:
      - DATABASE_URL=postgresql://trading_user:${POSTGRES_PASSWORD}@postgres:5432/trading_bot
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/0
      - RABBITMQ_URL=amqp://${RABBITMQ_USER}:${RABBITMQ_PASSWORD}@rabbitmq:5672/trading
      - BITPANDA_API_KEY=${BITPANDA_API_KEY}
      - BITPANDA_API_SECRET=${BITPANDA_API_SECRET}
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - ENVIRONMENT=development
    volumes:
      - ./api:/app
      - api_logs:/app/logs
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

  ai-engine:
    build:
      context: ./ai-engine
      dockerfile: Dockerfile
    container_name: trading_ai_engine
    environment:
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - OLLAMA_ENDPOINT=${OLLAMA_ENDPOINT:-http://ollama:11434}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/1
      - RABBITMQ_URL=amqp://${RABBITMQ_USER}:${RABBITMQ_PASSWORD}@rabbitmq:5672/trading
    volumes:
      - ./ai-engine:/app
      - ai_models:/app/models
    depends_on:
      - redis
      - rabbitmq
      - ollama
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # ===================
  # AI MODELS
  # ===================
  ollama:
    image: ollama/ollama:latest
    container_name: trading_ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]  # Für GPU-Support
    # Nach dem Start: docker exec trading_ollama ollama pull gemma:7b

  # ===================
  # DATA COLLECTORS
  # ===================
  market-data-collector:
    build:
      context: ./data-collector
      dockerfile: Dockerfile
    container_name: trading_data_collector
    environment:
      - DATABASE_URL=postgresql://trading_user:${POSTGRES_PASSWORD}@postgres:5432/trading_bot
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379/2
      - ALPHA_VANTAGE_API_KEY=${ALPHA_VANTAGE_API_KEY}
      - COINGECKO_API_KEY=${COINGECKO_API_KEY}
      - NEWS_API_KEY=${NEWS_API_KEY}
      - TWITTER_BEARER_TOKEN=${TWITTER_BEARER_TOKEN}
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    volumes:
      - ./data-collector:/app

  # ===================
  # FRONTEND
  # ===================
  dashboard:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      target: development
    container_name: trading_dashboard
    environment:
      - REACT_APP_API_URL=http://localhost:8000
      - REACT_APP_WS_URL=ws://localhost:8000
      - NODE_ENV=development
    volumes:
      - ./frontend:/app
      - /app/node_modules
    ports:
      - "3000:3000"
    depends_on:
      - api
    restart: unless-stopped
    command: ["npm", "start"]

  # ===================
  # MONITORING
  # ===================
  prometheus:
    image: prom/prometheus:v2.40.0
    container_name: trading_prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    restart: unless-stopped

  grafana:
    image: grafana/grafana:9.3.0
    container_name: trading_grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    ports:
      - "3001:3000"
    depends_on:
      - prometheus
    restart: unless-stopped

  jaeger:
    image: jaegertracing/all-in-one:1.41
    container_name: trading_jaeger
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "16686:16686"
      - "14268:14268"
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    restart: unless-stopped

  # ===================
  # REVERSE PROXY
  # ===================
  nginx:
    image: nginx:alpine
    container_name: trading_nginx
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/ssl/certs
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - api
      - dashboard
    restart: unless-stopped

# ===================
# VOLUMES
# ===================
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  rabbitmq_data:
    driver: local
  ollama_data:
    driver: local
  api_logs:
    driver: local
  ai_models:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# ===================
# NETWORKS
# ===================
networks:
  default:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

---
# ================================
# KUBERNETES DEPLOYMENT MANIFESTS
# ================================

# Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: trading-bot
  labels:
    name: trading-bot
    environment: production

---
# ConfigMap für Application Config
apiVersion: v1
kind: ConfigMap
metadata:
  name: trading-bot-config
  namespace: trading-bot
data:
  DATABASE_HOST: "postgres-service"
  DATABASE_PORT: "5432"
  DATABASE_NAME: "trading_bot"
  REDIS_HOST: "redis-service"
  REDIS_PORT: "6379"
  RABBITMQ_HOST: "rabbitmq-service"
  RABBITMQ_PORT: "5672"
  ENVIRONMENT: "production"
  LOG_LEVEL: "INFO"
  MAX_WORKERS: "4"
  RATE_LIMIT_PER_MINUTE: "100"

---
# Secret für sensitive Daten
apiVersion: v1
kind: Secret
metadata:
  name: trading-bot-secrets
  namespace: trading-bot
type: Opaque
stringData:
  POSTGRES_PASSWORD: "your-postgres-password"
  REDIS_PASSWORD: "your-redis-password"
  RABBITMQ_PASSWORD: "your-rabbitmq-password"
  BITPANDA_API_KEY: "your-bitpanda-api-key"
  BITPANDA_API_SECRET: "your-bitpanda-secret"
  AZURE_OPENAI_API_KEY: "your-azure-openai-key"
  AZURE_OPENAI_ENDPOINT: "your-azure-endpoint"
  DEEPSEEK_API_KEY: "your-deepseek-key"
  JWT_SECRET_KEY: "your-jwt-secret"

---
# PostgreSQL Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: trading-bot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: timescale/timescaledb:2.11.0-pg15
        env:
        - name: POSTGRES_DB
          value: "trading_bot"
        - name: POSTGRES_USER
          value: "trading_user"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: trading-bot-secrets
              key: POSTGRES_PASSWORD
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - trading_user
            - -d
            - trading_bot
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - trading_user
            - -d
            - trading_bot
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-pvc

---
# PostgreSQL PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: trading-bot
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: gp2  # AWS EBS, anpassen je nach Provider

---
# PostgreSQL Service
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
  namespace: trading-bot
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
  type: ClusterIP

---
# Redis Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: trading-bot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7.0-alpine
        command:
        - redis-server
        - --requirepass
        - $(REDIS_PASSWORD)
        env:
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: trading-bot-secrets
              key: REDIS_PASSWORD
        ports:
        - containerPort: 6379
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          tcpSocket:
            port: 6379
          initialDelaySeconds: 30
          periodSeconds: 10

---
# Redis Service
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  namespace: trading-bot
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
  type: ClusterIP

---
# Trading API Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trading-api
  namespace: trading-bot
spec:
  replicas: 3
  selector:
    matchLabels:
      app: trading-api
  template:
    metadata:
      labels:
        app: trading-api
    spec:
      containers:
      - name: api
        image: trading-bot/api:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          value: "postgresql://trading_user:$(POSTGRES_PASSWORD)@postgres-service:5432/trading_bot"
        - name: REDIS_URL
          value: "redis://:$(REDIS_PASSWORD)@redis-service:6379/0"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: trading-bot-secrets
              key: POSTGRES_PASSWORD
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: trading-bot-secrets
              key: REDIS_PASSWORD
        - name: BITPANDA_API_KEY
          valueFrom:
            secretKeyRef:
              name: trading-bot-secrets
              key: BITPANDA_API_KEY
        envFrom:
        - configMapRef:
            name: trading-bot-config
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
# Trading API Service
apiVersion: v1
kind: Service
metadata:
  name: trading-api-service
  namespace: trading-bot
spec:
  selector:
    app: trading-api
  ports:
  - port: 8000
    targetPort: 8000
  type: ClusterIP

---
# AI Engine Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-engine
  namespace: trading-bot
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ai-engine
  template:
    metadata:
      labels:
        app: ai-engine
    spec:
      containers:
      - name: ai-engine
        image: trading-bot/ai-engine:latest
        env:
        - name: AZURE_OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: trading-bot-secrets
              key: AZURE_OPENAI_API_KEY
        - name: DEEPSEEK_API_KEY
          valueFrom:
            secretKeyRef:
              name: trading-bot-secrets
              key: DEEPSEEK_API_KEY
        envFrom:
        - configMapRef:
            name: trading-bot-config
        resources:
          requests:
            memory: "1Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        # GPU Resources für Ollama (optional)
        # resources:
        #   limits:
        #     nvidia.com/gpu: 1

---
# Frontend Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dashboard
  namespace: trading-bot
spec:
  replicas: 2
  selector:
    matchLabels:
      app: dashboard
  template:
    metadata:
      labels:
        app: dashboard
    spec:
      containers:
      - name: dashboard
        image: trading-bot/dashboard:latest
        ports:
        - containerPort: 80
        env:
        - name: REACT_APP_API_URL
          value: "https://api.your-domain.com"
        - name: REACT_APP_WS_URL
          value: "wss://api.your-domain.com"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

---
# Dashboard Service
apiVersion: v1
kind: Service
metadata:
  name: dashboard-service
  namespace: trading-bot
spec:
  selector:
    app: dashboard
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP

---
# Ingress für External Access
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: trading-bot-ingress
  namespace: trading-bot
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - api.your-domain.com
    - app.your-domain.com
    secretName: trading-bot-tls
  rules:
  - host: api.your-domain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: trading-api-service
            port:
              number: 8000
  - host: app.your-domain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: dashboard-service
            port:
              number: 80

---
# HorizontalPodAutoscaler für API
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: trading-api-hpa
  namespace: trading-bot
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: trading-api
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80


# CI/CD Pipeline & Development Environment
# ai-trading-bot/.github/workflows/

# ================================
# MAIN CI/CD PIPELINE
# ================================
# .github/workflows/ci-cd.yml
name: AI Trading Bot CI/CD

on:
  push:
    branches: [ main, develop ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main, develop ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # ===================
  # CODE QUALITY & TESTS
  # ===================
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.11', '3.12']
    
    services:
      postgres:
        image: timescale/timescaledb:2.11.0-pg15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7.0-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r api/requirements.txt
        pip install -r api/requirements-dev.txt

    - name: Run pre-commit hooks
      uses: pre-commit/action@v3.0.0

    - name: Lint with flake8
      run: |
        flake8 api/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 api/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Type checking with mypy
      run: |
        mypy api/app --ignore-missing-imports

    - name: Security check with bandit
      run: |
        bandit -r api/app -f json -o bandit-report.json
      continue-on-error: true

    - name: Run pytest
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379/0
        ENVIRONMENT: testing
      run: |
        cd api
        pytest --cov=app --cov-report=xml --cov-report=html tests/

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./api/coverage.xml
        flags: unittests
        name: codecov-umbrella

    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}
        path: |
          api/htmlcov/
          bandit-report.json

  # ===================
  # SECURITY SCANNING
  # ===================
  security:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

    - name: Run Snyk to check for vulnerabilities
      uses: snyk/actions/python@master
      env:
        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
      with:
        args: --severity-threshold=high

  # ===================
  # BUILD DOCKER IMAGES
  # ===================
  build:
    runs-on: ubuntu-latest
    needs: [test, security]
    if: github.event_name != 'pull_request'
    
    strategy:
      matrix:
        service: [api, ai-engine, dashboard, data-collector]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha,prefix={{branch}}-

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: ./${{ matrix.service }}
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64,linux/arm64

  # ===================
  # DEPLOY TO STAGING
  # ===================
  deploy-staging:
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/develop'
    environment: staging
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'

    - name: Configure kubectl
      env:
        KUBE_CONFIG: ${{ secrets.KUBE_CONFIG_STAGING }}
      run: |
        echo "$KUBE_CONFIG" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig

    - name: Deploy to staging
      run: |
        envsubst < infrastructure/k8s/staging/kustomization.yaml | kubectl apply -f -
        kubectl rollout status deployment/trading-api -n trading-bot-staging
        kubectl rollout status deployment/ai-engine -n trading-bot-staging
        kubectl rollout status deployment/dashboard -n trading-bot-staging

    - name: Run health checks
      run: |
        kubectl wait --for=condition=ready pod -l app=trading-api -n trading-bot-staging --timeout=300s
        kubectl get pods -n trading-bot-staging

  # ===================
  # DEPLOY TO PRODUCTION
  # ===================
  deploy-production:
    runs-on: ubuntu-latest
    needs: build
    if: startsWith(github.ref, 'refs/tags/v')
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'

    - name: Configure kubectl
      env:
        KUBE_CONFIG: ${{ secrets.KUBE_CONFIG_PRODUCTION }}
      run: |
        echo "$KUBE_CONFIG" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig

    - name: Deploy to production
      run: |
        envsubst < infrastructure/k8s/production/kustomization.yaml | kubectl apply -f -
        kubectl rollout status deployment/trading-api -n trading-bot-production
        kubectl rollout status deployment/ai-engine -n trading-bot-production
        kubectl rollout status deployment/dashboard -n trading-bot-production

    - name: Verify deployment
      run: |
        kubectl wait --for=condition=ready pod -l app=trading-api -n trading-bot-production --timeout=300s
        kubectl get pods -n trading-bot-production
        
    - name: Run smoke tests
      run: |
        curl -f https://api.your-domain.com/health || exit 1
        curl -f https://app.your-domain.com || exit 1

---
# ================================
# DEPENDENCY UPDATE AUTOMATION
# ================================
# .github/workflows/dependency-update.yml
name: Dependency Update

on:
  schedule:
    - cron: '0 2 * * 1'  # Every Monday at 2 AM
  workflow_dispatch:

jobs:
  update-dependencies:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.PAT_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Update Python dependencies
      run: |
        pip install pip-tools
        cd api
        pip-compile --upgrade requirements.in
        pip-compile --upgrade requirements-dev.in

    - name: Update Node.js dependencies
      run: |
        cd frontend
        npm update
        npm audit fix

    - name: Create Pull Request
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ secrets.PAT_TOKEN }}
        commit-message: 'chore: update dependencies'
        title: 'Automated dependency update'
        body: |
          This PR updates all dependencies to their latest versions.
          
          Please review the changes and ensure all tests pass.
        branch: automated-dependency-update
        delete-branch: true

---
# ================================
# DOCKER BUILD OPTIMIZATION
# ================================
# .github/workflows/docker-build.yml
name: Optimized Docker Build

on:
  push:
    paths:
      - 'api/**'
      - 'ai-engine/**'
      - 'frontend/**'
      - 'data-collector/**'

jobs:
  changes:
    runs-on: ubuntu-latest
    outputs:
      api: ${{ steps.changes.outputs.api }}
      ai-engine: ${{ steps.changes.outputs.ai-engine }}
      frontend: ${{ steps.changes.outputs.frontend }}
      data-collector: ${{ steps.changes.outputs.data-collector }}
    steps:
    - uses: actions/checkout@v4
    - uses: dorny/paths-filter@v2
      id: changes
      with:
        filters: |
          api:
            - 'api/**'
          ai-engine:
            - 'ai-engine/**'
          frontend:
            - 'frontend/**'
          data-collector:
            - 'data-collector/**'

  build-api:
    needs: changes
    if: ${{ needs.changes.outputs.api == 'true' }}
    runs-on: ubuntu-latest
    steps:
    - name: Build API service
      run: echo "Building API service"
      # API-specific build steps

  build-ai-engine:
    needs: changes
    if: ${{ needs.changes.outputs.ai-engine == 'true' }}
    runs-on: ubuntu-latest
    steps:
    - name: Build AI Engine service
      run: echo "Building AI Engine service"
      # AI Engine-specific build steps

---
# ================================
# PERFORMANCE TESTING
# ================================
# .github/workflows/performance.yml
name: Performance Testing

on:
  schedule:
    - cron: '0 0 * * 0'  # Weekly on Sunday
  workflow_dispatch:

jobs:
  load-test:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install locust

    - name: Run load tests
      env:
        TARGET_HOST: https://api-staging.your-domain.com
      run: |
        cd tests/performance
        locust --headless --users 100 --spawn-rate 10 --run-time 300s --host $TARGET_HOST

    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: tests/performance/results/

---
# ================================
# DEVELOPMENT ENVIRONMENT SETUP
# ================================

# .devcontainer/devcontainer.json
{
  "name": "AI Trading Bot Development",
  "dockerComposeFile": "../docker-compose.dev.yml",
  "service": "dev-environment",
  "workspaceFolder": "/workspace",
  "shutdownAction": "stopCompose",
  
  "customizations": {
    "vscode": {
      "extensions": [
        "ms-python.python",
        "ms-python.flake8",
        "ms-python.mypy-type-checker",
        "ms-python.black-formatter",
        "ms-python.isort",
        "ms-vscode.docker",
        "ms-kubernetes-tools.vscode-kubernetes-tools",
        "github.copilot",
        "redhat.vscode-yaml",
        "ms-vscode.vscode-json",
        "bradlc.vscode-tailwindcss",
        "esbenp.prettier-vscode"
      ],
      "settings": {
        "python.defaultInterpreterPath": "/usr/local/bin/python",
        "python.linting.enabled": true,
        "python.linting.flake8Enabled": true,
        "python.formatting.provider": "black",
        "python.sortImports.provider": "isort",
        "editor.formatOnSave": true,
        "editor.codeActionsOnSave": {
          "source.organizeImports": true
        }
      }
    }
  },
  
  "forwardPorts": [8000, 3000, 5432, 6379],
  "portsAttributes": {
    "8000": {
      "label": "FastAPI",
      "onAutoForward": "notify"
    },
    "3000": {
      "label": "React Dashboard",
      "onAutoForward": "notify"
    }
  },
  
  "postCreateCommand": "pip install -r api/requirements-dev.txt && cd frontend && npm install",
  
  "remoteUser": "vscode"
}

---
# ================================
# MAKEFILE FÜR LOKALE ENTWICKLUNG
# ================================

# Makefile
.PHONY: help build up down logs test clean install lint format

# Default target
help: ## Show this help message
	@echo "AI Trading Bot - Development Commands"
	@echo "====================================="
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-20s\033[0m %s\n", $$1, $$2}'

# Development environment
install: ## Install all dependencies
	@echo "Installing Python dependencies..."
	cd api && pip install -r requirements.txt -r requirements-dev.txt
	@echo "Installing Node.js dependencies..."
	cd frontend && npm install
	@echo "Setting up pre-commit hooks..."
	pre-commit install

setup: ## Initial project setup
	@echo "Setting up development environment..."
	cp .env.example .env
	@echo "Please edit .env file with your configuration"
	make install
	make build

# Docker operations
build: ## Build all Docker images
	docker-compose build

up: ## Start all services
	docker-compose up -d

down: ## Stop all services
	docker-compose down

restart: ## Restart all services
	docker-compose restart

logs: ## Show logs for all services
	docker-compose logs -f

logs-api: ## Show API logs
	docker-compose logs -f api

logs-ai: ## Show AI Engine logs
	docker-compose logs -f ai-engine

# Database operations
db-migrate: ## Run database migrations
	docker-compose exec api alembic upgrade head

db-reset: ## Reset database (WARNING: This will delete all data)
	docker-compose down -v
	docker-compose up -d postgres
	sleep 5
	make db-migrate

db-seed: ## Seed database with test data
	docker-compose exec api python scripts/seed_data.py

# Development
dev: ## Start development environment
	docker-compose -f docker-compose.dev.yml up

test: ## Run all tests
	docker-compose exec api pytest tests/ -v --cov=app

test-unit: ## Run unit tests only
	docker-compose exec api pytest tests/unit/ -v

test-integration: ## Run integration tests only
	docker-compose exec api pytest tests/integration/ -v

test-e2e: ## Run end-to-end tests
	cd frontend && npm run test:e2e

# Code quality
lint: ## Run all linters
	docker-compose exec api flake8 app/
	docker-compose exec api mypy app/
	docker-compose exec api bandit -r app/
	cd frontend && npm run lint

format: ## Format code
	docker-compose exec api black app/ tests/
	docker-compose exec api isort app/ tests/
	cd frontend && npm run format

pre-commit: ## Run pre-commit hooks
	pre-commit run --all-files

# Monitoring
metrics: ## Open Grafana dashboard
	@echo "Opening Grafana at http://localhost:3001"
	@echo "Default credentials: admin/admin"
	open http://localhost:3001

monitoring: ## Start monitoring stack
	docker-compose -f docker-compose.monitoring.yml up -d

# Deployment
deploy-staging: ## Deploy to staging
	kubectl apply -f infrastructure/k8s/staging/

deploy-prod: ## Deploy to production (requires confirmation)
	@echo "Are you sure you want to deploy to production? [y/N]" && read ans && [ $${ans:-N} = y ]
	kubectl apply -f infrastructure/k8s/production/

# Cleanup
clean: ## Clean up Docker resources
	docker-compose down -v --remove-orphans
	docker system prune -f

clean-all: ## Clean up everything (images, volumes, etc.)
	docker-compose down -v --remove-orphans
	docker system prune -af
	docker volume prune -f

# Security
security-scan: ## Run security scans
	docker run --rm -v $(PWD):/workspace aquasec/trivy fs /workspace
	docker-compose exec api bandit -r app/ -f json -o bandit-report.json

# AI Models
ollama-setup: ## Setup Ollama models
	docker-compose exec ollama ollama pull gemma:7b
	docker-compose exec ollama ollama pull mistral:7b

# Backup
backup-db: ## Backup database
	docker-compose exec postgres pg_dump -U trading_user trading_bot > backup_$(shell date +%Y%m%d_%H%M%S).sql

# Environment variables
env-check: ## Check required environment variables
	@echo "Checking environment variables..."
	@python scripts/check_env.py

# Documentation
docs: ## Generate API documentation
	docker-compose exec api python scripts/generate_docs.py
	@echo "API documentation generated at docs/"

# Performance
benchmark: ## Run performance benchmarks
	docker-compose exec api python scripts/benchmark.py

load-test: ## Run load tests
	cd tests/performance && locust --headless --users 50 --spawn-rate 5 --run-time 60s --host http://localhost:8000

---
# ================================
# ENVIRONMENT CONFIGURATION
# ================================

# .env.example
# Database Configuration
POSTGRES_PASSWORD=your_postgres_password
DATABASE_URL=postgresql://trading_user:your_postgres_password@postgres:5432/trading_bot

# Redis Configuration
REDIS_PASSWORD=your_redis_password
REDIS_URL=redis://:your_redis_password@redis:6379/0

# Message Broker
RABBITMQ_USER=trading_user
RABBITMQ_PASSWORD=your_rabbitmq_password
RABBITMQ_URL=amqp://trading_user:your_rabbitmq_password@rabbitmq:5672/trading

# External APIs
BITPANDA_API_KEY=your_bitpanda_api_key
BITPANDA_API_SECRET=your_bitpanda_secret
BITPANDA_SANDBOX=true

# AI Services
AZURE_OPENAI_API_KEY=your_azure_openai_key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_MODEL=gpt-4
DEEPSEEK_API_KEY=your_deepseek_key
OLLAMA_ENDPOINT=http://ollama:11434

# Data Sources
ALPHA_VANTAGE_API_KEY=your_alpha_vantage_key
COINGECKO_API_KEY=your_coingecko_key
NEWS_API_KEY=your_news_api_key
TWITTER_BEARER_TOKEN=your_twitter_bearer_token

# Security
JWT_SECRET_KEY=your_jwt_secret_key_here_make_it_long_and_random
JWT_ALGORITHM=HS256
JWT_EXPIRE_MINUTES=60

# Application Settings
ENVIRONMENT=development
LOG_LEVEL=INFO
DEBUG=true
CORS_ORIGINS=http://localhost:3000,http://localhost:8000

# Monitoring
GRAFANA_PASSWORD=admin
PROMETHEUS_RETENTION=30d

# Trading Configuration
DEFAULT_RISK_TOLERANCE=0.05
MAX_PORTFOLIO_RISK=0.15
DEFAULT_STOP_LOSS=0.05
DEFAULT_TAKE_PROFIT=0.15
MIN_TRADE_AMOUNT=10.00

# Feature Flags
ENABLE_AUTO_TRADING=false
ENABLE_PAPER_TRADING=true
ENABLE_RISK_ALERTS=true
ENABLE_EMAIL_NOTIFICATIONS=false

---
# ================================
# PRE-COMMIT CONFIGURATION
# ================================

# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-json
      - id: check-toml
      - id: check-xml
      - id: check-merge-conflict
      - id: debug-statements
      - id: check-docstring-first

  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black
        language_version: python3

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
        args: ["--profile", "black"]

  - repo: https://github.com/pycqa/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
        args: [--max-line-length=127, --extend-ignore=E203,W503]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.3.0
    hooks:
      - id: mypy
        additional_dependencies: [types-all]

  - repo: https://github.com/PyCQA/bandit
    rev: 1.7.5
    hook



    # AI Trading Bot - Complete Project Structure Setup
# Run this script to create the entire project structure

#!/bin/bash

echo "🚀 Creating AI Trading Bot Project Structure..."

# Main project directory
mkdir -p ai-trading-bot
cd ai-trading-bot

# Create main directories
mkdir -p {api,ai-engine,frontend,data-collector,infrastructure,tests,docs,scripts}

# API structure (FastAPI Backend)
mkdir -p api/{app,tests,migrations,scripts}
mkdir -p api/app/{routers,services,models,core,utils,middleware}
mkdir -p api/app/models/{database,schemas}
mkdir -p api/tests/{unit,integration,fixtures}

# AI Engine structure
mkdir -p ai-engine/{app,models,tests}
mkdir -p ai-engine/app/{analyzers,services,utils}

# Frontend structure (React Dashboard)
mkdir -p frontend/{src,public,tests}
mkdir -p frontend/src/{components,pages,services,utils,styles}

# Data Collector structure
mkdir -p data-collector/{app,collectors,tests}
mkdir -p data-collector/app/{news,social,market}

# Infrastructure
mkdir -p infrastructure/{k8s,monitoring,nginx,database}
mkdir -p infrastructure/k8s/{base,staging,production}
mkdir -p infrastructure/monitoring/{prometheus,grafana}

# GitHub Actions
mkdir -p .github/{workflows,templates}

# VSCode configuration
mkdir -p .vscode

# DevContainer
mkdir -p .devcontainer

echo "📁 Project structure created successfully!"

# List the structure
echo "📋 Project Structure:"
tree -I '__pycache__|*.pyc|node_modules|.git' -L 4


# ai-trading-bot/api/app/main.py
"""
AI Trading Bot - FastAPI Main Application
High-performance async API with comprehensive middleware and routing
"""

import time
import logging
from contextlib import asynccontextmanager
from typing import Dict, Any

import uvicorn
from fastapi import FastAPI, Request, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse
from fastapi.openapi.docs import get_swagger_ui_html
from fastapi.openapi.utils import get_openapi
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
from slowapi.middleware import SlowAPIMiddleware
import redis.asyncio as redis

from app.core.config import settings
from app.core.database import create_tables, close_db_connections
from app.core.logging import setup_logging
from app.middleware.security import SecurityHeadersMiddleware
from app.middleware.request_logging import RequestLoggingMiddleware
from app.middleware.error_handling import ErrorHandlingMiddleware

# Import all routers
from app.routers import (
    auth,
    portfolio,
    trading,
    market,
    ai_analysis,
    risk,
    settings as settings_router,
    websocket as ws_router,
    health
)

# Setup logging
logger = setup_logging(__name__)

# Rate limiter setup
limiter = Limiter(key_func=get_remote_address)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan events - startup and shutdown"""
    # Startup
    logger.info("🚀 Starting AI Trading Bot API...")
    
    try:
        # Initialize database
        await create_tables()
        logger.info("✅ Database tables created/verified")
        
        # Initialize Redis connection
        app.state.redis = redis.from_url(
            settings.REDIS_URL,
            encoding="utf-8",
            decode_responses=True
        )
        await app.state.redis.ping()
        logger.info("✅ Redis connection established")
        
        # Initialize background tasks
        # await start_background_tasks()
        logger.info("✅ Background tasks initialized")
        
        logger.info("🎯 AI Trading Bot API started successfully!")
        
    except Exception as e:
        logger.error(f"❌ Failed to start application: {e}")
        raise
    
    yield
    
    # Shutdown
    logger.info("🔄 Shutting down AI Trading Bot API...")
    
    try:
        # Close database connections
        await close_db_connections()
        logger.info("✅ Database connections closed")
        
        # Close Redis connection
        if hasattr(app.state, 'redis'):
            await app.state.redis.close()
            logger.info("✅ Redis connection closed")
        
        logger.info("👋 AI Trading Bot API shutdown complete")
        
    except Exception as e:
        logger.error(f"❌ Error during shutdown: {e}")

# Create FastAPI application
app = FastAPI(
    title="AI Trading Bot API",
    description="""
    🤖 **AI-Powered Trading Bot API**
    
    A sophisticated trading bot that uses multiple AI models to analyze markets 
    and execute profitable trades automatically.
    
    ## Features
    
    * **Multi-AI Analysis**: GPT-4.1, DeepSeek-R1, Ollama Gemini/Mistral
    * **Real-time Trading**: Bitpanda Pro API integration
    * **Risk Management**: Advanced portfolio protection
    * **Social Sentiment**: Twitter, Reddit, News analysis
    * **Austrian Tax Compliance**: KESt calculation support
    
    ## Authentication
    
    All endpoints require JWT authentication. Get your token from `/auth/login`.
    
    ## Rate Limits
    
    * Standard: 100 requests/minute
    * Trading: 50 requests/minute
    * AI Analysis: 20 requests/minute
    """,
    version="1.0.0",
    contact={
        "name": "AI Trading Bot Team",
        "email": "support@ai-trading-bot.com",
    },
    license_info={
        "name": "MIT License",
        "url": "https://opensource.org/licenses/MIT",
    },
    docs_url=None,  # Disable default docs
    redoc_url=None,  # Disable redoc
    lifespan=lifespan,
    # OpenAPI tags for grouping endpoints
    openapi_tags=[
        {"name": "Authentication", "description": "User authentication and authorization"},
        {"name": "Portfolio", "description": "Portfolio management and tracking"},
        {"name": "Trading", "description": "Order management and execution"},
        {"name": "Market Data", "description": "Real-time market prices and charts"},
        {"name": "AI Analysis", "description": "Multi-AI trading analysis"},
        {"name": "Risk Management", "description": "Portfolio risk monitoring"},
        {"name": "Settings", "description": "User preferences and configuration"},
        {"name": "WebSocket", "description": "Real-time data streams"},
        {"name": "Health", "description": "System health and monitoring"},
    ]
)

# ================================
# MIDDLEWARE SETUP
# ================================

# Rate limiting middleware
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)
app.add_middleware(SlowAPIMiddleware)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["X-Request-ID", "X-Rate-Limit-Remaining"]
)

# Security headers middleware
app.add_middleware(SecurityHeadersMiddleware)

# Trusted host middleware (prevents Host header attacks)
if settings.ENVIRONMENT == "production":
    app.add_middleware(
        TrustedHostMiddleware,
        allowed_hosts=settings.ALLOWED_HOSTS
    )

# Request logging middleware
app.add_middleware(RequestLoggingMiddleware)

# Global error handling middleware
app.add_middleware(ErrorHandlingMiddleware)

# ================================
# CUSTOM OPENAPI & DOCS
# ================================

def custom_openapi():
    """Custom OpenAPI schema with additional security definitions"""
    if app.openapi_schema:
        return app.openapi_schema
    
    openapi_schema = get_openapi(
        title=app.title,
        version=app.version,
        description=app.description,
        routes=app.routes,
    )
    
    # Add JWT security scheme
    openapi_schema["components"]["securitySchemes"] = {
        "BearerAuth": {
            "type": "http",
            "scheme": "bearer",
            "bearerFormat": "JWT",
            "description": "Enter JWT token in format: Bearer <token>"
        }
    }
    
    # Add global security requirement
    openapi_schema["security"] = [{"BearerAuth": []}]
    
    app.openapi_schema = openapi_schema
    return app.openapi_schema

app.openapi = custom_openapi

@app.get("/docs", include_in_schema=False)
async def custom_swagger_ui_html():
    """Custom Swagger UI with enhanced styling"""
    return get_swagger_ui_html(
        openapi_url=app.openapi_url,
        title=f"{app.title} - Documentation",
        oauth2_redirect_url=app.swagger_ui_oauth2_redirect_url,
        swagger_js_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui-bundle.js",
        swagger_css_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui.css",
        swagger_favicon_url="🤖",
    )

# ================================
# GLOBAL EXCEPTION HANDLERS
# ================================

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Enhanced HTTP exception handler with request context"""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": {
                "code": exc.status_code,
                "message": exc.detail,
                "timestamp": time.time(),
                "path": str(request.url.path),
                "method": request.method,
                "request_id": getattr(request.state, "request_id", None)
            }
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """Global exception handler for unhandled errors"""
    logger.error(
        f"Unhandled exception: {type(exc).__name__}: {exc}",
        extra={
            "path": str(request.url.path),
            "method": request.method,
            "user_agent": request.headers.get("user-agent"),
        },
        exc_info=True
    )
    
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": {
                "code": 500,
                "message": "Internal server error occurred",
                "timestamp": time.time(),
                "path": str(request.url.path),
                "method": request.method,
                "request_id": getattr(request.state, "request_id", None)
            }
        }
    )

# ================================
# ROOT ENDPOINTS
# ================================

@app.get("/", tags=["Root"])
async def root():
    """API root endpoint with basic information"""
    return {
        "message": "🤖 AI Trading Bot API",
        "version": app.version,
        "status": "operational",
        "timestamp": time.time(),
        "docs": "/docs",
        "health": "/health"
    }

@app.get("/info", tags=["Root"])
async def api_info():
    """Detailed API information and capabilities"""
    return {
        "api": {
            "name": app.title,
            "version": app.version,
            "environment": settings.ENVIRONMENT,
            "debug": settings.DEBUG
        },
        "features": {
            "multi_ai_analysis": True,
            "real_time_trading": True,
            "risk_management": True,
            "social_sentiment": True,
            "austrian_tax_support": True,
            "websocket_streaming": True
        },
        "endpoints": {
            "authentication": "/auth/*",
            "portfolio": "/portfolio/*",
            "trading": "/trading/*",
            "market_data": "/market/*",
            "ai_analysis": "/ai/*",
            "risk_management": "/risk/*",
            "websocket": "/ws/*"
        },
        "limits": {
            "rate_limit_per_minute": 100,
            "max_portfolio_positions": 50,
            "max_daily_trades": 100
        }
    }

# ================================
# ROUTER REGISTRATION
# ================================

# Health check router (no auth required)
app.include_router(
    health.router,
    prefix="/health",
    tags=["Health"]
)

# Authentication router (no auth required for login)
app.include_router(
    auth.router,
    prefix="/auth",
    tags=["Authentication"]
)

# Portfolio management
app.include_router(
    portfolio.router,
    prefix="/portfolio",
    tags=["Portfolio"],
    dependencies=[]  # Auth dependency added in router
)

# Trading operations
app.include_router(
    trading.router,
    prefix="/trading",
    tags=["Trading"]
)

# Market data
app.include_router(
    market.router,
    prefix="/market",
    tags=["Market Data"]
)

# AI analysis
app.include_router(
    ai_analysis.router,
    prefix="/ai",
    tags=["AI Analysis"]
)

# Risk management
app.include_router(
    risk.router,
    prefix="/risk",
    tags=["Risk Management"]
)

# User settings
app.include_router(
    settings_router.router,
    prefix="/settings",
    tags=["Settings"]
)

# WebSocket connections
app.include_router(
    ws_router.router,
    prefix="/ws",
    tags=["WebSocket"]
)

# ================================
# DEVELOPMENT HELPERS
# ================================

if settings.DEBUG:
    @app.get("/debug/redis", include_in_schema=False)
    async def debug_redis():
        """Debug endpoint to test Redis connection"""
        try:
            await app.state.redis.ping()
            info = await app.state.redis.info()
            return {
                "status": "connected",
                "redis_version": info.get("redis_version"),
                "used_memory": info.get("used_memory_human"),
                "connected_clients": info.get("connected_clients")
            }
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    @app.get("/debug/config", include_in_schema=False)
    async def debug_config():
        """Debug endpoint to view current configuration (sanitized)"""
        return {
            "environment": settings.ENVIRONMENT,
            "debug": settings.DEBUG,
            "database_url": settings.DATABASE_URL.replace(
                settings.DATABASE_URL.split("://")[1].split("@")[0], 
                "***:***"
            ),
            "redis_configured": bool(settings.REDIS_URL),
            "cors_origins": settings.CORS_ORIGINS,
            "log_level": settings.LOG_LEVEL
        }

# ================================
# APPLICATION STARTUP
# ================================

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.DEBUG,
        log_level=settings.LOG_LEVEL.lower(),
        access_log=True,
        reload_dirs=["app"] if settings.DEBUG else None,
        loop="uvloop" if not settings.DEBUG else "auto"  # uvloop for production performance
    )



    # ai-trading-bot/api/app/core/config.py
"""
Core Configuration Management
Environment-based settings with validation and type safety
"""

import secrets
import logging
from typing import Any, Dict, List, Optional, Union
from pathlib import Path

from pydantic import (
    BaseSettings, 
    PostgresDsn, 
    validator, 
    Field,
    SecretStr
)
from pydantic.networks import AnyHttpUrl

class Settings(BaseSettings):
    """Application settings with environment variable support"""
    
    # ================================
    # PROJECT METADATA
    # ================================
    PROJECT_NAME: str = "AI Trading Bot"
    PROJECT_VERSION: str = "1.0.0"
    API_V1_STR: str = "/api/v1"
    
    # ================================
    # ENVIRONMENT & DEBUG
    # ================================
    ENVIRONMENT: str = Field(default="development", env="ENVIRONMENT")
    DEBUG: bool = Field(default=False, env="DEBUG")
    LOG_LEVEL: str = Field(default="INFO", env="LOG_LEVEL")
    
    @validator("LOG_LEVEL")
    def validate_log_level(cls, v):
        valid_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
        if v.upper() not in valid_levels:
            raise ValueError(f"LOG_LEVEL must be one of {valid_levels}")
        return v.upper()
    
    # ================================
    # SECURITY SETTINGS
    # ================================
    SECRET_KEY: str = Field(default_factory=lambda: secrets.token_urlsafe(32), env="SECRET_KEY")
    JWT_SECRET_KEY: str = Field(default_factory=lambda: secrets.token_urlsafe(32), env="JWT_SECRET_KEY")
    JWT_ALGORITHM: str = Field(default="HS256", env="JWT_ALGORITHM")
    JWT_EXPIRE_MINUTES: int = Field(default=60, env="JWT_EXPIRE_MINUTES")
    JWT_REFRESH_EXPIRE_DAYS: int = Field(default=7, env="JWT_REFRESH_EXPIRE_DAYS")
    
    # Password hashing
    PASSWORD_MIN_LENGTH: int = 8
    PASSWORD_HASH_SCHEMES: List[str] = ["bcrypt"]
    PASSWORD_DEPRECATED: str = "auto"
    
    # ================================
    # DATABASE CONFIGURATION
    # ================================
    POSTGRES_HOST: str = Field(default="localhost", env="POSTGRES_HOST")
    POSTGRES_PORT: int = Field(default=5432, env="POSTGRES_PORT") 
    POSTGRES_USER: str = Field(default="trading_user", env="POSTGRES_USER")
    POSTGRES_PASSWORD: SecretStr = Field(env="POSTGRES_PASSWORD")
    POSTGRES_DB: str = Field(default="trading_bot", env="POSTGRES_DB")
    
    # Constructed DATABASE_URL (can be overridden)
    DATABASE_URL: Optional[PostgresDsn] = Field(default=None, env="DATABASE_URL")
    
    @validator("DATABASE_URL", pre=True)
    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> Any:
        if isinstance(v, str):
            return v
        
        # Build DATABASE_URL from components
        password = values.get("POSTGRES_PASSWORD")
        if isinstance(password, SecretStr):
            password = password.get_secret_value()
        
        return PostgresDsn.build(
            scheme="postgresql",
            user=values.get("POSTGRES_USER"),
            password=password,
            host=values.get("POSTGRES_HOST"),
            port=str(values.get("POSTGRES_PORT")),
            path=f"/{values.get('POSTGRES_DB') or ''}"
        )
    
    # Database pool settings
    DB_POOL_SIZE: int = Field(default=10, env="DB_POOL_SIZE")
    DB_MAX_OVERFLOW: int = Field(default=20, env="DB_MAX_OVERFLOW")
    DB_POOL_TIMEOUT: int = Field(default=30, env="DB_POOL_TIMEOUT")
    DB_POOL_RECYCLE: int = Field(default=3600, env="DB_POOL_RECYCLE")
    
    # ================================
    # REDIS CONFIGURATION
    # ================================
    REDIS_HOST: str = Field(default="localhost", env="REDIS_HOST")
    REDIS_PORT: int = Field(default=6379, env="REDIS_PORT")
    REDIS_PASSWORD: Optional[SecretStr] = Field(default=None, env="REDIS_PASSWORD")
    REDIS_DB: int = Field(default=0, env="REDIS_DB")
    REDIS_URL: Optional[str] = Field(default=None, env="REDIS_URL")
    
    @validator("REDIS_URL", pre=True)
    def assemble_redis_connection(cls, v: Optional[str], values: Dict[str, Any]) -> str:
        if isinstance(v, str):
            return v
        
        # Build REDIS_URL from components
        password = values.get("REDIS_PASSWORD")
        if password:
            password = password.get_secret_value()
            auth = f":{password}@"
        else:
            auth = ""
        
        return f"redis://{auth}{values.get('REDIS_HOST')}:{values.get('REDIS_PORT')}/{values.get('REDIS_DB')}"
    
    # ================================
    # MESSAGE BROKER (RABBITMQ)
    # ================================
    RABBITMQ_HOST: str = Field(default="localhost", env="RABBITMQ_HOST")
    RABBITMQ_PORT: int = Field(default=5672, env="RABBITMQ_PORT")
    RABBITMQ_USER: str = Field(default="guest", env="RABBITMQ_USER")
    RABBITMQ_PASSWORD: SecretStr = Field(default=SecretStr("guest"), env="RABBITMQ_PASSWORD")
    RABBITMQ_VHOST: str = Field(default="/", env="RABBITMQ_VHOST")
    RABBITMQ_URL: Optional[str] = Field(default=None, env="RABBITMQ_URL")
    
    @validator("RABBITMQ_URL", pre=True)
    def assemble_rabbitmq_connection(cls, v: Optional[str], values: Dict[str, Any]) -> str:
        if isinstance(v, str):
            return v
        
        password = values.get("RABBITMQ_PASSWORD")
        if isinstance(password, SecretStr):
            password = password.get_secret_value()
        
        return f"amqp://{values.get('RABBITMQ_USER')}:{password}@{values.get('RABBITMQ_HOST')}:{values.get('RABBITMQ_PORT')}{values.get('RABBITMQ_VHOST')}"
    
    # ================================
    # EXTERNAL APIS
    # ================================
    
    # Bitpanda API
    BITPANDA_API_KEY: Optional[SecretStr] = Field(default=None, env="BITPANDA_API_KEY")
    BITPANDA_API_SECRET: Optional[SecretStr] = Field(default=None, env="BITPANDA_API_SECRET")
    BITPANDA_SANDBOX: bool = Field(default=True, env="BITPANDA_SANDBOX")
    BITPANDA_BASE_URL: str = Field(
        default="https://api.exchange.bitpanda.com",
        env="BITPANDA_BASE_URL"
    )
    
    # Azure OpenAI
    AZURE_OPENAI_API_KEY: Optional[SecretStr] = Field(default=None, env="AZURE_OPENAI_API_KEY")
    AZURE_OPENAI_ENDPOINT: Optional[str] = Field(default=None, env="AZURE_OPENAI_ENDPOINT")
    AZURE_OPENAI_MODEL: str = Field(default="gpt-4", env="AZURE_OPENAI_MODEL")
    AZURE_OPENAI_VERSION: str = Field(default="2024-02-01", env="AZURE_OPENAI_VERSION")
    
    # DeepSeek API
    DEEPSEEK_API_KEY: Optional[SecretStr] = Field(default=None, env="DEEPSEEK_API_KEY")
    DEEPSEEK_BASE_URL: str = Field(default="https://api.deepseek.com", env="DEEPSEEK_BASE_URL")
    
    # Ollama
    OLLAMA_ENDPOINT: str = Field(default="http://localhost:11434", env="OLLAMA_ENDPOINT")
    OLLAMA_MODEL_GEMINI: str = Field(default="gemma:7b", env="OLLAMA_MODEL_GEMINI")
    OLLAMA_MODEL_MISTRAL: str = Field(default="mistral:7b", env="OLLAMA_MODEL_MISTRAL")
    
    # Market Data APIs
    ALPHA_VANTAGE_API_KEY: Optional[SecretStr] = Field(default=None, env="ALPHA_VANTAGE_API_KEY")
    COINGECKO_API_KEY: Optional[SecretStr] = Field(default=None, env="COINGECKO_API_KEY")
    NEWS_API_KEY: Optional[SecretStr] = Field(default=None, env="NEWS_API_KEY")
    
    # Social Media APIs
    TWITTER_BEARER_TOKEN: Optional[SecretStr] = Field(default=None, env="TWITTER_BEARER_TOKEN")
    REDDIT_CLIENT_ID: Optional[SecretStr] = Field(default=None, env="REDDIT_CLIENT_ID")
    REDDIT_CLIENT_SECRET: Optional[SecretStr] = Field(default=None, env="REDDIT_CLIENT_SECRET")
    
    # ================================
    # CORS & SECURITY
    # ================================
    CORS_ORIGINS: List[AnyHttpUrl] = Field(
        default=["http://localhost:3000", "http://localhost:8000"],
        env="CORS_ORIGINS"
    )
    
    @validator("CORS_ORIGINS", pre=True)
    def assemble_cors_origins(cls, v: Union[str, List[str]]) -> Union[List[str], str]:
        if isinstance(v, str) and not v.startswith("["):
            return [i.strip() for i in v.split(",")]
        elif isinstance(v, (list, str)):
            return v
        raise ValueError(v)
    
    ALLOWED_HOSTS: List[str] = Field(
        default=["localhost", "127.0.0.1", "0.0.0.0"],
        env="ALLOWED_HOSTS"
    )
    
    @validator("ALLOWED_HOSTS", pre=True)
    def assemble_allowed_hosts(cls, v: Union[str, List[str]]) -> List[str]:
        if isinstance(v, str):
            return [i.strip() for i in v.split(",")]
        return v
    
    # ================================
    # RATE LIMITING
    # ================================
    RATE_LIMIT_PER_MINUTE: int = Field(default=100, env="RATE_LIMIT_PER_MINUTE")
    RATE_LIMIT_TRADING_PER_MINUTE: int = Field(default=50, env="RATE_LIMIT_TRADING_PER_MINUTE")
    RATE_LIMIT_AI_PER_MINUTE: int = Field(default=20, env="RATE_LIMIT_AI_PER_MINUTE")
    
    # ================================
    # TRADING CONFIGURATION
    # ================================
    DEFAULT_RISK_TOLERANCE: float = Field(default=0.05, env="DEFAULT_RISK_TOLERANCE")  # 5%
    MAX_PORTFOLIO_RISK: float = Field(default=0.15, env="MAX_PORTFOLIO_RISK")  # 15%
    DEFAULT_STOP_LOSS: float = Field(default=0.05, env="DEFAULT_STOP_LOSS")  # 5%
    DEFAULT_TAKE_PROFIT: float = Field(default=0.15, env="DEFAULT_TAKE_PROFIT")  # 15%
    MIN_TRADE_AMOUNT: float = Field(default=10.00, env="MIN_TRADE_AMOUNT")  # 10 EUR
    MAX_DAILY_TRADES: int = Field(default=100, env="MAX_DAILY_TRADES")
    MAX_PORTFOLIO_POSITIONS: int = Field(default=50, env="MAX_PORTFOLIO_POSITIONS")
    
    @validator("DEFAULT_RISK_TOLERANCE", "MAX_PORTFOLIO_RISK", "DEFAULT_STOP_LOSS", "DEFAULT_TAKE_PROFIT")
    def validate_percentage(cls, v):
        if not 0 <= v <= 1:
            raise ValueError("Percentage values must be between 0 and 1")
        return v
    
    # ================================
    # FEATURE FLAGS
    # ================================
    ENABLE_AUTO_TRADING: bool = Field(default=False, env="ENABLE_AUTO_TRADING")
    ENABLE_PAPER_TRADING: bool = Field(default=True, env="ENABLE_PAPER_TRADING")
    ENABLE_RISK_ALERTS: bool = Field(default=True, env="ENABLE_RISK_ALERTS")
    ENABLE_EMAIL_NOTIFICATIONS: bool = Field(default=False, env="ENABLE_EMAIL_NOTIFICATIONS")
    ENABLE_WEBSOCKET: bool = Field(default=True, env="ENABLE_WEBSOCKET")
    ENABLE_AI_ANALYSIS: bool = Field(default=True, env="ENABLE_AI_ANALYSIS")
    
    # ================================
    # MONITORING & OBSERVABILITY
    # ================================
    ENABLE_METRICS: bool = Field(default=True, env="ENABLE_METRICS")
    METRICS_PORT: int = Field(default=9090, env="METRICS_PORT")
    JAEGER_AGENT_HOST: str = Field(default="localhost", env="JAEGER_AGENT_HOST")
    JAEGER_AGENT_PORT: int = Field(default=6831, env="JAEGER_AGENT_PORT")
    
    # ================================
    # FILE PATHS
    # ================================
    BASE_DIR: Path = Path(__file__).resolve().parent.parent.parent
    LOGS_DIR: Path = BASE_DIR / "logs"
    STATIC_DIR: Path = BASE_DIR / "static"
    TEMPLATES_DIR: Path = BASE_DIR / "templates"
    
    # ================================
    # EMAIL CONFIGURATION (Optional)
    # ================================
    SMTP_TLS: bool = Field(default=True, env="SMTP_TLS")
    SMTP_PORT: Optional[int] = Field(default=587, env="SMTP_PORT")
    SMTP_HOST: Optional[str] = Field(default=None, env="SMTP_HOST")
    SMTP_USER: Optional[str] = Field(default=None, env="SMTP_USER")
    SMTP_PASSWORD: Optional[SecretStr] = Field(default=None, env="SMTP_PASSWORD")
    
    # Email templates
    EMAILS_FROM_EMAIL: Optional[str] = Field(default=None, env="EMAILS_FROM_EMAIL")
    EMAILS_FROM_NAME: Optional[str] = Field(default=None, env="EMAILS_FROM_NAME")
    EMAIL_RESET_TOKEN_EXPIRE_HOURS: int = Field(default=48, env="EMAIL_RESET_TOKEN_EXPIRE_HOURS")
    
    # ================================
    # AUSTRIAN TAX SETTINGS
    # ================================
    AUSTRIA_CAPITAL_GAINS_TAX: float = Field(default=0.275, env="AUSTRIA_CAPITAL_GAINS_TAX")  # 27.5%
    CRYPTO_SPECULATION_PERIOD_DAYS: int = Field(default=365, env="CRYPTO_SPECULATION_PERIOD_DAYS")  # 1 year
    
    # ================================
    # WORKER & BACKGROUND TASKS
    # ================================
    CELERY_BROKER_URL: Optional[str] = Field(default=None, env="CELERY_BROKER_URL")
    CELERY_RESULT_BACKEND: Optional[str] = Field(default=None, env="CELERY_RESULT_BACKEND")
    
    @validator("CELERY_BROKER_URL", pre=True)
    def assemble_celery_broker(cls, v: Optional[str], values: Dict[str, Any]) -> Optional[str]:
        if v:
            return v
        # Use RabbitMQ URL as default broker
        return values.get("RABBITMQ_URL")
    
    @validator("CELERY_RESULT_BACKEND", pre=True)  
    def assemble_celery_backend(cls, v: Optional[str], values: Dict[str, Any]) -> Optional[str]:
        if v:
            return v
        # Use Redis URL as default result backend
        return values.get("REDIS_URL")
    
    # ================================
    # WEBSOCKET SETTINGS
    # ================================
    WEBSOCKET_HEARTBEAT_INTERVAL: int = Field(default=30, env="WEBSOCKET_HEARTBEAT_INTERVAL")
    WEBSOCKET_MAX_CONNECTIONS: int = Field(default=1000, env="WEBSOCKET_MAX_CONNECTIONS")
    WEBSOCKET_MESSAGE_QUEUE_SIZE: int = Field(default=100, env="WEBSOCKET_MESSAGE_QUEUE_SIZE")
    
    # ================================
    # AI ANALYSIS SETTINGS
    # ================================
    AI_ANALYSIS_CACHE_TTL: int = Field(default=300, env="AI_ANALYSIS_CACHE_TTL")  # 5 minutes
    AI_CONSENSUS_THRESHOLD: float = Field(default=0.7, env="AI_CONSENSUS_THRESHOLD")  # 70%
    AI_MAX_CONCURRENT_REQUESTS: int = Field(default=10, env="AI_MAX_CONCURRENT_REQUESTS")
    
    # ================================
    # DATA COLLECTION SETTINGS
    # ================================
    MARKET_DATA_UPDATE_INTERVAL: int = Field(default=60, env="MARKET_DATA_UPDATE_INTERVAL")  # seconds
    SENTIMENT_UPDATE_INTERVAL: int = Field(default=300, env="SENTIMENT_UPDATE_INTERVAL")  # 5 minutes
    NEWS_UPDATE_INTERVAL: int = Field(default=600, env="NEWS_UPDATE_INTERVAL")  # 10 minutes
    
    # ================================
    # VALIDATION METHODS
    # ================================
    
    def get_database_url(self) -> str:
        """Get database URL as string"""
        return str(self.DATABASE_URL)
    
    def get_redis_url(self) -> str:
        """Get Redis URL as string"""
        return str(self.REDIS_URL)
    
    def is_production(self) -> bool:
        """Check if running in production environment"""
        return self.ENVIRONMENT.lower() == "production"
    
    def is_development(self) -> bool:
        """Check if running in development environment"""
        return self.ENVIRONMENT.lower() == "development"
    
    def get_secret_value(self, field_name: str) -> Optional[str]:
        """Safely get secret value by field name"""
        field_value = getattr(self, field_name, None)
        if isinstance(field_value, SecretStr):
            return field_value.get_secret_value()
        return field_value
    
    # ================================
    # CONFIGURATION CLASS SETTINGS
    # ================================
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = True
        # Allow extra fields for future extensibility
        extra = "allow"

# Create global settings instance
settings = Settings()

# Create logs directory if it doesn't exist
settings.LOGS_DIR.mkdir(exist_ok=True)

# Validate critical settings on import
def validate_critical_settings():
    """Validate that critical settings are properly configured"""
    errors = []
    
    # Check database configuration
    if not settings.DATABASE_URL:
        errors.append("DATABASE_URL is not configured")
    
    # Check Redis configuration
    if not settings.REDIS_URL:
        errors.append("REDIS_URL is not configured")
    
    # Check JWT secret
    if not settings.JWT_SECRET_KEY or len(settings.JWT_SECRET_KEY) < 32:
        errors.append("JWT_SECRET_KEY must be at least 32 characters long")
    
    # Check production-specific settings
    if settings.is_production():
        if settings.DEBUG:
            errors.append("DEBUG should be False in production")
        
        if not settings.get_secret_value("BITPANDA_API_KEY"):
            errors.append("BITPANDA_API_KEY is required in production")
    
    if errors:
        error_msg = "Configuration validation failed:\n" + "\n".join(f"- {error}" for error in errors)
        raise ValueError(error_msg)

# Validate on import (can be disabled by setting SKIP_CONFIG_VALIDATION=1)
import os
if not os.getenv("SKIP_CONFIG_VALIDATION"):
    validate_critical_settings()

# Export settings instance
__all__ = ["settings", "Settings"]





1# ai-trading-bot/api/app/models/database/__init__.py
"""
Database Models Module
Exports all database models for easy importing
"""

from .base import Base
from .user import User
from .portfolio import Portfolio
from .asset import Asset
from .position import Position
from .order import Order
from .ai_analysis import AIAnalysis
from .risk_alert import RiskAlert
from .system_config import SystemConfig

# Time-series models (TimescaleDB)
from .market_data import MarketData
from .portfolio_history import PortfolioHistory
from .price_update import PriceUpdate
from .sentiment_data import SentimentData
from .system_metrics import SystemMetrics

__all__ = [
    "Base",
    "User",
    "Portfolio", 
    "Asset",
    "Position",
    "Order",
    "AIAnalysis",
    "RiskAlert",
    "SystemConfig",
    "MarketData",
    "PortfolioHistory",
    "PriceUpdate",
    "SentimentData",
    "SystemMetrics"
]

# ================================
# ai-trading-bot/api/app/models/database/base.py
"""
SQLAlchemy Base Model with common functionality
"""

import uuid
from datetime import datetime
from typing import Any, Dict

from sqlalchemy import Column, DateTime, String
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.ext.declarative import declarative_base, declared_attr
from sqlalchemy.orm import Session


class BaseModel:
    """Base model with common fields and methods"""
    
    @declared_attr
    def __tablename__(cls) -> str:
        """Generate table name from class name"""
        return cls.__name__.lower() + 's'
    
    # Common fields for all models
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4, index=True)
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert model instance to dictionary"""
        return {
            column.name: getattr(self, column.name)
            for column in self.__table__.columns
        }
    
    def update(self, db: Session, **kwargs) -> None:
        """Update model instance with provided values"""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
        self.updated_at = datetime.utcnow()
        db.commit()
        db.refresh(self)
    
    def delete(self, db: Session) -> None:
        """Delete model instance"""
        db.delete(self)
        db.commit()
    
    def __repr__(self) -> str:
        return f"<{self.__class__.__name__}(id={self.id})>"

# Create declarative base
Base = declarative_base(cls=BaseModel)

# ================================
# ai-trading-bot/api/app/models/database/user.py
"""
User Model - User accounts and authentication
"""

from sqlalchemy import Boolean, Column, String, Numeric, DateTime
from sqlalchemy.orm import relationship

from .base import Base


class User(Base):
    """User model for authentication and profile management"""
    __tablename__ = "users"
    
    # Basic user information
    username = Column(String(50), unique=True, nullable=False, index=True)
    email = Column(String(255), unique=True, nullable=False, index=True)
    password_hash = Column(String(255), nullable=False)
    
    # Account status
    is_active = Column(Boolean, default=True, nullable=False)
    is_verified = Column(Boolean, default=False, nullable=False)
    is_superuser = Column(Boolean, default=False, nullable=False)
    
    # Authentication tracking
    last_login = Column(DateTime, nullable=True)
    failed_login_attempts = Column(Integer, default=0)
    locked_until = Column(DateTime, nullable=True)
    
    # Trading configuration
    risk_tolerance = Column(Numeric(3, 2), default=0.05, nullable=False)  # 5%
    max_portfolio_risk = Column(Numeric(3, 2), default=0.15, nullable=False)  # 15%
    auto_trading_enabled = Column(Boolean, default=False, nullable=False)
    paper_trading_mode = Column(Boolean, default=True, nullable=False)
    
    # Austrian tax settings
    tax_residence = Column(String(2), default='AT', nullable=False)
    tax_id = Column(String(50), nullable=True)
    
    # Preferences
    preferred_currency = Column(String(3), default='EUR', nullable=False)
    timezone = Column(String(50), default='Europe/Vienna', nullable=False)
    
    # Relationships
    portfolios = relationship("Portfolio", back_populates="user", cascade="all, delete-orphan")
    risk_alerts = relationship("RiskAlert", back_populates="user", cascade="all, delete-orphan")
    
    def __repr__(self) -> str:
        return f"<User(username='{self.username}', email='{self.email}')>"
    
    @property
    def is_locked(self) -> bool:
        """Check if user account is locked"""
        if self.locked_until:
            return datetime.utcnow() < self.locked_until
        return False

# ================================
# ai-trading-bot/api/app/models/database/asset.py
"""
Asset Model - Tradeable assets (stocks, crypto, ETFs, etc.)
"""

from sqlalchemy import Column, String, Boolean, BigInteger, Enum as SQLEnum
from sqlalchemy.orm import relationship
from enum import Enum

from .base import Base


class AssetType(str, Enum):
    """Asset type enumeration"""
    CRYPTO = "crypto"
    STOCK = "stock"
    ETF = "etf"
    COMMODITY = "commodity"
    FOREX = "forex"


class Asset(Base):
    """Asset model for tradeable instruments"""
    __tablename__ = "assets"
    
    # Basic asset information
    symbol = Column(String(20), unique=True, nullable=False, index=True)
    name = Column(String(100), nullable=False)
    asset_type = Column(SQLEnum(AssetType), nullable=False, index=True)
    exchange = Column(String(50), nullable=True)
    
    # Asset status
    is_active = Column(Boolean, default=True, nullable=False, index=True)
    is_tradeable = Column(Boolean, default=True, nullable=False)
    
    # Asset metadata
    sector = Column(String(50), nullable=True)
    market_cap = Column(BigInteger, nullable=True)
    description = Column(String(500), nullable=True)
    
    # External identifiers
    external_id = Column(String(100), nullable=True)  # Bitpanda instrument ID
    isin = Column(String(12), nullable=True)  # For stocks/ETFs
    
    # Relationships
    positions = relationship("Position", back_populates="asset")
    orders = relationship("Order", back_populates="asset")
    ai_analyses = relationship("AIAnalysis", back_populates="asset")
    market_data = relationship("MarketData", back_populates="asset")
    price_updates = relationship("PriceUpdate", back_populates="asset")
    sentiment_data = relationship("SentimentData", back_populates="asset")
    
    def __repr__(self) -> str:
        return f"<Asset(symbol='{self.symbol}', type='{self.asset_type}')>"

# ================================
# ai-trading-bot/api/app/models/database/portfolio.py
"""
Portfolio Model - User portfolio management
"""

from sqlalchemy import Column, String, Numeric, ForeignKey
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship

from .base import Base


class Portfolio(Base):
    """Portfolio model for managing user investments"""
    __tablename__ = "portfolios"
    
    # Portfolio ownership
    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id", ondelete="CASCADE"), nullable=False, index=True)
    name = Column(String(100), nullable=False, default="Main Portfolio")
    
    # Portfolio balances
    initial_balance = Column(Numeric(15, 2), nullable=False)
    current_balance = Column(Numeric(15, 2), nullable=False)
    total_invested = Column(Numeric(15, 2), default=0, nullable=False)
    total_profit_loss = Column(Numeric(15, 2), default=0, nullable=False)
    
    # Portfolio settings
    currency = Column(String(3), default='EUR', nullable=False)
    is_active = Column(Boolean, default=True, nullable=False)
    
    # Relationships
    user = relationship("User", back_populates="portfolios")
    positions = relationship("Position", back_populates="portfolio", cascade="all, delete-orphan")
    orders = relationship("Order", back_populates="portfolio", cascade="all, delete-orphan")
    portfolio_history = relationship("PortfolioHistory", back_populates="portfolio")
    
    def __repr__(self) -> str:
        return f"<Portfolio(name='{self.name}', balance={self.current_balance})>"
    
    @property
    def total_value(self) -> float:
        """Calculate total portfolio value (cash + investments)"""
        return float(self.current_balance + self.total_invested)
    
    @property
    def profit_loss_percentage(self) -> float:
        """Calculate profit/loss percentage"""
        if self.initial_balance > 0:
            return float(self.total_profit_loss / self.initial_balance * 100)
        return 0.0

# ================================
# ai-trading-bot/api/app/models/database/position.py
"""
Position Model - Trading positions
"""

from sqlalchemy import Column, Numeric, String, DateTime, ForeignKey, Enum as SQLEnum
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship
from enum import Enum

from .base import Base


class PositionStatus(str, Enum):
    """Position status enumeration"""
    OPEN = "open"
    CLOSED = "closed"
    PARTIAL = "partial"


class Position(Base):
    """Position model for tracking investment positions"""
    __tablename__ = "positions"
    
    # Position ownership
    portfolio_id = Column(UUID(as_uuid=True), ForeignKey("portfolios.id", ondelete="CASCADE"), nullable=False, index=True)
    asset_id = Column(UUID(as_uuid=True), ForeignKey("assets.id"), nullable=False, index=True)
    
    # Position details
    quantity = Column(Numeric(20, 8), nullable=False)
    avg_buy_price = Column(Numeric(15, 8), nullable=False)
    current_price = Column(Numeric(15, 8), nullable=True)
    
    # P&L calculation
    unrealized_pnl = Column(Numeric(15, 2), default=0, nullable=False)
    realized_pnl = Column(Numeric(15, 2), default=0, nullable=False)
    
    # Position management
    status = Column(SQLEnum(PositionStatus), default=PositionStatus.OPEN, nullable=False, index=True)
    opened_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    closed_at = Column(DateTime, nullable=True)
    
    # Risk management
    stop_loss_price = Column(Numeric(15, 8), nullable=True)
    take_profit_price = Column(Numeric(15, 8), nullable=True)
    
    # Relationships
    portfolio = relationship("Portfolio", back_populates="positions")
    asset = relationship("Asset", back_populates="positions")
    orders = relationship("Order", back_populates="position")
    
    def __repr__(self) -> str:
        return f"<Position(asset={self.asset.symbol if self.asset else 'Unknown'}, quantity={self.quantity})>"
    
    @property
    def market_value(self) -> float:
        """Calculate current market value of position"""
        if self.current_price:
            return float(self.quantity * self.current_price)
        return float(self.quantity * self.avg_buy_price)
    
    @property
    def unrealized_pnl_percentage(self) -> float:
        """Calculate unrealized P&L percentage"""
        if self.current_price and self.avg_buy_price > 0:
            return float((self.current_price - self.avg_buy_price) / self.avg_buy_price * 100)
        return 0.0

# ================================
# ai-trading-bot/api/app/models/database/order.py
"""
Order Model - Trading orders
"""

from sqlalchemy import Column, Numeric, String, DateTime, ForeignKey, Enum as SQLEnum
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship
from enum import Enum

from .base import Base


class OrderType(str, Enum):
    """Order type enumeration"""
    BUY = "buy"
    SELL = "sell"
    STOP_LOSS = "stop_loss"
    TAKE_PROFIT = "take_profit"
    MARKET = "market"
    LIMIT = "limit"


class OrderStatus(str, Enum):
    """Order status enumeration"""
    PENDING = "pending"
    EXECUTED = "executed"
    CANCELLED = "cancelled"
    FAILED = "failed"
    PARTIAL = "partial"


class Order(Base):
    """Order model for trading order management"""
    __tablename__ = "orders"
    
    # Order ownership
    portfolio_id = Column(UUID(as_uuid=True), ForeignKey("portfolios.id", ondelete="CASCADE"), nullable=False, index=True)
    position_id = Column(UUID(as_uuid=True), ForeignKey("positions.id"), nullable=True)  # NULL for opening positions
    asset_id = Column(UUID(as_uuid=True), ForeignKey("assets.id"), nullable=False, index=True)
    
    # Order details
    order_type = Column(SQLEnum(OrderType), nullable=False)
    quantity = Column(Numeric(20, 8), nullable=False)
    price = Column(Numeric(15, 8), nullable=True)  # NULL for market orders
    stop_price = Column(Numeric(15, 8), nullable=True)  # For stop orders
    
    # Order execution
    status = Column(SQLEnum(OrderStatus), default=OrderStatus.PENDING, nullable=False, index=True)
    executed_quantity = Column(Numeric(20, 8), default=0, nullable=False)
    executed_price = Column(Numeric(15, 8), nullable=True)
    
    # Timestamps
    executed_at = Column(DateTime, nullable=True)
    cancelled_at = Column(DateTime, nullable=True)
    expires_at = Column(DateTime, nullable=True)  # Order expiration
    
    # External reference
    external_order_id = Column(String(100), nullable=True, index=True)  # Bitpanda order ID
    
    # Fees and costs
    fee_amount = Column(Numeric(15, 8), default=0, nullable=False)
    fee_currency = Column(String(10), nullable=True)
    
    # Order metadata
    notes = Column(String(500), nullable=True)
    source = Column(String(50), default="manual", nullable=False)  # manual, ai, stop_loss, etc.
    
    # Relationships
    portfolio = relationship("Portfolio", back_populates="orders")
    position = relationship("Position", back_populates="orders")
    asset = relationship("Asset", back_populates="orders")
    
    def __repr__(self) -> str:
        return f"<Order(type={self.order_type}, asset={self.asset.symbol if self.asset else 'Unknown'}, quantity={self.quantity})>"
    
    @property
    def total_value(self) -> float:
        """Calculate total order value"""
        price = self.executed_price or self.price or 0
        return float(self.quantity * price)
    
    @property
    def is_executed(self) -> bool:
        """Check if order is fully executed"""
        return self.status == OrderStatus.EXECUTED
    
    @property
    def is_pending(self) -> bool:
        """Check if order is pending execution"""
        return self.status == OrderStatus.PENDING

# ================================
# ai-trading-bot/api/app/models/database/ai_analysis.py
"""
AI Analysis Model - AI trading analysis results
"""

from sqlalchemy import Column, String, Numeric, Text, DateTime, ForeignKey, Enum as SQLEnum
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.orm import relationship
from enum import Enum

from .base import Base


class AnalysisType(str, Enum):
    """Analysis type enumeration"""
    FUNDAMENTAL = "fundamental"
    TECHNICAL = "technical"
    SENTIMENT = "sentiment"
    CONSENSUS = "consensus"


class Recommendation(str, Enum):
    """Trading recommendation enumeration"""
    BUY = "BUY"
    SELL = "SELL"
    HOLD = "HOLD"


class AIAnalysis(Base):
    """AI Analysis model for storing AI trading analysis"""
    __tablename__ = "ai_analyses"
    
    # Analysis target
    asset_id = Column(UUID(as_uuid=True), ForeignKey("assets.id"), nullable=False, index=True)
    
    # Analysis details
    analysis_type = Column(SQLEnum(AnalysisType), nullable=False, index=True)
    ai_model = Column(String(50), nullable=False)  # gpt-4.1, deepseek-r1, gemini, etc.
    
    # Analysis results
    recommendation = Column(SQLEnum(Recommendation), nullable=False)
    confidence_score = Column(Numeric(5, 4), nullable=False)  # 0.0000 to 1.0000
    target_price = Column(Numeric(15, 8), nullable=True)
    reasoning = Column(Text, nullable=True)
    
    # Key indicators (flexible JSON storage)
    indicators = Column(JSONB, nullable=True)
    
    # Analysis metadata
    expires_at = Column(DateTime, nullable=True)  # When analysis becomes stale
    market_conditions = Column(String(100), nullable=True)  # bullish, bearish, sideways
    
    # Performance tracking
    prediction_accuracy = Column(Numeric(5, 4), nullable=True)  # Retroactively filled
    
    # Relationships
    asset = relationship("Asset", back_populates="ai_analyses")
    
    def __repr__(self) -> str:
        return f"<AIAnalysis(type={self.analysis_type}, asset={self.asset.symbol if self.asset else 'Unknown'}, recommendation={self.recommendation})>"
    
    @property
    def is_expired(self) -> bool:
        """Check if analysis is expired"""
        if self.expires_at:
            return datetime.utcnow() > self.expires_at
        return False
    
    @property
    def confidence_percentage(self) -> float:
        """Get confidence score as percentage"""
        return float(self.confidence_score * 100)

# ================================
# ai-trading-bot/api/app/models/database/risk_alert.py
"""
Risk Alert Model - Risk management alerts
"""

from sqlalchemy import Column, String, Numeric, Text, Boolean, DateTime, ForeignKey, Enum as SQLEnum
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship
from enum import Enum

from .base import Base


class AlertType(str, Enum):
    """Alert type enumeration"""
    DRAWDOWN = "drawdown"
    CONCENTRATION = "concentration"
    VOLATILITY = "volatility"
    STOP_LOSS = "stop_loss"
    MARGIN_CALL = "margin_call"
    POSITION_SIZE = "position_size"


class AlertSeverity(str, Enum):
    """Alert severity enumeration"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class RiskAlert(Base):
    """Risk Alert model for risk management notifications"""
    __tablename__ = "risk_alerts"
    
    # Alert ownership
    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id", ondelete="CASCADE"), nullable=False, index=True)
    portfolio_id = Column(UUID(as_uuid=True), ForeignKey("portfolios.id", ondelete="CASCADE"), nullable=True, index=True)
    
    # Alert details
    alert_type = Column(SQLEnum(AlertType), nullable=False, index=True)
    severity = Column(SQLEnum(AlertSeverity), nullable=False, index=True)
    message = Column(Text, nullable=False)
    
    # Alert data
    current_value = Column(Numeric(15, 8), nullable=True)
    threshold_value = Column(Numeric(15, 8), nullable=True)
    
    # Alert status
    is_active = Column(Boolean, default=True, nullable=False, index=True)
    acknowledged_at = Column(DateTime, nullable=True)
    resolved_at = Column(DateTime, nullable=True)
    
    # Relationships
    user = relationship("User", back_populates="risk_alerts")
    portfolio = relationship("Portfolio")
    
    def __repr__(self) -> str:
        return f"<RiskAlert(type={self.alert_type}, severity={self.severity})>"
    
    @property
    def is_resolved(self) -> bool:
        """Check if alert is resolved"""
        return self.resolved_at is not None
    
    @property
    def is_acknowledged(self) -> bool:
        """Check if alert is acknowledged"""
        return self.acknowledged_at is not None

# ================================
# ai-trading-bot/api/app/models/database/system_config.py
"""
System Configuration Model - Application settings
"""

from sqlalchemy import Column, String, Text, DateTime, ForeignKey
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship

from .base import Base


class SystemConfig(Base):
    """System Configuration model for application settings"""
    __tablename__ = "system_config"
    
    # Configuration key (primary identifier)
    key = Column(String(100), unique=True, nullable=False, index=True)
    value = Column(Text, nullable=False)
    description = Co



    # ai-trading-bot/api/app/models/database/market_data.py
"""
Market Data Model - OHLCV time-series data (TimescaleDB)
"""

from sqlalchemy import Column, String, Numeric, Integer, DateTime, ForeignKey, PrimaryKeyConstraint
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship

from .base import Base


class MarketData(Base):
    """Market Data model for OHLCV time-series data"""
    __tablename__ = "market_data"
    
    # Time-series primary key
    time = Column(DateTime, nullable=False, primary_key=True)
    asset_id = Column(UUID(as_uuid=True), ForeignKey("assets.id"), nullable=False, primary_key=True, index=True)
    timeframe = Column(String(10), nullable=False, primary_key=True)  # 1m, 5m, 1h, 1d, etc.
    
    # OHLCV data
    open_price = Column(Numeric(15, 8), nullable=False)
    high_price = Column(Numeric(15, 8), nullable=False)
    low_price = Column(Numeric(15, 8), nullable=False)
    close_price = Column(Numeric(15, 8), nullable=False)
    volume = Column(Numeric(20, 8), nullable=False)
    
    # Additional metrics
    volume_quote = Column(Numeric(20, 8), nullable=True)  # Volume in quote currency
    trades_count = Column(Integer, nullable=True)
    
    # Technical indicators (calculated on-the-fly or cached)
    sma_20 = Column(Numeric(15, 8), nullable=True)  # Simple Moving Average 20
    ema_12 = Column(Numeric(15, 8), nullable=True)  # Exponential Moving Average 12
    rsi_14 = Column(Numeric(5, 2), nullable=True)   # RSI 14
    
    # Relationships
    asset = relationship("Asset", back_populates="market_data")
    
    # Override the default id and timestamps from Base since we use composite PK
    __table_args__ = (
        PrimaryKeyConstraint('time', 'asset_id', 'timeframe'),
        {'timescaledb_hypertable': {'time_column_name': 'time'}}
    )
    
    def __repr__(self) -> str:
        return f"<MarketData(time={self.time}, asset={self.asset.symbol if self.asset else 'Unknown'}, close={self.close_price})>"
    
    @property
    def price_change(self) -> float:
        """Calculate price change from open to close"""
        return float(self.close_price - self.open_price)
    
    @property
    def price_change_percentage(self) -> float:
        """Calculate percentage price change"""
        if self.open_price > 0:
            return float((self.close_price - self.open_price) / self.open_price * 100)
        return 0.0

# ================================
# ai-trading-bot/api/app/models/database/portfolio_history.py
"""
Portfolio History Model - Portfolio value over time (TimescaleDB)
"""

from sqlalchemy import Column, Numeric, DateTime, ForeignKey, PrimaryKeyConstraint
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship

from .base import Base


class PortfolioHistory(Base):
    """Portfolio History model for tracking portfolio performance over time"""
    __tablename__ = "portfolio_history"
    
    # Time-series primary key
    time = Column(DateTime, nullable=False, primary_key=True)
    portfolio_id = Column(UUID(as_uuid=True), ForeignKey("portfolios.id", ondelete="CASCADE"), nullable=False, primary_key=True, index=True)
    
    # Portfolio metrics
    total_value = Column(Numeric(15, 2), nullable=False)
    cash_balance = Column(Numeric(15, 2), nullable=False)
    invested_value = Column(Numeric(15, 2), nullable=False)
    unrealized_pnl = Column(Numeric(15, 2), nullable=False)
    realized_pnl = Column(Numeric(15, 2), nullable=False)
    
    # Performance metrics
    daily_return = Column(Numeric(8, 6), nullable=True)
    sharpe_ratio = Column(Numeric(8, 4), nullable=True)
    max_drawdown = Column(Numeric(8, 4), nullable=True)
    volatility = Column(Numeric(8, 4), nullable=True)
    
    # Risk metrics
    value_at_risk = Column(Numeric(15, 2), nullable=True)  # VaR 95%
    beta = Column(Numeric(6, 4), nullable=True)  # Beta vs market
    
    # Relationships
    portfolio = relationship("Portfolio", back_populates="portfolio_history")
    
    # Override the default id and timestamps from Base since we use composite PK
    __table_args__ = (
        PrimaryKeyConstraint('time', 'portfolio_id'),
        {'timescaledb_hypertable': {'time_column_name': 'time'}}
    )
    
    def __repr__(self) -> str:
        return f"<PortfolioHistory(time={self.time}, value={self.total_value})>"

# ================================
# ai-trading-bot/api/app/models/database/price_update.py
"""
Price Update Model - Real-time price updates (TimescaleDB)
"""

from sqlalchemy import Column, Numeric, Integer, DateTime, ForeignKey, PrimaryKeyConstraint
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship

from .base import Base


class PriceUpdate(Base):
    """Price Update model for real-time price streaming"""
    __tablename__ = "price_updates"
    
    # Time-series primary key
    time = Column(DateTime, nullable=False, primary_key=True, default=datetime.utcnow)
    asset_id = Column(UUID(as_uuid=True), ForeignKey("assets.id"), nullable=False, primary_key=True, index=True)
    
    # Price data
    price = Column(Numeric(15, 8), nullable=False)
    volume_24h = Column(Numeric(20, 8), nullable=True)
    change_24h = Column(Numeric(15, 8), nullable=True)
    change_percent_24h = Column(Numeric(8, 4), nullable=True)
    
    # Market data
    market_cap = Column(Numeric(20, 2), nullable=True)
    rank = Column(Integer, nullable=True)
    
    # Volume weighted average price
    vwap = Column(Numeric(15, 8), nullable=True)
    
    # Bid/Ask spread (for forex/stocks)
    bid_price = Column(Numeric(15, 8), nullable=True)
    ask_price = Column(Numeric(15, 8), nullable=True)
    spread = Column(Numeric(15, 8), nullable=True)
    
    # Relationships
    asset = relationship("Asset", back_populates="price_updates")
    
    # Override the default id and timestamps from Base since we use composite PK
    __table_args__ = (
        PrimaryKeyConstraint('time', 'asset_id'),
        {'timescaledb_hypertable': {'time_column_name': 'time'}}
    )
    
    def __repr__(self) -> str:
        return f"<PriceUpdate(time={self.time}, asset={self.asset.symbol if self.asset else 'Unknown'}, price={self.price})>"
    
    @property
    def bid_ask_spread_percentage(self) -> float:
        """Calculate bid-ask spread as percentage"""
        if self.bid_price and self.ask_price and self.ask_price > 0:
            return float((self.ask_price - self.bid_price) / self.ask_price * 100)
        return 0.0

# ================================
# ai-trading-bot/api/app/models/database/sentiment_data.py
"""
Sentiment Data Model - Social media sentiment analysis (TimescaleDB)
"""

from sqlalchemy import Column, Numeric, Integer, DateTime, ForeignKey, PrimaryKeyConstraint
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship

from .base import Base


class SentimentData(Base):
    """Sentiment Data model for social media sentiment tracking"""
    __tablename__ = "sentiment_data"
    
    # Time-series primary key
    time = Column(DateTime, nullable=False, primary_key=True, default=datetime.utcnow)
    asset_id = Column(UUID(as_uuid=True), ForeignKey("assets.id"), nullable=False, primary_key=True, index=True)
    
    # Sentiment scores (-1 to 1, where -1 is very negative, 1 is very positive)
    twitter_sentiment = Column(Numeric(5, 4), nullable=True)
    reddit_sentiment = Column(Numeric(5, 4), nullable=True)
    news_sentiment = Column(Numeric(5, 4), nullable=True)
    overall_sentiment = Column(Numeric(5, 4), nullable=True)
    
    # Volume metrics (number of mentions)
    twitter_mentions = Column(Integer, default=0, nullable=False)
    reddit_mentions = Column(Integer, default=0, nullable=False)
    news_articles = Column(Integer, default=0, nullable=False)
    
    # Fear & Greed Index (0-100, where 0 is extreme fear, 100 is extreme greed)
    fear_greed_index = Column(Integer, nullable=True)
    
    # Trending metrics
    trending_score = Column(Numeric(5, 2), nullable=True)
    hashtag_count = Column(Integer, default=0, nullable=False)
    
    # Relationships
    asset = relationship("Asset", back_populates="sentiment_data")
    
    # Override the default id and timestamps from Base since we use composite PK
    __table_args__ = (
        PrimaryKeyConstraint('time', 'asset_id'),
        {'timescaledb_hypertable': {'time_column_name': 'time'}}
    )
    
    def __repr__(self) -> str:
        return f"<SentimentData(time={self.time}, asset={self.asset.symbol if self.asset else 'Unknown'}, sentiment={self.overall_sentiment})>"
    
    @property
    def total_mentions(self) -> int:
        """Calculate total mentions across all platforms"""
        return (self.twitter_mentions or 0) + (self.reddit_mentions or 0) + (self.news_articles or 0)
    
    @property
    def sentiment_strength(self) -> str:
        """Get sentiment strength description"""
        if self.overall_sentiment is None:
            return "neutral"
        
        sentiment = float(self.overall_sentiment)
        if sentiment >= 0.5:
            return "very_positive"
        elif sentiment >= 0.2:
            return "positive"
        elif sentiment >= -0.2:
            return "neutral"
        elif sentiment >= -0.5:
            return "negative"
        else:
            return "very_negative"

# ================================
# ai-trading-bot/api/app/models/database/system_metrics.py
"""
System Metrics Model - Application performance metrics (TimescaleDB)
"""

from sqlalchemy import Column, String, Numeric, Text, DateTime, PrimaryKeyConstraint
from sqlalchemy.dialects.postgresql import JSONB

from .base import Base


class SystemMetrics(Base):
    """System Metrics model for application monitoring"""
    __tablename__ = "system_metrics"
    
    # Time-series primary key
    time = Column(DateTime, nullable=False, primary_key=True, default=datetime.utcnow)
    metric_name = Column(String(100), nullable=False, primary_key=True, index=True)
    
    # Metric values (flexible storage)
    value = Column(Numeric(15, 8), nullable=True)
    string_value = Column(Text, nullable=True)
    
    # Labels for grouping metrics (JSON for flexibility)
    labels = Column(JSONB, nullable=True)
    
    # Override the default id and timestamps from Base since we use composite PK
    __table_args__ = (
        PrimaryKeyConstraint('time', 'metric_name'),
        {'timescaledb_hypertable': {'time_column_name': 'time'}}
    )
    
    def __repr__(self) -> str:
        return f"<SystemMetrics(time={self.time}, metric={self.metric_name}, value={self.value})>"


# ================================
# Helper function to create TimescaleDB hypertables
# ================================

async def create_hypertables(engine):
    """Create TimescaleDB hypertables for time-series tables"""
    
    hypertable_queries = [
        # Market data hypertable
        """
        SELECT create_hypertable('market_data', 'time', 
                                if_not_exists => TRUE,
                                chunk_time_interval => INTERVAL '1 day');
        """,
        
        # Portfolio history hypertable  
        """
        SELECT create_hypertable('portfolio_history', 'time',
                                if_not_exists => TRUE,
                                chunk_time_interval => INTERVAL '1 day');
        """,
        
        # Price updates hypertable
        """
        SELECT create_hypertable('price_updates', 'time',
                                if_not_exists => TRUE,
                                chunk_time_interval => INTERVAL '1 hour');
        """,
        
        # Sentiment data hypertable
        """
        SELECT create_hypertable('sentiment_data', 'time',
                                if_not_exists => TRUE,
                                chunk_time_interval => INTERVAL '6 hours');
        """,
        
        # System metrics hypertable
        """
        SELECT create_hypertable('system_metrics', 'time',
                                if_not_exists => TRUE,
                                chunk_time_interval => INTERVAL '1 hour');
        """
    ]
    
    async with engine.begin() as conn:
        for query in hypertable_queries:
            try:
                await conn.execute(text(query))
                print(f"✅ Hypertable created: {query.split("'")[1]}")
            except Exception as e:
                print(f"⚠️ Hypertable creation failed: {e}")


# ================================
# Data retention policies
# ================================

async def create_retention_policies(engine):
    """Create data retention policies for time-series data"""
    
    retention_queries = [
        # Keep detailed market data for 1 year
        """
        SELECT add_retention_policy('market_data', INTERVAL '1 year', if_not_exists => TRUE);
        """,
        
        # Keep sentiment data for 6 months
        """
        SELECT add_retention_policy('sentiment_data', INTERVAL '6 months', if_not_exists => TRUE);
        """,
        
        # Keep system metrics for 3 months
        """
        SELECT add_retention_policy('system_metrics', INTERVAL '3 months', if_not_exists => TRUE);
        """,
        
        # Portfolio history is kept indefinitely for compliance
        # Price updates kept for 1 year
        """
        SELECT add_retention_policy('price_updates', INTERVAL '1 year', if_not_exists => TRUE);
        """
    ]
    
    async with engine.begin() as conn:
        for query in retention_queries:
            try:
                await conn.execute(text(query))
                print(f"✅ Retention policy created")
            except Exception as e:
                print(f"⚠️ Retention policy creation failed: {e}")


# ================================
# Continuous aggregates for performance
# ================================

async def create_continuous_aggregates(engine):
    """Create continuous aggregates for better query performance"""
    
    aggregate_queries = [
        # Hourly market data aggregates
        """
        CREATE MATERIALIZED VIEW IF NOT EXISTS market_data_hourly
        WITH (timescaledb.continuous) AS
        SELECT 
            time_bucket('1 hour', time) AS hour,
            asset_id,
            first(open_price, time) as open_price,
            max(high_price) as high_price,
            min(low_price) as low_price,
            last(close_price, time) as close_price,
            sum(volume) as volume
        FROM market_data
        WHERE timeframe = '1m'
        GROUP BY hour, asset_id;
        """,
        
        # Daily portfolio performance
        """
        CREATE MATERIALIZED VIEW IF NOT EXISTS portfolio_daily_performance
        WITH (timescaledb.continuous) AS
        SELECT 
            time_bucket('1 day', time) AS day,
            portfolio_id,
            last(total_value, time) as total_value,
            last(unrealized_pnl, time) as unrealized_pnl,
            last(realized_pnl, time) as realized_pnl,
            avg(daily_return) as avg_daily_return,
            max(max_drawdown) as max_drawdown
        FROM portfolio_history
        GROUP BY day, portfolio_id;
        """
    ]
    
    async with engine.begin() as conn:
        for query in aggregate_queries:
            try:
                await conn.execute(text(query))
                print(f"✅ Continuous aggregate created")
            except Exception as e:
                print(f"⚠️ Continuous aggregate creation failed: {e}")


# Export all time-series models
__all__ = [
    "MarketData",
    "PortfolioHistory", 
    "PriceUpdate",
    "SentimentData",
    "SystemMetrics",
    "create_hypertables",
    "create_retention_policies",
    "create_continuous_aggregates"
]




# ai-trading-bot/api/app/core/database.py
"""
Database Connection & Session Management
Async SQLAlchemy with connection pooling and TimescaleDB support
"""

import logging
from typing import AsyncGenerator, Optional
from contextlib import asynccontextmanager

from sqlalchemy import create_engine, text, event
from sqlalchemy.ext.asyncio import (
    AsyncSession,
    async_sessionmaker,
    create_async_engine,
    AsyncEngine
)
from sqlalchemy.pool import QueuePool
from sqlalchemy.orm import Session

from app.core.config import settings
from app.models.database import Base
from app.models.database.timeseries import (
    create_hypertables,
    create_retention_policies,
    create_continuous_aggregates
)

logger = logging.getLogger(__name__)

# Global database engine and session maker
async_engine: Optional[AsyncEngine] = None
async_session_maker: Optional[async_sessionmaker] = None

# ================================
# DATABASE ENGINE SETUP
# ================================

def create_database_engine() -> AsyncEngine:
    """Create async database engine with optimized connection pooling"""
    
    # Convert sync DATABASE_URL to async
    database_url = str(settings.DATABASE_URL).replace(
        "postgresql://", "postgresql+asyncpg://"
    )
    
    logger.info(f"Creating database engine for: {database_url.split('@')[1] if '@' in database_url else 'localhost'}")
    
    engine = create_async_engine(
        database_url,
        
        # Connection pool settings
        poolclass=QueuePool,
        pool_size=settings.DB_POOL_SIZE,
        max_overflow=settings.DB_MAX_OVERFLOW,
        pool_timeout=settings.DB_POOL_TIMEOUT,
        pool_recycle=settings.DB_POOL_RECYCLE,
        pool_pre_ping=True,  # Validate connections before use
        
        # Async settings
        future=True,
        echo=settings.DEBUG,  # Log SQL queries in debug mode
        echo_pool=settings.DEBUG,  # Log connection pool events
        
        # Performance optimizations
        connect_args={
            "server_settings": {
                "application_name": f"ai_trading_bot_{settings.ENVIRONMENT}",
                "jit": "off",  # Disable JIT for faster simple queries
            },
            "command_timeout": 60,
            "statement_cache_size": 0,  # Disable statement cache for high-throughput
        }
    )
    
    return engine

# ================================
# SESSION MANAGEMENT
# ================================

def create_session_maker(engine: AsyncEngine) -> async_sessionmaker:
    """Create async session maker"""
    return async_sessionmaker(
        engine,
        class_=AsyncSession,
        expire_on_commit=False,
        autoflush=True,
        autocommit=False
    )

async def get_session() -> AsyncGenerator[AsyncSession, None]:
    """
    Dependency to get database session
    Use this in FastAPI endpoints: db: AsyncSession = Depends(get_session)
    """
    if not async_session_maker:
        raise RuntimeError("Database not initialized. Call init_database() first.")
    
    async with async_session_maker() as session:
        try:
            yield session
        except Exception as e:
            await session.rollback()
            logger.error(f"Database session error: {e}")
            raise
        finally:
            await session.close()

@asynccontextmanager
async def get_session_context() -> AsyncGenerator[AsyncSession, None]:
    """
    Context manager for database sessions
    Use this in services: async with get_session_context() as db:
    """
    if not async_session_maker:
        raise RuntimeError("Database not initialized. Call init_database() first.")
    
    async with async_session_maker() as session:
        try:
            yield session
        except Exception as e:
            await session.rollback()
            logger.error(f"Database session error: {e}")
            raise
        finally:
            await session.close()

# ================================
# DATABASE INITIALIZATION
# ================================

async def init_database() -> None:
    """Initialize database engine and session maker"""
    global async_engine, async_session_maker
    
    try:
        # Create engine
        async_engine = create_database_engine()
        async_session_maker = create_session_maker(async_engine)
        
        logger.info("✅ Database engine initialized successfully")
        
    except Exception as e:
        logger.error(f"❌ Failed to initialize database: {e}")
        raise

async def create_tables() -> None:
    """Create all database tables and TimescaleDB setup"""
    if not async_engine:
        raise RuntimeError("Database engine not initialized")
    
    try:
        # Create all tables
        async with async_engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        
        logger.info("✅ Database tables created successfully")
        
        # Setup TimescaleDB hypertables
        await create_hypertables(async_engine)
        logger.info("✅ TimescaleDB hypertables created")
        
        # Setup retention policies
        await create_retention_policies(async_engine)
        logger.info("✅ Data retention policies created")
        
        # Setup continuous aggregates
        await create_continuous_aggregates(async_engine)
        logger.info("✅ Continuous aggregates created")
        
        # Insert default system configuration
        await insert_default_config()
        logger.info("✅ Default system configuration inserted")
        
    except Exception as e:
        logger.error(f"❌ Failed to create tables: {e}")
        raise

async def close_db_connections() -> None:
    """Close all database connections"""
    global async_engine
    
    if async_engine:
        await async_engine.dispose()
        logger.info("✅ Database connections closed")

# ================================
# HEALTH CHECKS
# ================================

async def check_database_health() -> dict:
    """Check database connection and basic functionality"""
    if not async_engine:
        return {
            "status": "error",
            "message": "Database engine not initialized",
            "details": {}
        }
    
    try:
        async with async_engine.begin() as conn:
            # Test basic connection
            result = await conn.execute(text("SELECT 1 as health_check"))
            health_check = result.scalar()
            
            # Check TimescaleDB extension
            timescale_result = await conn.execute(
                text("SELECT extname FROM pg_extension WHERE extname = 'timescaledb'")
            )
            timescale_installed = timescale_result.scalar() is not None
            
            # Get database statistics
            stats_result = await conn.execute(text("""
                SELECT 
                    pg_database_size(current_database()) as db_size,
                    (SELECT count(*) FROM information_schema.tables WHERE table_schema = 'public') as table_count,
                    (SELECT count(*) FROM pg_stat_activity WHERE state = 'active') as active_connections
            """))
            stats = stats_result.first()
            
            return {
                "status": "healthy",
                "message": "Database connection successful",
                "details": {
                    "health_check": health_check == 1,
                    "timescaledb_installed": timescale_installed,
                    "database_size": stats.db_size if stats else 0,
                    "table_count": stats.table_count if stats else 0,
                    "active_connections": stats.active_connections if stats else 0,
                    "pool_size": async_engine.pool.size(),
                    "checked_out_connections": async_engine.pool.checkedout()
                }
            }
            
    except Exception as e:
        logger.error(f"Database health check failed: {e}")
        return {
            "status": "unhealthy",
            "message": f"Database health check failed: {str(e)}",
            "details": {}
        }

# ================================
# DATABASE UTILITIES
# ================================

async def execute_sql_file(file_path: str) -> None:
    """Execute SQL commands from file"""
    if not async_engine:
        raise RuntimeError("Database engine not initialized")
    
    try:
        with open(file_path, 'r') as file:
            sql_commands = file.read()
        
        async with async_engine.begin() as conn:
            # Split and execute each statement
            for statement in sql_commands.split(';'):
                statement = statement.strip()
                if statement:
                    await conn.execute(text(statement))
        
        logger.info(f"✅ SQL file executed successfully: {file_path}")
        
    except Exception as e:
        logger.error(f"❌ Failed to execute SQL file {file_path}: {e}")
        raise

async def backup_database(backup_path: str) -> None:
    """Create database backup using pg_dump"""
    import subprocess
    import asyncio
    
    try:
        # Extract connection details from DATABASE_URL
        db_url = str(settings.DATABASE_URL)
        
        # pg_dump command
        cmd = [
            "pg_dump",
            "--no-password",
            "--format=custom",
            "--compress=9",
            "--file", backup_path,
            db_url
        ]
        
        # Run pg_dump asynchronously
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await process.communicate()
        
        if process.returncode == 0:
            logger.info(f"✅ Database backup created: {backup_path}")
        else:
            logger.error(f"❌ Database backup failed: {stderr.decode()}")
            raise RuntimeError(f"pg_dump failed: {stderr.decode()}")
            
    except Exception as e:
        logger.error(f"❌ Database backup error: {e}")
        raise

# ================================
# DEFAULT DATA INSERTION
# ================================

async def insert_default_config() -> None:
    """Insert default system configuration"""
    from app.models.database import SystemConfig
    
    default_configs = [
        {
            "key": "max_daily_trades",
            "value": "100",
            "description": "Maximum number of trades per day per user"
        },
        {
            "key": "min_trade_amount",
            "value": "10.00", 
            "description": "Minimum trade amount in EUR"
        },
        {
            "key": "max_position_size",
            "value": "0.20",
            "description": "Maximum position size as percentage of portfolio"
        },
        {
            "key": "stop_loss_default",
            "value": "0.05",
            "description": "Default stop loss percentage"
        },
        {
            "key": "take_profit_default", 
            "value": "0.15",
            "description": "Default take profit percentage"
        },
        {
            "key": "ai_analysis_enabled",
            "value": "true",
            "description": "Enable AI analysis features"
        },
        {
            "key": "risk_alerts_enabled",
            "value": "true", 
            "description": "Enable risk management alerts"
        },
        {
            "key": "paper_trading_default",
            "value": "true",
            "description": "Enable paper trading mode by default for new users"
        }
    ]
    
    try:
        async with get_session_context() as session:
            for config_data in default_configs:
                # Check if config already exists
                existing = await session.get(SystemConfig, config_data["key"])
                if not existing:
                    config = SystemConfig(**config_data)
                    session.add(config)
            
            await session.commit()
            logger.info(f"✅ Default system configuration inserted ({len(default_configs)} items)")
            
    except Exception as e:
        logger.error(f"❌ Failed to insert default configuration: {e}")
        raise

async def insert_default_assets() -> None:
    """Insert default tradeable assets"""
    from app.models.database import Asset, AssetType
    
    default_assets = [
        # Cryptocurrencies
        {"symbol": "BTC", "name": "Bitcoin", "asset_type": AssetType.CRYPTO, "exchange": "bitpanda"},
        {"symbol": "ETH", "name": "Ethereum", "asset_type": AssetType.CRYPTO, "exchange": "bitpanda"},
        {"symbol": "ADA", "name": "Cardano", "asset_type": AssetType.CRYPTO, "exchange": "bitpanda"},
        {"symbol": "DOT", "name": "Polkadot", "asset_type": AssetType.CRYPTO, "exchange": "bitpanda"},
        {"symbol": "SOL", "name": "Solana", "asset_type": AssetType.CRYPTO, "exchange": "bitpanda"},
        
        # Stocks
        {"symbol": "AAPL", "name": "Apple Inc.", "asset_type": AssetType.STOCK, "exchange": "nasdaq", "sector": "Technology"},
        {"symbol": "GOOGL", "name": "Alphabet Inc.", "asset_type": AssetType.STOCK, "exchange": "nasdaq", "sector": "Technology"},
        {"symbol": "MSFT", "name": "Microsoft Corporation", "asset_type": AssetType.STOCK, "exchange": "nasdaq", "sector": "Technology"},
        {"symbol": "TSLA", "name": "Tesla Inc.", "asset_type": AssetType.STOCK, "exchange": "nasdaq", "sector": "Automotive"},
        {"symbol": "NVDA", "name": "NVIDIA Corporation", "asset_type": AssetType.STOCK, "exchange": "nasdaq", "sector": "Technology"},
        
        # ETFs
        {"symbol": "SPY", "name": "SPDR S&P 500 ETF", "asset_type": AssetType.ETF, "exchange": "nyse"},
        {"symbol": "QQQ", "name": "Invesco QQQ Trust", "asset_type": AssetType.ETF, "exchange": "nasdaq"},
        {"symbol": "VTI", "name": "Vanguard Total Stock Market ETF", "asset_type": AssetType.ETF, "exchange": "nyse"},
        
        # Commodities
        {"symbol": "GOLD", "name": "Gold", "asset_type": AssetType.COMMODITY, "exchange": "lbma"},
        {"symbol": "SILVER", "name": "Silver", "asset_type": AssetType.COMMODITY, "exchange": "lbma"},
    ]
    
    try:
        async with get_session_context() as session:
            for asset_data in default_assets:
                # Check if asset already exists
                existing = await session.execute(
                    text("SELECT id FROM assets WHERE symbol = :symbol"),
                    {"symbol": asset_data["symbol"]}
                )
                if not existing.scalar():
                    asset = Asset(**asset_data)
                    session.add(asset)
            
            await session.commit()
            logger.info(f"✅ Default assets inserted ({len(default_assets)} items)")
            
    except Exception as e:
        logger.error(f"❌ Failed to insert default assets: {e}")
        raise

# ================================
# DATABASE EVENTS & HOOKS
# ================================

@event.listens_for(AsyncEngine, "connect")
def set_postgresql_search_path(dbapi_connection, connection_record):
    """Set PostgreSQL search path and timezone on connection"""
    with dbapi_connection.cursor() as cursor:
        # Set search path
        cursor.execute("SET search_path TO public")
        # Set timezone
        cursor.execute("SET timezone TO 'UTC'")
        # Enable TimescaleDB if not already enabled
        cursor.execute("CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE")

@event.listens_for(AsyncEngine, "before_cursor_execute")
def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    """Log slow queries in debug mode"""
    if settings.DEBUG:
        context._query_start_time = time.time()

@event.listens_for(AsyncEngine, "after_cursor_execute") 
def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    """Log query execution time in debug mode"""
    if settings.DEBUG and hasattr(context, '_query_start_time'):
        total = time.time() - context._query_start_time
        if total > 0.1:  # Log queries taking more than 100ms
            logger.warning(f"Slow query ({total:.3f}s): {statement[:100]}...")

# ================================
# EXPORTS
# ================================

__all__ = [
    "init_database",
    "create_tables", 
    "close_db_connections",
    "get_session",
    "get_session_context",
    "check_database_health",
    "execute_sql_file",
    "backup_database",
    "insert_default_config",
    "insert_default_assets"
]




# ai-trading-bot/api/app/core/auth.py
"""
JWT Authentication & Authorization System
Secure user authentication with password hashing and token management
"""

import logging
from datetime import datetime, timedelta
from typing import Any, Dict, Optional, Union

import bcrypt
import jwt
from fastapi import HTTPException, status
from passlib.context import CryptContext

from app.core.config import settings

logger = logging.getLogger(__name__)

# ================================
# PASSWORD HASHING
# ================================

class PasswordManager:
    """Secure password hashing and verification using bcrypt"""
    
    def __init__(self):
        self.pwd_context = CryptContext(
            schemes=settings.PASSWORD_HASH_SCHEMES,
            deprecated=settings.PASSWORD_DEPRECATED
        )
    
    def hash_password(self, password: str) -> str:
        """Hash a password using bcrypt"""
        if not password or len(password) < settings.PASSWORD_MIN_LENGTH:
            raise ValueError(f"Password must be at least {settings.PASSWORD_MIN_LENGTH} characters long")
        
        return self.pwd_context.hash(password)
    
    def verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """Verify a password against its hash"""
        try:
            return self.pwd_context.verify(plain_password, hashed_password)
        except Exception as e:
            logger.error(f"Password verification error: {e}")
            return False
    
    def needs_update(self, hashed_password: str) -> bool:
        """Check if password hash needs update (deprecated algorithm)"""
        return self.pwd_context.needs_update(hashed_password)

# Global password manager instance
password_manager = PasswordManager()

# ================================
# JWT TOKEN MANAGEMENT
# ================================

class JWTManager:
    """JWT token creation and validation"""
    
    def __init__(self):
        self.secret_key = settings.JWT_SECRET_KEY
        self.algorithm = settings.JWT_ALGORITHM
        self.access_token_expire_minutes = settings.JWT_EXPIRE_MINUTES
        self.refresh_token_expire_days = settings.JWT_REFRESH_EXPIRE_DAYS
    
    def create_access_token(
        self, 
        subject: Union[str, Any], 
        expires_delta: Optional[timedelta] = None,
        additional_claims: Optional[Dict[str, Any]] = None
    ) -> str:
        """Create JWT access token"""
        
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(minutes=self.access_token_expire_minutes)
        
        # Base payload
        payload = {
            "exp": expire,
            "iat": datetime.utcnow(),
            "sub": str(subject),
            "type": "access"
        }
        
        # Add additional claims
        if additional_claims:
            payload.update(additional_claims)
        
        try:
            encoded_jwt = jwt.encode(payload, self.secret_key, algorithm=self.algorithm)
            logger.debug(f"Access token created for subject: {subject}")
            return encoded_jwt
            
        except Exception as e:
            logger.error(f"Failed to create access token: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Could not create access token"
            )
    
    def create_refresh_token(
        self, 
        subject: Union[str, Any],
        expires_delta: Optional[timedelta] = None
    ) -> str:
        """Create JWT refresh token"""
        
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(days=self.refresh_token_expire_days)
        
        payload = {
            "exp": expire,
            "iat": datetime.utcnow(),
            "sub": str(subject),
            "type": "refresh"
        }
        
        try:
            encoded_jwt = jwt.encode(payload, self.secret_key, algorithm=self.algorithm)
            logger.debug(f"Refresh token created for subject: {subject}")
            return encoded_jwt
            
        except Exception as e:
            logger.error(f"Failed to create refresh token: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Could not create refresh token"
            )
    
    def verify_token(self, token: str) -> Dict[str, Any]:
        """Verify and decode JWT token"""
        try:
            payload = jwt.decode(
                token, 
                self.secret_key, 
                algorithms=[self.algorithm]
            )
            
            # Check token type
            token_type = payload.get("type")
            if not token_type:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid token: missing type"
                )
            
            # Check expiration
            exp = payload.get("exp")
            if exp and datetime.utcfromtimestamp(exp) < datetime.utcnow():
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Token has expired"
                )
            
            return payload
            
        except jwt.ExpiredSignatureError:
            logger.warning("Token expired")
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Token has expired"
            )
        
        except jwt.InvalidTokenError as e:
            logger.warning(f"Invalid token: {e}")
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token"
            )
        
        except Exception as e:
            logger.error(f"Token verification error: {e}")
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Could not validate token"
            )
    
    def refresh_access_token(self, refresh_token: str) -> str:
        """Create new access token from refresh token"""
        payload = self.verify_token(refresh_token)
        
        # Verify it's a refresh token
        if payload.get("type") != "refresh":
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid refresh token"
            )
        
        # Extract user ID and create new access token
        user_id = payload.get("sub")
        if not user_id:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid refresh token: missing subject"
            )
        
        return self.create_access_token(subject=user_id)

# Global JWT manager instance
jwt_manager = JWTManager()

# ================================
# USER AUTHENTICATION SERVICE
# ================================

class AuthService:
    """User authentication business logic"""
    
    def __init__(self):
        self.password_manager = password_manager
        self.jwt_manager = jwt_manager
    
    async def authenticate_user(
        self, 
        db, 
        username_or_email: str, 
        password: str
    ) -> Optional["User"]:
        """Authenticate user with username/email and password"""
        from app.models.database import User
        from sqlalchemy import or_
        
        try:
            # Find user by username or email
            result = await db.execute(
                select(User).where(
                    or_(
                        User.username == username_or_email,
                        User.email == username_or_email
                    )
                )
            )
            user = result.scalar_one_or_none()
            
            if not user:
                logger.warning(f"Authentication failed: user not found for {username_or_email}")
                return None
            
            # Check if account is active
            if not user.is_active:
                logger.warning(f"Authentication failed: inactive account for {username_or_email}")
                return None
            
            # Check if account is locked
            if user.is_locked:
                logger.warning(f"Authentication failed: locked account for {username_or_email}")
                raise HTTPException(
                    status_code=status.HTTP_423_LOCKED,
                    detail=f"Account is locked until {user.locked_until}"
                )
            
            # Verify password
            if not self.password_manager.verify_password(password, user.password_hash):
                # Increment failed login attempts
                user.failed_login_attempts += 1
                
                # Lock account after too many failed attempts
                if user.failed_login_attempts >= 5:
                    user.locked_until = datetime.utcnow() + timedelta(minutes=30)
                    logger.warning(f"Account locked due to too many failed attempts: {username_or_email}")
                
                await db.commit()
                logger.warning(f"Authentication failed: invalid password for {username_or_email}")
                return None
            
            # Reset failed login attempts on successful login
            user.failed_login_attempts = 0
            user.locked_until = None
            user.last_login = datetime.utcnow()
            await db.commit()
            
            logger.info(f"User authenticated successfully: {username_or_email}")
            return user
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Authentication error: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Authentication service error"
            )
    
    async def create_user_tokens(self, user: "User") -> Dict[str, str]:
        """Create access and refresh tokens for user"""
        
        # Additional claims for the token
        additional_claims = {
            "username": user.username,
            "email": user.email,
            "is_superuser": user.is_superuser,
            "is_verified": user.is_verified
        }
        
        access_token = self.jwt_manager.create_access_token(
            subject=user.id,
            additional_claims=additional_claims
        )
        
        refresh_token = self.jwt_manager.create_refresh_token(subject=user.id)
        
        return {
            "access_token": access_token,
            "refresh_token": refresh_token,
            "token_type": "bearer",
            "expires_in": settings.JWT_EXPIRE_MINUTES * 60  # seconds
        }
    
    async def get_current_user(self, db, token: str) -> "User":
        """Get current user from JWT token"""
        from app.models.database import User
        
        # Verify token
        payload = self.jwt_manager.verify_token(token)
        
        # Extract user ID
        user_id = payload.get("sub")
        if not user_id:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token: missing user ID"
            )
        
        # Get user from database
        user = await db.get(User, user_id)
        if not user:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="User not found"
            )
        
        # Check if user is still active
        if not user.is_active:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Inactive user"
            )
        
        return user
    
    async def register_user(
        self, 
        db,
        username: str,
        email: str,
        password: str,
        **kwargs
    ) -> "User":
        """Register a new user"""
        from app.models.database import User
        
        try:
            # Check if user already exists
            existing_user = await db.execute(
                select(User).where(
                    or_(User.username == username, User.email == email)
                )
            )
            if existing_user.scalar_one_or_none():
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Username or email already registered"
                )
            
            # Hash password
            hashed_password = self.password_manager.hash_password(password)
            
            # Create user
            user = User(
                username=username,
                email=email,
                password_hash=hashed_password,
                **kwargs
            )
            
            db.add(user)
            await db.commit()
            await db.refresh(user)
            
            logger.info(f"New user registered: {username} ({email})")
            return user
            
        except HTTPException:
            raise
        except Exception as e:
            await db.rollback()
            logger.error(f"User registration error: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="User registration failed"
            )
    
    async def change_password(
        self, 
        db,
        user: "User",
        current_password: str,
        new_password: str
    ) -> bool:
        """Change user password"""
        
        try:
            # Verify current password
            if not self.password_manager.verify_password(current_password, user.password_hash):
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Invalid current password"
                )
            
            # Hash new password
            new_hashed_password = self.password_manager.hash_password(new_password)
            
            # Update user
            user.password_hash = new_hashed_password
            await db.commit()
            
            logger.info(f"Password changed for user: {user.username}")
            return True
            
        except HTTPException:
            raise
        except Exception as e:
            await db.rollback()
            logger.error(f"Password change error: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Password change failed"
            )

# Global auth service instance
auth_service = AuthService()

# ================================
# SECURITY UTILITIES
# ================================

def generate_password_reset_token(email: str) -> str:
    """Generate password reset token"""
    delta = timedelta(hours=settings.EMAIL_RESET_TOKEN_EXPIRE_HOURS)
    now = datetime.utcnow()
    expires = now + delta
    
    payload = {
        "exp": expires,
        "iat": now,
        "sub": email,
        "type": "password_reset"
    }
    
    return jwt.encode(payload, settings.JWT_SECRET_KEY, algorithm=settings.JWT_ALGORITHM)

def verify_password_reset_token(token: str) -> Optional[str]:
    """Verify password reset token and return email"""
    try:
        payload = jwt.decode(token, settings.JWT_SECRET_KEY, algorithms=[settings.JWT_ALGORITHM])
        
        if payload.get("type") != "password_reset":
            return None
        
        email: str = payload.get("sub")
        return email
        
    except jwt.PyJWTError:
        return None

def generate_email_verification_token(email: str) -> str:
    """Generate email verification token"""
    delta = timedelta(days=7)  # 7 days to verify email
    now = datetime.utcnow()
    expires = now + delta
    
    payload = {
        "exp": expires,
        "iat": now,
        "sub": email,
        "type": "email_verification"
    }
    
    return jwt.encode(payload, settings.JWT_SECRET_KEY, algorithm=settings.JWT_ALGORITHM)

def verify_email_verification_token(token: str) -> Optional[str]:
    """Verify email verification token and return email"""
    try:
        payload = jwt.decode(token, settings.JWT_SECRET_KEY, algorithms=[settings.JWT_ALGORITHM])
        
        if payload.get("type") != "email_verification":
            return None
        
        email: str = payload.get("sub")
        return email
        
    except jwt.PyJWTError:
        return None

# ================================
# PERMISSION CHECKING
# ================================

class PermissionChecker:
    """Check user permissions and roles"""
    
    @staticmethod
    def check_superuser(user: "User") -> bool:
        """Check if user is superuser"""
        return user.is_superuser
    
    @staticmethod
    def check_active(user: "User") -> bool:
        """Check if user is active"""
        return user.is_active and not user.is_locked
    
    @staticmethod
    def check_verified(user: "User") -> bool:
        """Check if user email is verified"""
        return user.is_verified
    
    @staticmethod
    def check_portfolio_owner(user: "User", portfolio: "Portfolio") -> bool:
        """Check if user owns the portfolio"""
        return str(user.id) == str(portfolio.user_id)
    
    @staticmethod
    def check_trading_enabled(user: "User") -> bool:
        """Check if user has trading enabled"""
        return user.is_active and user.is_verified and not user.is_locked

# ================================
# SECURITY DECORATORS
# ================================

def require_superuser(func):
    """Decorator to require superuser permission"""
    def wrapper(current_user: "User", *args, **kwargs):
        if not PermissionChecker.check_superuser(current_user):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Superuser permission required"
            )
        return func(current_user, *args, **kwargs)
    return wrapper

def require_verified_user(func):
    """Decorator to require verified user"""
    def wrapper(current_user: "User", *args, **kwargs):
        if not PermissionChecker.check_verified(current_user):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Email verification required"
            )
        return func(current_user, *args, **kwargs)
    return wrapper

# ================================
# EXPORTS
# ================================

__all__ = [
    "password_manager",
    "jwt_manager", 
    "auth_service",
    "PermissionChecker",
    "generate_password_reset_token",
    "verify_password_reset_token",
    "generate_email_verification_token",
    "verify_email_verification_token",
    "require_superuser",
    "require_verified_user"
]



# ai-trading-bot/api/app/core/auth.py
"""
JWT Authentication & Authorization System
Secure user authentication with password hashing and token management
"""

import logging
from datetime import datetime, timedelta
from typing import Any, Dict, Optional, Union

import bcrypt
import jwt
from fastapi import HTTPException, status
from passlib.context import CryptContext

from app.core.config import settings

logger = logging.getLogger(__name__)

# ================================
# PASSWORD HASHING
# ================================

class PasswordManager:
    """Secure password hashing and verification using bcrypt"""
    
    def __init__(self):
        self.pwd_context = CryptContext(
            schemes=settings.PASSWORD_HASH_SCHEMES,
            deprecated=settings.PASSWORD_DEPRECATED
        )
    
    def hash_password(self, password: str) -> str:
        """Hash a password using bcrypt"""
        if not password or len(password) < settings.PASSWORD_MIN_LENGTH:
            raise ValueError(f"Password must be at least {settings.PASSWORD_MIN_LENGTH} characters long")
        
        return self.pwd_context.hash(password)
    
    def verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """Verify a password against its hash"""
        try:
            return self.pwd_context.verify(plain_password, hashed_password)
        except Exception as e:
            logger.error(f"Password verification error: {e}")
            return False
    
    def needs_update(self, hashed_password: str) -> bool:
        """Check if password hash needs update (deprecated algorithm)"""
        return self.pwd_context.needs_update(hashed_password)

# Global password manager instance
password_manager = PasswordManager()

# ================================
# JWT TOKEN MANAGEMENT
# ================================

class JWTManager:
    """JWT token creation and validation"""
    
    def __init__(self):
        self.secret_key = settings.JWT_SECRET_KEY
        self.algorithm = settings.JWT_ALGORITHM
        self.access_token_expire_minutes = settings.JWT_EXPIRE_MINUTES
        self.refresh_token_expire_days = settings.JWT_REFRESH_EXPIRE_DAYS
    
    def create_access_token(
        self, 
        subject: Union[str, Any], 
        expires_delta: Optional[timedelta] = None,
        additional_claims: Optional[Dict[str, Any]] = None
    ) -> str:
        """Create JWT access token"""
        
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(minutes=self.access_token_expire_minutes)
        
        # Base payload
        payload = {
            "exp": expire,
            "iat": datetime.utcnow(),
            "sub": str(subject),
            "type": "access"
        }
        
        # Add additional claims
        if additional_claims:
            payload.update(additional_claims)
        
        try:
            encoded_jwt = jwt.encode(payload, self.secret_key, algorithm=self.algorithm)
            logger.debug(f"Access token created for subject: {subject}")
            return encoded_jwt
            
        except Exception as e:
            logger.error(f"Failed to create access token: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Could not create access token"
            )
    
    def create_refresh_token(
        self, 
        subject: Union[str, Any],
        expires_delta: Optional[timedelta] = None
    ) -> str:
        """Create JWT refresh token"""
        
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(days=self.refresh_token_expire_days)
        
        payload = {
            "exp": expire,
            "iat": datetime.utcnow(),
            "sub": str(subject),
            "type": "refresh"
        }
        
        try:
            encoded_jwt = jwt.encode(payload, self.secret_key, algorithm=self.algorithm)
            logger.debug(f"Refresh token created for subject: {subject}")
            return encoded_jwt
            
        except Exception as e:
            logger.error(f"Failed to create refresh token: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Could not create refresh token"
            )
    
    def verify_token(self, token: str) -> Dict[str, Any]:
        """Verify and decode JWT token"""
        try:
            payload = jwt.decode(
                token, 
                self.secret_key, 
                algorithms=[self.algorithm]
            )
            
            # Check token type
            token_type = payload.get("type")
            if not token_type:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid token: missing type"
                )
            
            # Check expiration
            exp = payload.get("exp")
            if exp and datetime.utcfromtimestamp(exp) < datetime.utcnow():
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Token has expired"
                )
            
            return payload
            
        except jwt.ExpiredSignatureError:
            logger.warning("Token expired")
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Token has expired"
            )
        
        except jwt.InvalidTokenError as e:
            logger.warning(f"Invalid token: {e}")
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token"
            )
        
        except Exception as e:
            logger.error(f"Token verification error: {e}")
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Could not validate token"
            )
    
    def refresh_access_token(self, refresh_token: str) -> str:
        """Create new access token from refresh token"""
        payload = self.verify_token(refresh_token)
        
        # Verify it's a refresh token
        if payload.get("type") != "refresh":
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid refresh token"
            )
        
        # Extract user ID and create new access token
        user_id = payload.get("sub")
        if not user_id:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid refresh token: missing subject"
            )
        
        return self.create_access_token(subject=user_id)

# Global JWT manager instance
jwt_manager = JWTManager()

# ================================
# USER AUTHENTICATION SERVICE
# ================================

class AuthService:
    """User authentication business logic"""
    
    def __init__(self):
        self.password_manager = password_manager
        self.jwt_manager = jwt_manager
    
    async def authenticate_user(
        self, 
        db, 
        username_or_email: str, 
        password: str
    ) -> Optional["User"]:
        """Authenticate user with username/email and password"""
        from app.models.database import User
        from sqlalchemy import or_
        
        try:
            # Find user by username or email
            result = await db.execute(
                select(User).where(
                    or_(
                        User.username == username_or_email,
                        User.email == username_or_email
                    )
                )
            )
            user = result.scalar_one_or_none()
            
            if not user:
                logger.warning(f"Authentication failed: user not found for {username_or_email}")
                return None
            
            # Check if account is active
            if not user.is_active:
                logger.warning(f"Authentication failed: inactive account for {username_or_email}")
                return None
            
            # Check if account is locked
            if user.is_locked:
                logger.warning(f"Authentication failed: locked account for {username_or_email}")
                raise HTTPException(
                    status_code=status.HTTP_423_LOCKED,
                    detail=f"Account is locked until {user.locked_until}"
                )
            
            # Verify password
            if not self.password_manager.verify_password(password, user.password_hash):
                # Increment failed login attempts
                user.failed_login_attempts += 1
                
                # Lock account after too many failed attempts
                if user.failed_login_attempts >= 5:
                    user.locked_until = datetime.utcnow() + timedelta(minutes=30)
                    logger.warning(f"Account locked due to too many failed attempts: {username_or_email}")
                
                await db.commit()
                logger.warning(f"Authentication failed: invalid password for {username_or_email}")
                return None
            
            # Reset failed login attempts on successful login
            user.failed_login_attempts = 0
            user.locked_until = None
            user.last_login = datetime.utcnow()
            await db.commit()
            
            logger.info(f"User authenticated successfully: {username_or_email}")
            return user
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Authentication error: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Authentication service error"
            )
    
    async def create_user_tokens(self, user: "User") -> Dict[str, str]:
        """Create access and refresh tokens for user"""
        
        # Additional claims for the token
        additional_claims = {
            "username": user.username,
            "email": user.email,
            "is_superuser": user.is_superuser,
            "is_verified": user.is_verified
        }
        
        access_token = self.jwt_manager.create_access_token(
            subject=user.id,
            additional_claims=additional_claims
        )
        
        refresh_token = self.jwt_manager.create_refresh_token(subject=user.id)
        
        return {
            "access_token": access_token,
            "refresh_token": refresh_token,
            "token_type": "bearer",
            "expires_in": settings.JWT_EXPIRE_MINUTES * 60  # seconds
        }
    
    async def get_current_user(self, db, token: str) -> "User":
        """Get current user from JWT token"""
        from app.models.database import User
        
        # Verify token
        payload = self.jwt_manager.verify_token(token)
        
        # Extract user ID
        user_id = payload.get("sub")
        if not user_id:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token: missing user ID"
            )
        
        # Get user from database
        user = await db.get(User, user_id)
        if not user:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="User not found"
            )
        
        # Check if user is still active
        if not user.is_active:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Inactive user"
            )
        
        return user
    
    async def register_user(
        self, 
        db,
        username: str,
        email: str,
        password: str,
        **kwargs
    ) -> "User":
        """Register a new user"""
        from app.models.database import User
        
        try:
            # Check if user already exists
            existing_user = await db.execute(
                select(User).where(
                    or_(User.username == username, User.email == email)
                )
            )
            if existing_user.scalar_one_or_none():
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Username or email already registered"
                )
            
            # Hash password
            hashed_password = self.password_manager.hash_password(password)
            
            # Create user
            user = User(
                username=username,
                email=email,
                password_hash=hashed_password,
                **kwargs
            )
            
            db.add(user)
            await db.commit()
            await db.refresh(user)
            
            logger.info(f"New user registered: {username} ({email})")
            return user
            
        except HTTPException:
            raise
        except Exception as e:
            await db.rollback()
            logger.error(f"User registration error: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="User registration failed"
            )
    
    async def change_password(
        self, 
        db,
        user: "User",
        current_password: str,
        new_password: str
    ) -> bool:
        """Change user password"""
        
        try:
            # Verify current password
            if not self.password_manager.verify_password(current_password, user.password_hash):
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Invalid current password"
                )
            
            # Hash new password
            new_hashed_password = self.password_manager.hash_password(new_password)
            
            # Update user
            user.password_hash = new_hashed_password
            await db.commit()
            
            logger.info(f"Password changed for user: {user.username}")
            return True
            
        except HTTPException:
            raise
        except Exception as e:
            await db.rollback()
            logger.error(f"Password change error: {e}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Password change failed"
            )

# Global auth service instance
auth_service = AuthService()

# ================================
# SECURITY UTILITIES
# ================================

def generate_password_reset_token(email: str) -> str:
    """Generate password reset token"""
    delta = timedelta(hours=settings.EMAIL_RESET_TOKEN_EXPIRE_HOURS)
    now = datetime.utcnow()
    expires = now + delta
    
    payload = {
        "exp": expires,
        "iat": now,
        "sub": email,
        "type": "password_reset"
    }
    
    return jwt.encode(payload, settings.JWT_SECRET_KEY, algorithm=settings.JWT_ALGORITHM)

def verify_password_reset_token(token: str) -> Optional[str]:
    """Verify password reset token and return email"""
    try:
        payload = jwt.decode(token, settings.JWT_SECRET_KEY, algorithms=[settings.JWT_ALGORITHM])
        
        if payload.get("type") != "password_reset":
            return None
        
        email: str = payload.get("sub")
        return email
        
    except jwt.PyJWTError:
        return None

def generate_email_verification_token(email: str) -> str:
    """Generate email verification token"""
    delta = timedelta(days=7)  # 7 days to verify email
    now = datetime.utcnow()
    expires = now + delta
    
    payload = {
        "exp": expires,
        "iat": now,
        "sub": email,
        "type": "email_verification"
    }
    
    return jwt.encode(payload, settings.JWT_SECRET_KEY, algorithm=settings.JWT_ALGORITHM)

def verify_email_verification_token(token: str) -> Optional[str]:
    """Verify email verification token and return email"""
    try:
        payload = jwt.decode(token, settings.JWT_SECRET_KEY, algorithms=[settings.JWT_ALGORITHM])
        
        if payload.get("type") != "email_verification":
            return None
        
        email: str = payload.get("sub")
        return email
        
    except jwt.PyJWTError:
        return None

# ================================
# PERMISSION CHECKING
# ================================

class PermissionChecker:
    """Check user permissions and roles"""
    
    @staticmethod
    def check_superuser(user: "User") -> bool:
        """Check if user is superuser"""
        return user.is_superuser
    
    @staticmethod
    def check_active(user: "User") -> bool:
        """Check if user is active"""
        return user.is_active and not user.is_locked
    
    @staticmethod
    def check_verified(user: "User") -> bool:
        """Check if user email is verified"""
        return user.is_verified
    
    @staticmethod
    def check_portfolio_owner(user: "User", portfolio: "Portfolio") -> bool:
        """Check if user owns the portfolio"""
        return str(user.id) == str(portfolio.user_id)
    
    @staticmethod
    def check_trading_enabled(user: "User") -> bool:
        """Check if user has trading enabled"""
        return user.is_active and user.is_verified and not user.is_locked

# ================================
# SECURITY DECORATORS
# ================================

def require_superuser(func):
    """Decorator to require superuser permission"""
    def wrapper(current_user: "User", *args, **kwargs):
        if not PermissionChecker.check_superuser(current_user):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Superuser permission required"
            )
        return func(current_user, *args, **kwargs)
    return wrapper

def require_verified_user(func):
    """Decorator to require verified user"""
    def wrapper(current_user: "User", *args, **kwargs):
        if not PermissionChecker.check_verified(current_user):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Email verification required"
            )
        return func(current_user, *args, **kwargs)
    return wrapper

# ================================
# EXPORTS
# ================================

__all__ = [
    "password_manager",
    "jwt_manager", 
    "auth_service",
    "PermissionChecker",
    "generate_password_reset_token",
    "verify_password_reset_token",
    "generate_email_verification_token",
    "verify_email_verification_token",
    "require_superuser",
    "require_verified_user"
]


# ai-trading-bot/api/app/middleware/__init__.py
"""
Custom middleware for security, logging, and error handling
"""

from .security import SecurityHeadersMiddleware
from .request_logging import RequestLoggingMiddleware  
from .error_handling import ErrorHandlingMiddleware

__all__ = [
    "SecurityHeadersMiddleware",
    "RequestLoggingMiddleware", 
    "ErrorHandlingMiddleware"
]

# ================================
# ai-trading-bot/api/app/middleware/security.py
"""
Security Headers Middleware
Adds security headers to all HTTP responses
"""

import uuid
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response

from app.core.config import settings


class SecurityHeadersMiddleware(BaseHTTPMiddleware):
    """Middleware to add security headers to all responses"""
    
    async def dispatch(self, request: Request, call_next) -> Response:
        # Generate unique request ID
        request_id = str(uuid.uuid4())
        request.state.request_id = request_id
        
        # Process request
        response = await call_next(request)
        
        # Add security headers
        security_headers = {
            # Request tracking
            "X-Request-ID": request_id,
            
            # Prevent clickjacking
            "X-Frame-Options": "DENY",
            
            # Prevent MIME type sniffing
            "X-Content-Type-Options": "nosniff",
            
            # XSS Protection
            "X-XSS-Protection": "1; mode=block",
            
            # Referrer Policy
            "Referrer-Policy": "strict-origin-when-cross-origin",
            
            # Content Security Policy
            "Content-Security-Policy": (
                "default-src 'self'; "
                "script-src 'self' 'unsafe-inline' 'unsafe-eval' cdnjs.cloudflare.com; "
                "style-src 'self' 'unsafe-inline' cdnjs.cloudflare.com; "
                "img-src 'self' data: https:; "
                "font-src 'self' cdnjs.cloudflare.com; "
                "connect-src 'self' wss: https:; "
                "frame-ancestors 'none';"
            ),
            
            # Permissions Policy (Feature Policy successor)
            "Permissions-Policy": (
                "geolocation=(), "
                "microphone=(), "
                "camera=(), "
                "payment=(), "
                "usb=(), "
                "magnetometer=(), "
                "gyroscope=(), "
                "speaker=()"
            )
        }
        
        # Add HSTS header in production
        if settings.is_production():
            security_headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains; preload"
        
        # Apply headers
        for header, value in security_headers.items():
            response.headers[header] = value
        
        return response

# ================================
# ai-trading-bot/api/app/middleware/request_logging.py
"""
Request Logging Middleware
Logs all incoming requests with timing and metadata
"""

import time
import logging
from typing import Callable
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response

logger = logging.getLogger(__name__)


class RequestLoggingMiddleware(BaseHTTPMiddleware):
    """Middleware to log HTTP requests with timing and metadata"""
    
    def __init__(self, app, log_level: str = "INFO"):
        super().__init__(app)
        self.log_level = getattr(logging, log_level.upper())
    
    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        # Start timing
        start_time = time.time()
        
        # Extract request info
        method = request.method
        url = str(request.url)
        user_agent = request.headers.get("user-agent", "")
        client_ip = self._get_client_ip(request)
        request_id = getattr(request.state, "request_id", "unknown")
        
        # Log request start
        logger.log(
            self.log_level,
            f"Request started: {method} {url}",
            extra={
                "request_id": request_id,
                "method": method,
                "url": url,
                "client_ip": client_ip,
                "user_agent": user_agent,
                "event": "request_start"
            }
        )
        
        # Process request
        try:
            response = await call_next(request)
            
            # Calculate response time
            process_time = time.time() - start_time
            
            # Log successful request
            logger.log(
                self.log_level,
                f"Request completed: {method} {url} - {response.status_code} in {process_time:.3f}s",
                extra={
                    "request_id": request_id,
                    "method": method,
                    "url": url,
                    "status_code": response.status_code,
                    "process_time": process_time,
                    "client_ip": client_ip,
                    "user_agent": user_agent,
                    "event": "request_complete"
                }
            )
            
            # Add timing header
            response.headers["X-Process-Time"] = str(process_time)
            
            return response
            
        except Exception as e:
            # Calculate error time
            process_time = time.time() - start_time
            
            # Log failed request
            logger.error(
                f"Request failed: {method} {url} - {type(e).__name__}: {str(e)} in {process_time:.3f}s",
                extra={
                    "request_id": request_id,
                    "method": method,
                    "url": url,
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "process_time": process_time,
                    "client_ip": client_ip,
                    "user_agent": user_agent,
                    "event": "request_error"
                },
                exc_info=True
            )
            
            # Re-raise the exception
            raise
    
    def _get_client_ip(self, request: Request) -> str:
        """Extract client IP from request headers"""
        # Check for forwarded headers (behind proxy/load balancer)
        forwarded_for = request.headers.get("X-Forwarded-For")
        if forwarded_for:
            return forwarded_for.split(",")[0].strip()
        
        real_ip = request.headers.get("X-Real-IP")
        if real_ip:
            return real_ip
        
        # Fallback to direct client
        return request.client.host if request.client else "unknown"

# ================================
# ai-trading-bot/api/app/middleware/error_handling.py
"""
Global Error Handling Middleware
Catches and formats unhandled exceptions
"""

import logging
import traceback
from typing import Callable
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import JSONResponse
from fastapi import status

from app.core.config import settings

logger = logging.getLogger(__name__)


class ErrorHandlingMiddleware(BaseHTTPMiddleware):
    """Global error handling middleware for unhandled exceptions"""
    
    async def dispatch(self, request: Request, call_next: Callable) -> JSONResponse:
        try:
            response = await call_next(request)
            return response
            
        except Exception as e:
            return await self._handle_exception(request, e)
    
    async def _handle_exception(self, request: Request, exc: Exception) -> JSONResponse:
        """Handle unhandled exceptions"""
        
        request_id = getattr(request.state, "request_id", "unknown")
        
        # Log the exception
        logger.error(
            f"Unhandled exception in {request.method} {request.url.path}: {type(exc).__name__}: {str(exc)}",
            extra={
                "request_id": request_id,
                "method": request.method,
                "url": str(request.url),
                "error": str(exc),
                "error_type": type(exc).__name__,
                "traceback": traceback.format_exc() if settings.DEBUG else None
            },
            exc_info=True
        )
        
        # Determine error response based on exception type
        error_responses = {
            "ValueError": (status.HTTP_400_BAD_REQUEST, "Invalid input data"),
            "PermissionError": (status.HTTP_403_FORBIDDEN, "Permission denied"),
            "FileNotFoundError": (status.HTTP_404_NOT_FOUND, "Resource not found"),
            "TimeoutError": (status.HTTP_408_REQUEST_TIMEOUT, "Request timeout"),
            "ConnectionError": (status.HTTP_503_SERVICE_UNAVAILABLE, "Service temporarily unavailable"),
        }
        
        # Get appropriate status code and message
        exc_type = type(exc).__name__
        status_code, message = error_responses.get(
            exc_type, 
            (status.HTTP_500_INTERNAL_SERVER_ERROR, "Internal server error")
        )
        
        # Prepare error response
        error_response = {
            "error": {
                "code": status_code,
                "message": message,
                "request_id": request_id,
                "timestamp": int(time.time()),
                "path": request.url.path,
                "method": request.method
            }
        }
        
        # Add debug information in development
        if settings.DEBUG:
            error_response["error"]["debug"] = {
                "exception_type": exc_type,
                "exception_message": str(exc),
                "traceback": traceback.format_exc().split('\n')
            }
        
        return JSONResponse(
            status_code=status_code,
            content=error_response
        )

# ================================
# Rate Limiting Middleware (Optional)
# ================================

import time
from collections import defaultdict
from starlette.responses import PlainTextResponse


class RateLimitMiddleware(BaseHTTPMiddleware):
    """Simple in-memory rate limiting middleware"""
    
    def __init__(self, app, calls: int = 100, period: int = 60):
        super().__init__(app)
        self.calls = calls
        self.period = period
        self.clients = defaultdict(list)
    
    async def dispatch(self, request: Request, call_next: Callable):
        # Get client identifier
        client_id = self._get_client_id(request)
        
        # Clean old requests
        now = time.time()
        self.clients[client_id] = [
            req_time for req_time in self.clients[client_id]
            if now - req_time < self.period
        ]
        
        # Check rate limit
        if len(self.clients[client_id]) >= self.calls:
            return PlainTextResponse(
                "Rate limit exceeded",
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                headers={
                    "Retry-After": str(self.period),
                    "X-RateLimit-Limit": str(self.calls),
                    "X-RateLimit-Remaining": "0",
                    "X-RateLimit-Reset": str(int(now + self.period))
                }
            )
        
        # Add current request
        self.clients[client_id].append(now)
        
        # Process request
        response = await call_next(request)
        
        # Add rate limit headers
        remaining = max(0, self.calls - len(self.clients[client_id]))
        response.headers["X-RateLimit-Limit"] = str(self.calls)
        response.headers["X-RateLimit-Remaining"] = str(remaining)
        response.headers["X-RateLimit-Reset"] = str(int(now + self.period))
        
        return response
    
    def _get_client_id(self, request: Request) -> str:
        """Get client identifier for rate limiting"""
        # Try to get authenticated user ID first
        if hasattr(request.state, "user_id"):
            return f"user:{request.state.user_id}"
        
        # Fallback to IP address
        forwarded_for = request.headers.get("X-Forwarded-For")
        if forwarded_for:
            return f"ip:{forwarded_for.split(',')[0].strip()}"
        
        return f"ip:{request.client.host if request.client else 'unknown'}"



        # ai-trading-bot/api/app/core/dependencies.py
"""
FastAPI Dependencies for Authentication, Database, and Common Services
Reusable dependency injection for consistent access control
"""

import logging
from typing import Generator, Optional, AsyncGenerator

from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import get_session
from app.core.auth import auth_service, PermissionChecker
from app.models.database import User, Portfolio

logger = logging.getLogger(__name__)

# ================================
# SECURITY SCHEME
# ================================

# HTTP Bearer token security scheme
security_scheme = HTTPBearer(
    scheme_name="Bearer Token",
    description="Enter JWT token (without 'Bearer' prefix)",
    auto_error=True
)

# ================================
# DATABASE DEPENDENCIES
# ================================

async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """
    Database session dependency
    Usage: db: AsyncSession = Depends(get_db)
    """
    async for session in get_session():
        yield session

# ================================
# AUTHENTICATION DEPENDENCIES
# ================================

async def get_current_user(
    credentials: HTTPAuthorizationCredentials = Depends(security_scheme),
    db: AsyncSession = Depends(get_db)
) -> User:
    """
    Get current authenticated user from JWT token
    Usage: current_user: User = Depends(get_current_user)
    """
    try:
        # Extract token from credentials
        token = credentials.credentials
        
        # Get user from token
        user = await auth_service.get_current_user(db, token)
        
        return user
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Authentication dependency error: {e}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials",
            headers={"WWW-Authenticate": "Bearer"}
        )

async def get_current_active_user(
    current_user: User = Depends(get_current_user)
) -> User:
    """
    Get current active user (must be active and not locked)
    Usage: user: User = Depends(get_current_active_user)
    """
    if not PermissionChecker.check_active(current_user):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Inactive or locked user account"
        )
    return current_user

async def get_current_verified_user(
    current_user: User = Depends(get_current_active_user)
) -> User:
    """
    Get current verified user (must be active and email verified)
    Usage: user: User = Depends(get_current_verified_user)
    """
    if not PermissionChecker.check_verified(current_user):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Email verification required"
        )
    return current_user

async def get_current_superuser(
    current_user: User = Depends(get_current_active_user)
) -> User:
    """
    Get current superuser (must be active and have superuser privileges)
    Usage: admin: User = Depends(get_current_superuser)
    """
    if not PermissionChecker.check_superuser(current_user):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Superuser privileges required"
        )
    return current_user

async def get_trading_enabled_user(
    current_user: User = Depends(get_current_verified_user)
) -> User:
    """
    Get user with trading privileges (verified, active, not locked)
    Usage: trader: User = Depends(get_trading_enabled_user)
    """
    if not PermissionChecker.check_trading_enabled(current_user):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Trading privileges required. Ensure your account is verified and active."
        )
    return current_user

# ================================
# OPTIONAL AUTHENTICATION
# ================================

async def get_current_user_optional(
    credentials: Optional[HTTPAuthorizationCredentials] = Depends(
        HTTPBearer(auto_error=False)
    ),
    db: AsyncSession = Depends(get_db)
) -> Optional[User]:
    """
    Get current user if authenticated, None otherwise (for public endpoints)
    Usage: user: Optional[User] = Depends(get_current_user_optional)
    """
    if not credentials:
        return None
    
    try:
        token = credentials.credentials
        user = await auth_service.get_current_user(db, token)
        return user
    except Exception:
        # Don't raise exception for optional auth
        return None

# ================================
# PORTFOLIO DEPENDENCIES
# ================================

async def get_user_portfolio(
    portfolio_id: str,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
) -> Portfolio:
    """
    Get portfolio by ID with ownership verification
    Usage: portfolio: Portfolio = Depends(get_user_portfolio)
    """
    # Get portfolio from database
    portfolio = await db.get(Portfolio, portfolio_id)
    
    if not portfolio:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Portfolio not found"
        )
    
    # Check ownership
    if not PermissionChecker.check_portfolio_owner(current_user, portfolio):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Access denied. You don't own this portfolio."
        )
    
    return portfolio

async def get_user_main_portfolio(
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
) -> Portfolio:
    """
    Get user's main portfolio (creates one if doesn't exist)
    Usage: portfolio: Portfolio = Depends(get_user_main_portfolio)
    """
    from sqlalchemy import select
    
    # Try to get existing main portfolio
    result = await db.execute(
        select(Portfolio).where(
            Portfolio.user_id == current_user.id,
            Portfolio.name == "Main Portfolio"
        )
    )
    portfolio = result.scalar_one_or_none()
    
    # Create main portfolio if it doesn't exist
    if not portfolio:
        portfolio = Portfolio(
            user_id=current_user.id,
            name="Main Portfolio",
            initial_balance=1000.00,  # Default starting balance
            current_balance=1000.00,
            currency="EUR"
        )
        db.add(portfolio)
        await db.commit()
        await db.refresh(portfolio)
        
        logger.info(f"Created main portfolio for user {current_user.username}")
    
    return portfolio

# ================================
# RATE LIMITING DEPENDENCIES
# ================================

class RateLimitChecker:
    """Rate limiting dependency factory"""
    
    def __init__(self, calls: int, period: int = 60):
        self.calls = calls
        self.period = period
    
    async def __call__(
        self,
        request,
        current_user: Optional[User] = Depends(get_current_user_optional)
    ):
        """Check rate limit for user or IP"""
        # Rate limiting logic would go here
        # For now, just return - actual implementation would use Redis
        pass

# Predefined rate limiters
rate_limit_standard = RateLimitChecker(calls=100, period=60)  # 100/minute
rate_limit_trading = RateLimitChecker(calls=50, period=60)    # 50/minute
rate_limit_ai = RateLimitChecker(calls=20, period=60)         # 20/minute

# ================================
# PERMISSION CHECKING DEPENDENCIES
# ================================

class RequirePermission:
    """Permission checking dependency factory"""
    
    def __init__(self, permission: str):
        self.permission = permission
    
    async def __call__(self, current_user: User = Depends(get_current_active_user)):
        """Check if user has required permission"""
        
        permission_checks = {
            "superuser": PermissionChecker.check_superuser,
            "verified": PermissionChecker.check_verified,
            "trading": PermissionChecker.check_trading_enabled,
            "active": PermissionChecker.check_active
        }
        
        check_func = permission_checks.get(self.permission)
        if not check_func:
            raise ValueError(f"Unknown permission: {self.permission}")
        
        if not check_func(current_user):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Permission required: {self.permission}"
            )
        
        return current_user

# Predefined permission checkers
require_superuser = RequirePermission("superuser")
require_verified = RequirePermission("verified")
require_trading = RequirePermission("trading")

# ================================
# COMMON QUERY DEPENDENCIES
# ================================

async def get_pagination(
    skip: int = 0,
    limit: int = 50
) -> dict:
    """
    Pagination dependency
    Usage: pagination: dict = Depends(get_pagination)
    """
    # Validate pagination parameters
    if skip < 0:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Skip parameter must be non-negative"
        )
    
    if limit <= 0 or limit > 100:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Limit parameter must be between 1 and 100"
        )
    
    return {"skip": skip, "limit": limit}

async def get_timeframe_filter(
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> dict:
    """
    Timeframe filtering dependency
    Usage: timeframe: dict = Depends(get_timeframe_filter)
    """
    from datetime import datetime
    
    filters = {}
    
    if start_date:
        try:
            filters["start_date"] = datetime.fromisoformat(start_date.replace('Z', '+00:00'))
        except ValueError:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid start_date format. Use ISO 8601 format."
            )
    
    if end_date:
        try:
            filters["end_date"] = datetime.fromisoformat(end_date.replace('Z', '+00:00'))
        except ValueError:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid end_date format. Use ISO 8601 format."
            )
    
    # Validate date range
    if filters.get("start_date") and filters.get("end_date"):
        if filters["start_date"] >= filters["end_date"]:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="start_date must be before end_date"
            )
    
    return filters

# ================================
# SERVICE DEPENDENCIES
# ================================

async def get_redis_client():
    """
    Get Redis client dependency
    Usage: redis = Depends(get_redis_client)
    """
    from fastapi import Request
    
    def _get_redis(request: Request):
        if not hasattr(request.app.state, 'redis'):
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Redis service not available"
            )
        return request.app.state.redis
    
    return _get_redis

# ================================
# VALIDATION DEPENDENCIES
# ================================

async def validate_asset_symbol(symbol: str) -> str:
    """
    Validate asset symbol format
    Usage: symbol: str = Depends(validate_asset_symbol)
    """
    if not symbol or not symbol.isalnum() or len(symbol) > 20:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid asset symbol. Must be alphanumeric and max 20 characters."
        )
    
    return symbol.upper()

async def validate_order_amount(amount: float) -> float:
    """
    Validate trading order amount
    Usage: amount: float = Depends(validate_order_amount)
    """
    from app.core.config import settings
    
    if amount <= 0:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Order amount must be positive"
        )
    
    if amount < settings.MIN_TRADE_AMOUNT:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Minimum order amount is {settings.MIN_TRADE_AMOUNT} EUR"
        )
    
    return amount

# ================================
# WEBSOCKET DEPENDENCIES
# ================================

async def get_websocket_user(
    websocket,
    token: Optional[str] = None,
    db: AsyncSession = Depends(get_db)
) -> Optional[User]:
    """
    Get authenticated user for WebSocket connections
    Usage: user = await get_websocket_user(websocket, token, db)
    """
    if not token:
        return None
    
    try:
        user = await auth_service.get_current_user(db, token)
        if not PermissionChecker.check_active(user):
            return None
        return user
    except Exception as e:
        logger.warning(f"WebSocket authentication failed: {e}")
        return None

# ================================
# HEALTH CHECK DEPENDENCIES
# ================================

async def check_service_health() -> dict:
    """
    Check service health dependency
    Usage: health: dict = Depends(check_service_health)
    """
    from app.core.database import check_database_health
    
    try:
        # Check database
        db_health = await check_database_health()
        
        # Add more service checks here (Redis, external APIs, etc.)
        health_status = {
            "database": db_health,
            "overall": "healthy" if db_health.get("status") == "healthy" else "unhealthy"
        }
        
        return health_status
        
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return {
            "database": {"status": "error", "message": str(e)},
            "overall": "unhealthy"
        }

# ================================
# COMMON DEPENDENCY COMBINATIONS
# ================================

# For endpoints requiring authenticated user
AuthenticatedUser = Depends(get_current_active_user)

# For endpoints requiring verified user
VerifiedUser = Depends(get_current_verified_user)

# For endpoints requiring superuser
SuperUser = Depends(get_current_superuser)

# For trading endpoints
TradingUser = Depends(get_trading_enabled_user)

# For database access
Database = Depends(get_db)

# For pagination
Pagination = Depends(get_pagination)

# For timeframe filtering
TimeframeFilter = Depends(get_timeframe_filter)

# ================================
# EXPORTS
# ================================

__all__ = [
    # Core dependencies
    "get_db",
    "get_current_user",
    "get_current_active_user", 
    "get_current_verified_user",
    "get_current_superuser",
    "get_trading_enabled_user",
    "get_current_user_optional",
    
    # Portfolio dependencies
    "get_user_portfolio",
    "get_user_main_portfolio",
    
    # Rate limiting
    "rate_limit_standard",
    "rate_limit_trading", 
    "rate_limit_ai",
    
    # Permission checking
    "require_superuser",
    "require_verified",
    "require_trading",
    
    # Common utilities
    "get_pagination",
    "get_timeframe_filter",
    "validate_asset_symbol",
    "validate_order_amount",
    "get_websocket_user",
    "check_service_health",
    
    # Shortcuts
    "AuthenticatedUser",
    "VerifiedUser", 
    "SuperUser",
    "TradingUser",
    "Database",
    "Pagination",
    "TimeframeFilter"
]

# ai-trading-bot/api/app/models/schemas/__init__.py
"""
Pydantic Schemas for Request/Response Models
Type-safe data validation and serialization
"""

from .auth import *
from .user import *
from .portfolio import *
from .trading import *
from .market import *
from .ai_analysis import *
from .common import *

# ================================
# ai-trading-bot/api/app/models/schemas/common.py
"""
Common Pydantic schemas and base models
"""

from datetime import datetime
from typing import Any, Dict, List, Optional, Union
from uuid import UUID

from pydantic import BaseModel, Field, validator


class BaseSchema(BaseModel):
    """Base schema with common configuration"""
    
    class Config:
        # Enable ORM mode for SQLAlchemy integration
        from_attributes = True
        # Use enum values instead of enum names
        use_enum_values = True
        # Validate assignment (for partial updates)
        validate_assignment = True
        # Allow population by field name or alias
        allow_population_by_field_name = True


class SuccessResponse(BaseSchema):
    """Standard success response"""
    success: bool = True
    message: str
    data: Optional[Dict[str, Any]] = None


class ErrorResponse(BaseSchema):
    """Standard error response"""
    success: bool = False
    error: str
    details: Optional[Dict[str, Any]] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)


class PaginationParams(BaseSchema):
    """Pagination parameters"""
    skip: int = Field(0, ge=0, description="Number of items to skip")
    limit: int = Field(50, ge=1, le=100, description="Number of items to return")


class PaginatedResponse(BaseSchema):
    """Paginated response wrapper"""
    items: List[Any]
    total: int = Field(description="Total number of items")
    skip: int = Field(description="Number of items skipped")
    limit: int = Field(description="Number of items per page")
    has_next: bool = Field(description="Whether there are more items")

    @validator('has_next', always=True)
    def calculate_has_next(cls, v, values):
        total = values.get('total', 0)
        skip = values.get('skip', 0)
        limit = values.get('limit', 50)
        return skip + limit < total


class TimeframeFilter(BaseSchema):
    """Timeframe filtering parameters"""
    start_date: Optional[datetime] = Field(None, description="Start date (ISO format)")
    end_date: Optional[datetime] = Field(None, description="End date (ISO format)")
    
    @validator('end_date')
    def validate_date_range(cls, v, values):
        if v and values.get('start_date') and v <= values['start_date']:
            raise ValueError('end_date must be after start_date')
        return v


# ================================
# ai-trading-bot/api/app/models/schemas/auth.py
"""
Authentication related schemas
"""

import re
from typing import Optional
from pydantic import EmailStr, Field, validator

from .common import BaseSchema


class LoginRequest(BaseSchema):
    """User login request"""
    username_or_email: str = Field(
        ..., 
        min_length=3, 
        max_length=255,
        description="Username or email address"
    )
    password: str = Field(
        ..., 
        min_length=8, 
        max_length=128,
        description="User password"
    )
    remember_me: bool = Field(False, description="Keep user logged in longer")


class RegisterRequest(BaseSchema):
    """User registration request"""
    username: str = Field(
        ..., 
        min_length=3, 
        max_length=50,
        description="Unique username"
    )
    email: EmailStr = Field(..., description="Valid email address")
    password: str = Field(
        ..., 
        min_length=8, 
        max_length=128,
        description="Strong password"
    )
    password_confirm: str = Field(
        ..., 
        description="Password confirmation"
    )
    
    # Optional fields
    preferred_currency: str = Field("EUR", description="Preferred currency")
    timezone: str = Field("Europe/Vienna", description="User timezone")
    
    @validator('username')
    def validate_username(cls, v):
        if not re.match(r'^[a-zA-Z0-9_-]+$', v):
            raise ValueError('Username can only contain letters, numbers, hyphens, and underscores')
        return v
    
    @validator('password')
    def validate_password(cls, v):
        if not re.search(r'[A-Z]', v):
            raise ValueError('Password must contain at least one uppercase letter')
        if not re.search(r'[a-z]', v):
            raise ValueError('Password must contain at least one lowercase letter')
        if not re.search(r'\d', v):
            raise ValueError('Password must contain at least one digit')
        if not re.search(r'[!@#$%^&*(),.?":{}|<>]', v):
            raise ValueError('Password must contain at least one special character')
        return v
    
    @validator('password_confirm')
    def passwords_match(cls, v, values):
        if 'password' in values and v != values['password']:
            raise ValueError('Passwords do not match')
        return v


class TokenResponse(BaseSchema):
    """JWT token response"""
    access_token: str = Field(..., description="JWT access token")
    refresh_token: str = Field(..., description="JWT refresh token")
    token_type: str = Field("bearer", description="Token type")
    expires_in: int = Field(..., description="Token expiration time in seconds")


class RefreshTokenRequest(BaseSchema):
    """Refresh token request"""
    refresh_token: str = Field(..., description="Valid refresh token")


class ChangePasswordRequest(BaseSchema):
    """Change password request"""
    current_password: str = Field(..., description="Current password")
    new_password: str = Field(..., min_length=8, description="New password")
    new_password_confirm: str = Field(..., description="New password confirmation")
    
    @validator('new_password')
    def validate_new_password(cls, v):
        if not re.search(r'[A-Z]', v):
            raise ValueError('Password must contain at least one uppercase letter')
        if not re.search(r'[a-z]', v):
            raise ValueError('Password must contain at least one lowercase letter')
        if not re.search(r'\d', v):
            raise ValueError('Password must contain at least one digit')
        return v
    
    @validator('new_password_confirm')
    def passwords_match(cls, v, values):
        if 'new_password' in values and v != values['new_password']:
            raise ValueError('Passwords do not match')
        return v


class ForgotPasswordRequest(BaseSchema):
    """Forgot password request"""
    email: EmailStr = Field(..., description="User email address")


class ResetPasswordRequest(BaseSchema):
    """Reset password request"""
    token: str = Field(..., description="Password reset token")
    new_password: str = Field(..., min_length=8, description="New password")
    new_password_confirm: str = Field(..., description="New password confirmation")
    
    @validator('new_password_confirm')
    def passwords_match(cls, v, values):
        if 'new_password' in values and v != values['new_password']:
            raise ValueError('Passwords do not match')
        return v


# ================================
# ai-trading-bot/api/app/models/schemas/user.py
"""
User related schemas
"""

from datetime import datetime
from typing import Optional
from uuid import UUID

from pydantic import EmailStr, Field

from .common import BaseSchema


class UserBase(BaseSchema):
    """Base user schema"""
    username: str = Field(..., description="Username")
    email: EmailStr = Field(..., description="Email address")
    preferred_currency: str = Field("EUR", description="Preferred currency")
    timezone: str = Field("Europe/Vienna", description="User timezone")


class UserCreate(UserBase):
    """User creation schema"""
    password: str = Field(..., min_length=8, description="Password")


class UserUpdate(BaseSchema):
    """User update schema"""
    email: Optional[EmailStr] = None
    preferred_currency: Optional[str] = None
    timezone: Optional[str] = None
    risk_tolerance: Optional[float] = Field(None, ge=0, le=1)
    max_portfolio_risk: Optional[float] = Field(None, ge=0, le=1)
    auto_trading_enabled: Optional[bool] = None
    paper_trading_mode: Optional[bool] = None


class UserResponse(UserBase):
    """User response schema"""
    id: UUID
    is_active: bool
    is_verified: bool
    is_superuser: bool
    last_login: Optional[datetime]
    created_at: datetime
    updated_at: datetime
    
    # Trading settings
    risk_tolerance: float
    max_portfolio_risk: float
    auto_trading_enabled: bool
    paper_trading_mode: bool
    
    # Austrian tax settings
    tax_residence: str
    tax_id: Optional[str]


class UserProfile(BaseSchema):
    """User profile with additional statistics"""
    user: UserResponse
    portfolio_count: int
    total_portfolio_value: float
    total_profit_loss: float
    total_trades: int
    account_age_days: int


# ================================
# ai-trading-bot/api/app/models/schemas/portfolio.py
"""
Portfolio related schemas
"""

from datetime import datetime
from decimal import Decimal
from typing import List, Optional
from uuid import UUID

from pydantic import Field, validator

from .common import BaseSchema


class PortfolioBase(BaseSchema):
    """Base portfolio schema"""
    name: str = Field(..., min_length=1, max_length=100, description="Portfolio name")
    currency: str = Field("EUR", description="Portfolio currency")


class PortfolioCreate(PortfolioBase):
    """Portfolio creation schema"""
    initial_balance: Decimal = Field(..., gt=0, description="Initial balance")


class PortfolioUpdate(BaseSchema):
    """Portfolio update schema"""
    name: Optional[str] = Field(None, min_length=1, max_length=100)


class PortfolioResponse(PortfolioBase):
    """Portfolio response schema"""
    id: UUID
    user_id: UUID
    initial_balance: Decimal
    current_balance: Decimal
    total_invested: Decimal
    total_profit_loss: Decimal
    is_active: bool
    created_at: datetime
    updated_at: datetime
    
    @property
    def total_value(self) -> Decimal:
        """Calculate total portfolio value"""
        return self.current_balance + self.total_invested
    
    @property
    def profit_loss_percentage(self) -> float:
        """Calculate profit/loss percentage"""
        if self.initial_balance > 0:
            return float(self.total_profit_loss / self.initial_balance * 100)
        return 0.0


class PortfolioSummary(BaseSchema):
    """Portfolio summary with metrics"""
    portfolio: PortfolioResponse
    positions_count: int
    active_orders_count: int
    
    # Performance metrics
    daily_return: Optional[float]
    weekly_return: Optional[float]
    monthly_return: Optional[float]
    yearly_return: Optional[float]
    
    # Risk metrics
    sharpe_ratio: Optional[float]
    max_drawdown: Optional[float]
    volatility: Optional[float]
    beta: Optional[float]


class PortfolioHistory(BaseSchema):
    """Portfolio value history point"""
    timestamp: datetime
    total_value: Decimal
    cash_balance: Decimal
    invested_value: Decimal
    unrealized_pnl: Decimal
    realized_pnl: Decimal
    daily_return: Optional[float]


# ================================
# ai-trading-bot/api/app/models/schemas/trading.py
"""
Trading related schemas
"""

from datetime import datetime
from decimal import Decimal
from enum import Enum
from typing import Optional
from uuid import UUID

from pydantic import Field, validator

from .common import BaseSchema


class OrderType(str, Enum):
    """Order type enumeration"""
    BUY = "buy"
    SELL = "sell"
    STOP_LOSS = "stop_loss"
    TAKE_PROFIT = "take_profit"
    MARKET = "market"
    LIMIT = "limit"


class OrderStatus(str, Enum):
    """Order status enumeration"""
    PENDING = "pending"
    EXECUTED = "executed"
    CANCELLED = "cancelled"
    FAILED = "failed"
    PARTIAL = "partial"


class PositionStatus(str, Enum):
    """Position status enumeration"""
    OPEN = "open"
    CLOSED = "closed"
    PARTIAL = "partial"


class CreateOrderRequest(BaseSchema):
    """Create trading order request"""
    asset_symbol: str = Field(..., description="Asset symbol (e.g., BTC, AAPL)")
    order_type: OrderType = Field(..., description="Order type")
    quantity: Decimal = Field(..., gt=0, description="Order quantity")
    price: Optional[Decimal] = Field(None, gt=0, description="Order price (for limit orders)")
    stop_price: Optional[Decimal] = Field(None, gt=0, description="Stop price (for stop orders)")
    stop_loss: Optional[Decimal] = Field(None, gt=0, description="Stop loss price")
    take_profit: Optional[Decimal] = Field(None, gt=0, description="Take profit price")
    
    @validator('price')
    def validate_price_for_limit_orders(cls, v, values):
        order_type = values.get('order_type')
        if order_type == OrderType.LIMIT and not v:
            raise ValueError('Price is required for limit orders')
        return v
    
    @validator('stop_price')
    def validate_stop_price_for_stop_orders(cls, v, values):
        order_type = values.get('order_type')
        if order_type in [OrderType.STOP_LOSS, OrderType.TAKE_PROFIT] and not v:
            raise ValueError('Stop price is required for stop orders')
        return v


class OrderResponse(BaseSchema):
    """Trading order response"""
    id: UUID
    portfolio_id: UUID
    asset_id: UUID
    asset_symbol: str
    order_type: OrderType
    quantity: Decimal
    price: Optional[Decimal]
    stop_price: Optional[Decimal]
    status: OrderStatus
    executed_quantity: Decimal
    executed_price: Optional[Decimal]
    fee_amount: Decimal
    fee_currency: Optional[str]
    external_order_id: Optional[str]
    created_at: datetime
    executed_at: Optional[datetime]
    cancelled_at: Optional[datetime]
    expires_at: Optional[datetime]
    
    @property
    def total_value(self) -> Decimal:
        """Calculate total order value"""
        price = self.executed_price or self.price or Decimal('0')
        return self.quantity * price


class PositionResponse(BaseSchema):
    """Trading position response"""
    id: UUID
    portfolio_id: UUID
    asset_id: UUID
    asset_symbol: str
    quantity: Decimal
    avg_buy_price: Decimal
    current_price: Optional[Decimal]
    unrealized_pnl: Decimal
    realized_pnl: Decimal
    status: PositionStatus
    opened_at: datetime
    closed_at: Optional[datetime]
    stop_loss_price: Optional[Decimal]
    take_profit_price: Optional[Decimal]
    
    @property
    def market_value(self) -> Decimal:
        """Calculate current market value"""
        price = self.current_price or self.avg_buy_price
        return self.quantity * price
    
    @property
    def unrealized_pnl_percentage(self) -> float:
        """Calculate unrealized P&L percentage"""
        if self.current_price and self.avg_buy_price > 0:
            return float((self.current_price - self.avg_buy_price) / self.avg_buy_price * 100)
        return 0.0


class UpdatePositionRequest(BaseSchema):
    """Update position request"""
    stop_loss_price: Optional[Decimal] = Field(None, gt=0)
    take_profit_price: Optional[Decimal] = Field(None, gt=0)


# ================================
# ai-trading-bot/api/app/models/schemas/market.py
"""
Market data related schemas
"""

from datetime import datetime
from decimal import Decimal
from enum import Enum
from typing import List, Optional

from pydantic import Field

from .common import BaseSchema


class AssetType(str, Enum):
    """Asset type enumeration"""
    CRYPTO = "crypto"
    STOCK = "stock"
    ETF = "etf"
    COMMODITY = "commodity"
    FOREX = "forex"


class AssetResponse(BaseSchema):
    """Asset information response"""
    id: str
    symbol: str
    name: str
    asset_type: AssetType
    exchange: Optional[str]
    sector: Optional[str]
    market_cap: Optional[int]
    is_active: bool
    is_tradeable: bool


class MarketDataResponse(BaseSchema):
    """Market data response"""
    symbol: str
    price: Decimal
    volume_24h: Optional[Decimal]
    change_24h: Optional[Decimal]
    change_percent_24h: Optional[float]
    market_cap: Optional[int]
    timestamp: datetime


class CandlestickData(BaseSchema):
    """OHLCV candlestick data"""
    timestamp: datetime
    open_price: Decimal
    high_price: Decimal
    low_price: Decimal
    close_price: Decimal
    volume: Decimal
    timeframe: str


class ChartDataRequest(BaseSchema):
    """Chart data request parameters"""
    symbol: str = Field(..., description="Asset symbol")
    timeframe: str = Field("1h", description="Timeframe (1m, 5m, 1h, 1d, etc.)")
    limit: int = Field(100, ge=1, le=1000, description="Number of data points")
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None


class TrendingAssetsResponse(BaseSchema):
    """Trending assets response"""
    assets: List[AssetResponse]
    timeframe: str
    updated_at: datetime


# ================================
# ai-trading-bot/api/app/models/schemas/ai_analysis.py
"""
AI Analysis related schemas
"""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional
from uuid import UUID

from pydantic import Field, validator

from .common import BaseSchema


class AnalysisType(str, Enum):
    """Analysis type enumeration"""
    FUNDAMENTAL = "fundamental"
    TECHNICAL = "technical"
    SENTIMENT = "sentiment"
    CONSENSUS = "consensus"


class Recommendation(str, Enum):
    """Trading recommendation enumeration"""
    BUY = "BUY"
    SELL = "SELL"
    HOLD = "HOLD"


class AIModelType(str, Enum):
    """AI model type enumeration"""
    GPT_4 = "gpt-4.1"
    DEEPSEEK = "deepseek-r1"
    GEMINI = "gemini"
    MISTRAL = "mistral"


class AnalysisRequest(BaseSchema):
    """AI analysis request"""
    symbols: List[str] = Field(..., min_items=1, max_items=10, description="Asset symbols to analyze")
    analysis_types: List[AnalysisType] = Field(
        default=[AnalysisType.CONSENSUS], 
        description="Types of analysis to perform"
    )
    timeframe: str = Field("1h", description="Analysis timeframe")
    include_reasoning: bool = Field(True, description="Include AI reasoning in response")


class AIAnalysisResponse(BaseSchema):
    """AI analysis response"""
    id: UUID
    asset_symbol: str
    analysis_type: AnalysisType
    ai_model: AIModelType
    recommendation: Recommendation
    confidence_score: float = Field(..., ge=0, le=1, description="Confidence score (0-1)")
    target_price: Optional[float] = Field(None, gt=0, description="Target price prediction")
    reasoning: Optional[str] = Field(None, description="AI reasoning for the recommendation")
    key_indicators: Dict[str, Any] = Field(default_factory=dict, description="Key technical indicators")
    market_conditions: Optional[str] = Field(None, description="Current market conditions")
    created_at: datetime
    expires_at: Optional[datetime]
    
    @property
    def confidence_percentage(self) -> float:
        """Get confidence score as percentage"""
        return self.confidence_score * 100
    
    @property
    def is_expired(self) -> bool:
        """Check if analysis is expired"""
        if self.expires_at:
            return datetime.utcnow() > self.expires_at
        return False


class ConsensusAnalysis(BaseSchema):
    """Multi-AI consensus analysis"""
    asset_symbol: str
    consensus_recommendation: Recommendation
    consensus_confidence: float = Field(..., ge=0, le=1)
    individual_analyses: List[AIAnalysisResponse]
    target_price_range: Optional[Dict[str, float]] = None  # min, max, avg
    created_at: datetime
    
    @validator('consensus_confidence')
    def validate_consensus_confidence(cls, v, values):
        """Ensure consensus confidence is reasonable"""
        if v < 0.3:  # Very lo




      # ai-trading-bot/api/app/routers/auth.py
"""
Authentication Router
Handles user authentication, registration, and account management
"""

import logging
from datetime import datetime, timedelta
from typing import Any, Dict, Optional

from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks
from fastapi.security import HTTPAuthorizationCredentials, HTTPBearer
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, or_

from app.core.auth import auth_service, generate_password_reset_token, verify_password_reset_token
from app.core.config import settings
from app.core.dependencies import get_db, get_current_active_user, rate_limit_standard
from app.models.database import User, Portfolio
from app.models.schemas.auth import (
    LoginRequest,
    RegisterRequest, 
    TokenResponse,
    RefreshTokenRequest,
    ChangePasswordRequest,
    ForgotPasswordRequest,
    ResetPasswordRequest
)
from app.models.schemas.user import UserResponse
from app.models.schemas.common import SuccessResponse, ErrorResponse

logger = logging.getLogger(__name__)

# Create router
router = APIRouter()

# Security scheme for optional auth
security_optional = HTTPBearer(auto_error=False)

# ================================
# AUTHENTICATION ENDPOINTS
# ================================

@router.post(
    "/login",
    response_model=TokenResponse,
    summary="User Login",
    description="Authenticate user with username/email and password",
    responses={
        200: {"description": "Login successful", "model": TokenResponse},
        400: {"description": "Invalid credentials", "model": ErrorResponse},
        423: {"description": "Account locked", "model": ErrorResponse}
    }
)
async def login(
    request: LoginRequest,
    db: AsyncSession = Depends(get_db),
    _: None = Depends(rate_limit_standard)
) -> TokenResponse:
    """User login endpoint"""
    
    try:
        # Authenticate user
        user = await auth_service.authenticate_user(
            db=db,
            username_or_email=request.username_or_email,
            password=request.password
        )
        
        if not user:
            # Don't reveal whether username or password was wrong
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid username/email or password"
            )
        
        # Create tokens with extended expiry if remember_me is true
        if request.remember_me:
            # Extend access token to 7 days for remember me
            access_token_expires = timedelta(days=7)
        else:
            access_token_expires = timedelta(minutes=settings.JWT_EXPIRE_MINUTES)
        
        # Generate tokens
        tokens = await auth_service.create_user_tokens(user)
        
        logger.info(f"User logged in successfully: {user.username}")
        
        return TokenResponse(**tokens)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Login error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Login service temporarily unavailable"
        )

@router.post(
    "/register",
    response_model=UserResponse,
    status_code=status.HTTP_201_CREATED,
    summary="User Registration",
    description="Register a new user account",
    responses={
        201: {"description": "User registered successfully", "model": UserResponse},
        400: {"description": "Registration failed", "model": ErrorResponse}
    }
)
async def register(
    request: RegisterRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    _: None = Depends(rate_limit_standard)
) -> UserResponse:
    """User registration endpoint"""
    
    try:
        # Register user
        user = await auth_service.register_user(
            db=db,
            username=request.username,
            email=request.email,
            password=request.password,
            preferred_currency=request.preferred_currency,
            timezone=request.timezone
        )
        
        # Create default portfolio
        default_portfolio = Portfolio(
            user_id=user.id,
            name="Main Portfolio",
            initial_balance=1000.00,  # Default starting balance
            current_balance=1000.00,
            currency=request.preferred_currency
        )
        db.add(default_portfolio)
        await db.commit()
        
        # Send verification email in background
        background_tasks.add_task(
            send_verification_email,
            user.email,
            user.username
        )
        
        logger.info(f"New user registered: {user.username} ({user.email})")
        
        return UserResponse.from_orm(user)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Registration error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Registration service temporarily unavailable"
        )

@router.post(
    "/logout",
    response_model=SuccessResponse,
    summary="User Logout",
    description="Logout user and invalidate tokens",
    responses={
        200: {"description": "Logout successful", "model": SuccessResponse}
    }
)
async def logout(
    current_user: User = Depends(get_current_active_user),
    credentials: HTTPAuthorizationCredentials = Depends(HTTPBearer())
) -> SuccessResponse:
    """User logout endpoint"""
    
    try:
        # In a production system, you would:
        # 1. Add token to blacklist in Redis
        # 2. Revoke refresh tokens
        # 3. Clear sessions
        
        # For now, we just log the logout
        logger.info(f"User logged out: {current_user.username}")
        
        return SuccessResponse(
            message="Logged out successfully",
            data={"username": current_user.username}
        )
        
    except Exception as e:
        logger.error(f"Logout error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Logout service temporarily unavailable"
        )

# ================================
# TOKEN MANAGEMENT
# ================================

@router.post(
    "/refresh",
    response_model=TokenResponse,
    summary="Refresh Access Token",
    description="Get new access token using refresh token",
    responses={
        200: {"description": "Token refreshed successfully", "model": TokenResponse},
        401: {"description": "Invalid refresh token", "model": ErrorResponse}
    }
)
async def refresh_token(
    request: RefreshTokenRequest,
    db: AsyncSession = Depends(get_db)
) -> TokenResponse:
    """Refresh access token endpoint"""
    
    try:
        # Refresh access token
        new_access_token = auth_service.jwt_manager.refresh_access_token(
            refresh_token=request.refresh_token
        )
        
        # Return new token (keep existing refresh token)
        return TokenResponse(
            access_token=new_access_token,
            refresh_token=request.refresh_token,
            token_type="bearer",
            expires_in=settings.JWT_EXPIRE_MINUTES * 60
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Token refresh error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Token refresh service temporarily unavailable"
        )

@router.get(
    "/me",
    response_model=UserResponse,
    summary="Get Current User",
    description="Get current authenticated user information",
    responses={
        200: {"description": "User information", "model": UserResponse},
        401: {"description": "Not authenticated", "model": ErrorResponse}
    }
)
async def get_current_user_info(
    current_user: User = Depends(get_current_active_user)
) -> UserResponse:
    """Get current user information"""
    return UserResponse.from_orm(current_user)

# ================================
# PASSWORD MANAGEMENT
# ================================

@router.post(
    "/change-password",
    response_model=SuccessResponse,
    summary="Change Password",
    description="Change user password (requires current password)",
    responses={
        200: {"description": "Password changed successfully", "model": SuccessResponse},
        400: {"description": "Invalid current password", "model": ErrorResponse}
    }
)
async def change_password(
    request: ChangePasswordRequest,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
) -> SuccessResponse:
    """Change user password endpoint"""
    
    try:
        # Change password
        success = await auth_service.change_password(
            db=db,
            user=current_user,
            current_password=request.current_password,
            new_password=request.new_password
        )
        
        if success:
            logger.info(f"Password changed for user: {current_user.username}")
            return SuccessResponse(
                message="Password changed successfully",
                data={"username": current_user.username}
            )
        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Failed to change password"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Password change error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Password change service temporarily unavailable"
        )

@router.post(
    "/forgot-password",
    response_model=SuccessResponse,
    summary="Forgot Password",
    description="Request password reset email",
    responses={
        200: {"description": "Reset email sent (if user exists)", "model": SuccessResponse}
    }
)
async def forgot_password(
    request: ForgotPasswordRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    _: None = Depends(rate_limit_standard)
) -> SuccessResponse:
    """Forgot password endpoint"""
    
    try:
        # Check if user exists
        result = await db.execute(
            select(User).where(User.email == request.email)
        )
        user = result.scalar_one_or_none()
        
        if user and user.is_active:
            # Generate reset token
            reset_token = generate_password_reset_token(user.email)
            
            # Send reset email in background
            background_tasks.add_task(
                send_password_reset_email,
                user.email,
                user.username,
                reset_token
            )
            
            logger.info(f"Password reset requested for: {user.email}")
        
        # Always return success (don't reveal if email exists)
        return SuccessResponse(
            message="If an account with that email exists, a password reset link has been sent",
            data={"email": request.email}
        )
        
    except Exception as e:
        logger.error(f"Forgot password error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Password reset service temporarily unavailable"
        )

@router.post(
    "/reset-password",
    response_model=SuccessResponse,
    summary="Reset Password",
    description="Reset password using reset token",
    responses={
        200: {"description": "Password reset successfully", "model": SuccessResponse},
        400: {"description": "Invalid or expired token", "model": ErrorResponse}
    }
)
async def reset_password(
    request: ResetPasswordRequest,
    db: AsyncSession = Depends(get_db)
) -> SuccessResponse:
    """Reset password endpoint"""
    
    try:
        # Verify reset token
        email = verify_password_reset_token(request.token)
        if not email:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid or expired reset token"
            )
        
        # Get user by email
        result = await db.execute(
            select(User).where(User.email == email)
        )
        user = result.scalar_one_or_none()
        
        if not user or not user.is_active:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="User not found or inactive"
            )
        
        # Hash new password
        new_password_hash = auth_service.password_manager.hash_password(request.new_password)
        
        # Update password
        user.password_hash = new_password_hash
        # Reset failed login attempts
        user.failed_login_attempts = 0
        user.locked_until = None
        
        await db.commit()
        
        logger.info(f"Password reset successfully for: {user.email}")
        
        return SuccessResponse(
            message="Password reset successfully",
            data={"email": email}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Password reset error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Password reset service temporarily unavailable"
        )

# ================================
# EMAIL VERIFICATION
# ================================

@router.post(
    "/verify-email",
    response_model=SuccessResponse,
    summary="Verify Email",
    description="Verify user email with verification token",
    responses={
        200: {"description": "Email verified successfully", "model": SuccessResponse},
        400: {"description": "Invalid or expired token", "model": ErrorResponse}
    }
)
async def verify_email(
    token: str,
    db: AsyncSession = Depends(get_db)
) -> SuccessResponse:
    """Email verification endpoint"""
    
    try:
        from app.core.auth import verify_email_verification_token
        
        # Verify token
        email = verify_email_verification_token(token)
        if not email:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid or expired verification token"
            )
        
        # Get user by email
        result = await db.execute(
            select(User).where(User.email == email)
        )
        user = result.scalar_one_or_none()
        
        if not user:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="User not found"
            )
        
        if user.is_verified:
            return SuccessResponse(
                message="Email already verified",
                data={"email": email}
            )
        
        # Verify email
        user.is_verified = True
        await db.commit()
        
        logger.info(f"Email verified for user: {user.email}")
        
        return SuccessResponse(
            message="Email verified successfully",
            data={"email": email}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Email verification error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Email verification service temporarily unavailable"
        )

@router.post(
    "/resend-verification",
    response_model=SuccessResponse,
    summary="Resend Verification Email",
    description="Resend email verification",
    responses={
        200: {"description": "Verification email sent", "model": SuccessResponse}
    }
)
async def resend_verification(
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_active_user),
    _: None = Depends(rate_limit_standard)
) -> SuccessResponse:
    """Resend email verification"""
    
    try:
        if current_user.is_verified:
            return SuccessResponse(
                message="Email already verified",
                data={"email": current_user.email}
            )
        
        # Send verification email in background
        background_tasks.add_task(
            send_verification_email,
            current_user.email,
            current_user.username
        )
        
        logger.info(f"Verification email resent to: {current_user.email}")
        
        return SuccessResponse(
            message="Verification email sent",
            data={"email": current_user.email}
        )
        
    except Exception as e:
        logger.error(f"Resend verification error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Email service temporarily unavailable"
        )

# ================================
# ACCOUNT MANAGEMENT
# ================================

@router.delete(
    "/delete-account",
    response_model=SuccessResponse,
    summary="Delete Account",
    description="Delete user account (requires password confirmation)",
    responses={
        200: {"description": "Account deleted successfully", "model": SuccessResponse},
        400: {"description": "Invalid password", "model": ErrorResponse}
    }
)
async def delete_account(
    password: str,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
) -> SuccessResponse:
    """Delete user account endpoint"""
    
    try:
        # Verify password
        if not auth_service.password_manager.verify_password(password, current_user.password_hash):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid password"
            )
        
        # In production, you might want to:
        # 1. Soft delete (mark as deleted)
        # 2. Archive data for compliance
        # 3. Cancel subscriptions
        # 4. Clean up related data
        
        # For now, deactivate the account
        current_user.is_active = False
        current_user.email = f"deleted_{current_user.id}@deleted.local"
        
        await db.commit()
        
        logger.info(f"Account deleted/deactivated: {current_user.username}")
        
        return SuccessResponse(
            message="Account deleted successfully",
            data={"username": current_user.username}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Account deletion error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Account deletion service temporarily unavailable"
        )

# ================================
# BACKGROUND TASKS (EMAIL SENDING)
# ================================

async def send_verification_email(email: str, username: str) -> None:
    """Send email verification email (background task)"""
    try:
        from app.core.auth import generate_email_verification_token
        
        # Generate verification token
        verification_token = generate_email_verification_token(email)
        
        # In production, send actual email here
        # For now, just log the token
        verification_url = f"{settings.FRONTEND_URL}/verify-email?token={verification_token}"
        
        logger.info(f"Email verification URL for {username} ({email}): {verification_url}")
        
        # TODO: Implement actual email sending
        # await email_service.send_verification_email(email, username, verification_url)
        
    except Exception as e:
        logger.error(f"Failed to send verification email to {email}: {e}")

async def send_password_reset_email(email: str, username: str, reset_token: str) -> None:
    """Send password reset email (background task)"""
    try:



# ai-trading-bot/api/app/routers/health.py
"""
Health Check Router
System health monitoring and status endpoints for load balancer and monitoring
"""

import asyncio
import logging
import time
from datetime import datetime
from typing import Any, Dict, List

import psutil
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import settings
from app.core.database import get_session, check_database_health
from app.models.schemas.common import BaseSchema

logger = logging.getLogger(__name__)

# Create router
router = APIRouter()

# ================================
# HEALTH CHECK SCHEMAS
# ================================

class ServiceHealth(BaseSchema):
    """Individual service health status"""
    name: str
    status: str  # healthy, unhealthy, degraded
    response_time_ms: float
    details: Dict[str, Any]
    last_checked: datetime


class SystemHealth(BaseSchema):
    """Overall system health"""
    status: str  # healthy, unhealthy, degraded
    timestamp: datetime
    uptime_seconds: float
    version: str
    environment: str
    services: List[ServiceHealth]
    system_metrics: Dict[str, Any]


class DatabaseHealth(BaseSchema):
    """Database health details"""
    status: str
    connection_pool: Dict[str, Any]
    active_connections: int
    database_size: str
    timescaledb_enabled: bool
    response_time_ms: float


# ================================
# SIMPLE HEALTH CHECKS
# ================================

@router.get(
    "/",
    response_model=Dict[str, str],
    summary="Basic Health Check",
    description="Simple health check for load balancers",
    tags=["Health"]
)
async def basic_health() -> Dict[str, str]:
    """
    Basic health endpoint for load balancers
    Returns 200 OK if service is running
    """
    return {
        "status": "healthy",
        "service": "ai-trading-bot-api",
        "timestamp": datetime.utcnow().isoformat()
    }

@router.get(
    "/ping",
    summary="Ping Endpoint",
    description="Simple ping response",
    tags=["Health"]
)
async def ping() -> Dict[str, str]:
    """Simple ping endpoint"""
    return {"message": "pong"}

@router.get(
    "/ready",
    summary="Readiness Check",
    description="Check if service is ready to handle requests",
    tags=["Health"]
)
async def readiness_check(
    db: AsyncSession = Depends(get_session)
) -> Dict[str, Any]:
    """
    Readiness check - service is ready to handle traffic
    Used by Kubernetes readiness probes
    """
    try:
        # Quick database connectivity check
        start_time = time.time()
        await db.execute(text("SELECT 1"))
        db_response_time = (time.time() - start_time) * 1000
        
        # Check if response time is acceptable (< 1000ms)
        if db_response_time > 1000:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Database response time too high"
            )
        
        return {
            "status": "ready",
            "database": "connected",
            "response_time_ms": round(db_response_time, 2),
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Readiness check failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Service not ready: {str(e)}"
        )

@router.get(
    "/live",
    summary="Liveness Check", 
    description="Check if service is alive",
    tags=["Health"]
)
async def liveness_check() -> Dict[str, str]:
    """
    Liveness check - service is alive
    Used by Kubernetes liveness probes
    """
    try:
        # Check if the application is responsive
        # This is a simple check that the process is running
        
        return {
            "status": "alive",
            "pid": psutil.Process().pid,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Liveness check failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Service not alive: {str(e)}"
        )

# ================================
# DETAILED HEALTH CHECKS
# ================================

@router.get(
    "/health",
    response_model=SystemHealth,
    summary="Detailed Health Check",
    description="Comprehensive system health status",
    tags=["Health"]
)
async def detailed_health_check(
    db: AsyncSession = Depends(get_session)
) -> SystemHealth:
    """
    Detailed health check with comprehensive service status
    """
    start_time = time.time()
    services = []
    overall_status = "healthy"
    
    try:
        # Check Database Health
        db_health = await check_database_service_health(db)
        services.append(db_health)
        
        if db_health.status != "healthy":
            overall_status = "degraded"
        
        # Check Redis Health (if available)
        redis_health = await check_redis_service_health()
        services.append(redis_health)
        
        if redis_health.status != "healthy":
            overall_status = "degraded" if overall_status == "healthy" else "unhealthy"
        
        # Check External APIs Health
        external_apis_health = await check_external_apis_health()
        services.extend(external_apis_health)
        
        # Check system metrics
        system_metrics = get_system_metrics()
        
        # Calculate uptime
        uptime_seconds = time.time() - start_time
        
        return SystemHealth(
            status=overall_status,
            timestamp=datetime.utcnow(),
            uptime_seconds=uptime_seconds,
            version=settings.PROJECT_VERSION,
            environment=settings.ENVIRONMENT,
            services=services,
            system_metrics=system_metrics
        )
        
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return SystemHealth(
            status="unhealthy",
            timestamp=datetime.utcnow(),
            uptime_seconds=0,
            version=settings.PROJECT_VERSION,
            environment=settings.ENVIRONMENT,
            services=[],
            system_metrics={},
        )

# ================================
# INDIVIDUAL SERVICE HEALTH CHECKS
# ================================

async def check_database_service_health(db: AsyncSession) -> ServiceHealth:
    """Check database health with detailed metrics"""
    start_time = time.time()
    
    try:
        # Basic connectivity
        await db.execute(text("SELECT 1"))
        
        # Get database statistics
        stats_query = """
        SELECT 
            pg_database_size(current_database()) as db_size,
            (SELECT count(*) FROM pg_stat_activity WHERE state = 'active') as active_connections,
            (SELECT setting FROM pg_settings WHERE name = 'max_connections') as max_connections
        """
        result = await db.execute(text(stats_query))
        stats = result.first()
        
        # Check TimescaleDB
        timescale_query = "SELECT extname FROM pg_extension WHERE extname = 'timescaledb'"
        timescale_result = await db.execute(text(timescale_query))
        timescaledb_enabled = timescale_result.scalar() is not None
        
        response_time = (time.time() - start_time) * 1000
        
        # Determine status based on response time and connection count
        status = "healthy"
        if response_time > 500:  # 500ms threshold
            status = "degraded"
        if response_time > 2000:  # 2s threshold
            status = "unhealthy"
        
        return ServiceHealth(
            name="database",
            status=status,
            response_time_ms=round(response_time, 2),
            details={
                "type": "PostgreSQL + TimescaleDB",
                "database_size": f"{stats.db_size // (1024*1024)} MB" if stats else "unknown",
                "active_connections": stats.active_connections if stats else 0,
                "max_connections": stats.max_connections if stats else 0,
                "timescaledb_enabled": timescaledb_enabled,
                "connection_pool": {
                    "size": db.get_bind().pool.size(),
                    "checked_out": db.get_bind().pool.checkedout(),
                    "overflow": db.get_bind().pool.overflow(),
                }
            },
            last_checked=datetime.utcnow()
        )
        
    except Exception as e:
        logger.error(f"Database health check failed: {e}")
        return ServiceHealth(
            name="database",
            status="unhealthy",
            response_time_ms=0,
            details={"error": str(e)},
            last_checked=datetime.utcnow()
        )

async def check_redis_service_health() -> ServiceHealth:
    """Check Redis service health"""
    start_time = time.time()
    
    try:
        # This would be implemented when Redis is available
        # For now, return a mock response
        
        response_time = (time.time() - start_time) * 1000
        
        return ServiceHealth(
            name="redis",
            status="healthy",  # Would be determined by actual Redis check
            response_time_ms=round(response_time, 2),
            details={
                "type": "Redis",
                "version": "7.0",
                "memory_used": "10MB",
                "connected_clients": 5
            },
            last_checked=datetime.utcnow()
        )
        
    except Exception as e:
        logger.error(f"Redis health check failed: {e}")
        return ServiceHealth(
            name="redis",
            status="unhealthy",
            response_time_ms=0,
            details={"error": str(e)},
            last_checked=datetime.utcnow()
        )

async def check_external_apis_health() -> List[ServiceHealth]:
    """Check external API services health"""
    services = []
    
    # Check Bitpanda API
    bitpanda_health = await check_api_endpoint(
        name="bitpanda_api",
        url="https://api.exchange.bitpanda.com/public/v1/time",
        timeout=5
    )
    services.append(bitpanda_health)
    
    # Check Alpha Vantage API (if configured)
    if settings.ALPHA_VANTAGE_API_KEY:
        alpha_vantage_health = await check_api_endpoint(
            name="alpha_vantage",
            url="https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=IBM&interval=1min&apikey=demo",
            timeout=5
        )
        services.append(alpha_vantage_health)
    
    # Check CoinGecko API
    coingecko_health = await check_api_endpoint(
        name="coingecko_api",
        url="https://api.coingecko.com/api/v3/ping",
        timeout=5
    )
    services.append(coingecko_health)
    
    return services

async def check_api_endpoint(name: str, url: str, timeout: int = 5) -> ServiceHealth:
    """Check external API endpoint health"""
    import aiohttp
    start_time = time.time()
    
    try:
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=timeout)) as session:
            async with session.get(url) as response:
                response_time = (time.time() - start_time) * 1000
                
                status = "healthy" if response.status == 200 else "unhealthy"
                if response_time > 2000:  # 2s threshold
                    status = "degraded"
                
                return ServiceHealth(
                    name=name,
                    status=status,
                    response_time_ms=round(response_time, 2),
                    details={
                        "url": url,
                        "status_code": response.status,
                        "content_type": response.headers.get("content-type", "unknown")
                    },
                    last_checked=datetime.utcnow()
                )
                
    except asyncio.TimeoutError:
        return ServiceHealth(
            name=name,
            status="unhealthy",
            response_time_ms=timeout * 1000,
            details={"error": "Request timeout", "url": url},
            last_checked=datetime.utcnow()
        )
    except Exception as e:
        return ServiceHealth(
            name=name,
            status="unhealthy",
            response_time_ms=0,
            details={"error": str(e), "url": url},
            last_checked=datetime.utcnow()
        )

# ================================
# SYSTEM METRICS
# ================================

def get_system_metrics() -> Dict[str, Any]:
    """Get system performance metrics"""
    try:
        # CPU metrics
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_count = psutil.cpu_count()
        
        # Memory metrics
        memory = psutil.virtual_memory()
        
        # Disk metrics
        disk = psutil.disk_usage('/')
        
        # Process metrics
        process = psutil.Process()
        process_memory = process.memory_info()
        
        return {
            "cpu": {
                "usage_percent": cpu_percent,
                "count": cpu_count,
                "load_average": list(psutil.getloadavg()) if hasattr(psutil, 'getloadavg') else None
            },
            "memory": {
                "total_mb": round(memory.total / (1024 * 1024), 2),
                "available_mb": round(memory.available / (1024 * 1024), 2),
                "used_percent": memory.percent,
                "process_rss_mb": round(process_memory.rss / (1024 * 1024), 2),
                "process_vms_mb": round(process_memory.vms / (1024 * 1024), 2)
            },
            "disk": {
                "total_gb": round(disk.total / (1024 * 1024 * 1024), 2),
                "free_gb": round(disk.free / (1024 * 1024 * 1024), 2),
                "used_percent": round((disk.used / disk.total) * 100, 2)
            },
            "process": {
                "pid": process.pid,
                "threads": process.num_threads(),
                "open_files": len(process.open_files()),
                "connections": len(process.connections())
            }
        }
        
    except Exception as e:
        logger.error(f"Failed to get system metrics: {e}")
        return {"error": str(e)}

# ================================
# MONITORING ENDPOINTS
# ================================

@router.get(
    "/metrics",
    summary="Prometheus Metrics",
    description="Metrics endpoint for Prometheus scraping",
    tags=["Monitoring"]
)
async def metrics_endpoint() -> str:
    """
    Prometheus metrics endpoint
    Returns metrics in Prometheus format
    """
    try:
        # Get system metrics
        system_metrics = get_system_metrics()
        
        # Format metrics for Prometheus
        metrics_lines = [
            "# HELP ai_trading_bot_cpu_usage_percent CPU usage percentage",
            "# TYPE ai_trading_bot_cpu_usage_percent gauge",
            f"ai_trading_bot_cpu_usage_percent {system_metrics.get('cpu', {}).get('usage_percent', 0)}",
            "",
            "# HELP ai_trading_bot_memory_usage_percent Memory usage percentage", 
            "# TYPE ai_trading_bot_memory_usage_percent gauge",
            f"ai_trading_bot_memory_usage_percent {system_metrics.get('memory', {}).get('used_percent', 0)}",
            "",
            "# HELP ai_trading_bot_process_memory_mb Process memory usage in MB",
            "# TYPE ai_trading_bot_process_memory_mb gauge", 
            f"ai_trading_bot_process_memory_mb {system_metrics.get('memory', {}).get('process_rss_mb', 0)}",
            "",
            "# HELP ai_trading_bot_disk_usage_percent Disk usage percentage",
            "# TYPE ai_trading_bot_disk_usage_percent gauge",
            f"ai_trading_bot_disk_usage_percent {system_metrics.get('disk', {}).get('used_percent', 0)}",
            "",
            "# HELP ai_trading_bot_uptime_seconds Application uptime in seconds",
            "# TYPE ai_trading_bot_uptime_seconds counter",
            f"ai_trading_bot_uptime_seconds {time.time()}",
            ""
        ]
        
        return "\n".join(metrics_lines)
        
    except Exception as e:
        logger.error(f"Metrics endpoint error: {e}")
        return f"# Error generating metrics: {str(e)}"

@router.get(
    "/version",
    summary="Version Information",
    description="Get API version and build information",
    tags=["Information"]
)
async def version_info() -> Dict[str, Any]:
    """Get version and build information"""
    return {
        "name": settings.PROJECT_NAME,
        "version": settings.PROJECT_VERSION,
        "environment": settings.ENVIRONMENT,
        "python_version": f"{psutil.python_version()}",
        "build_date": "2024-01-01",  # Would be set during build
        "git_commit": "abc123def",  # Would be set during build
        "features": {
            "auto_trading": settings.ENABLE_AUTO_TRADING,
            "paper_trading": settings.ENABLE_PAPER_TRADING,
            "ai_analysis": settings.ENABLE_AI_ANALYSIS,
            "websocket": settings.ENABLE_WEBSOCKET,
            "risk_alerts": settings.ENABLE_RISK_ALERTS
        }
    }

# ================================
# DEBUG ENDPOINTS (Development Only)
# ================================

if settings.DEBUG:
    @router.get(
        "/debug/config",
        summary="Debug Configuration",
        description="View sanitized configuration (debug only)",
        tags=["Debug"],
        include_in_schema=settings.DEBUG
    )
    async def debug_config() -> Dict[str, Any]:
        """Debug endpoint to view sanitized configuration"""
        return {
            "environment": settings.ENVIRONMENT,
            "debug": settings.DEBUG,
            "log_level": settings.LOG_LEVEL,
            "database_configured": bool(settings.DATABASE_URL),
            "redis_configured": bool(settings.REDIS_URL),
            "cors_origins": settings.CORS_ORIGINS,
            "features": {
                "auto_trading": settings.ENABLE_AUTO_TRADING,
                "paper_trading": settings.ENABLE_PAPER_TRADING,
                "ai_analysis": settings.ENABLE_AI_ANALYSIS
            },
            "apis_configured": {
                "bitpanda": bool(settings.BITPANDA_API_KEY),
                "azure_openai": bool(settings.AZURE_OPENAI_API_KEY),
                "deepseek": bool(settings.DEEPSEEK_API_KEY),
                "alpha_vantage": bool(settings.ALPHA_VANTAGE_API_KEY),
                "coingecko": bool(settings.COINGECKO_API_KEY)
            }
        }
    
    @router.get(
        "/debug/logs",
        summary="Recent Log Entries",
        description="Get recent log entries (debug only)",
        tags=["Debug"],
        include_in_schema=settings.DEBUG
    )
    async def debug_recent_logs(lines: int = 50) -> Dict[str, Any]:
        """Get recent log entries for debugging"""
        try:
            log_file = settings.LOGS_DIR / "app.log"
            if log_file.exists():
                with open(log_file, 'r') as f:
                    log_lines = f.readlines()
                    recent_lines = log_lines[-lines:] if len(log_lines) > lines else log_lines
                    
                return {
                    "total_lines": len(log_lines),
                    "returned_lines": len(recent_lines),
                    "logs": [line.strip() for line in recent_lines]
                }
            else:
                return {"error": "Log file not found"}
                
        except Exception as e:
            return {"error": f"Failed to read logs: {str(e)}"}

# ================================
# STARTUP HEALTH CHECK
# ================================

async def perform_startup_health_check() -> bool:
    """
    Perform health check during application startup
    Returns True if all critical services are healthy
    """
    try:
        # This w







      # ai-trading-bot/api/app/routers/portfolio.py
"""
Portfolio Router
Portfolio management, tracking, and performance analytics
"""

import logging
from datetime import datetime, timedelta
from decimal import Decimal
from typing import Any, Dict, List, Optional
from uuid import UUID

from fastapi import APIRouter, Depends, HTTPException, Query, status
from sqlalchemy import select, func, and_, desc
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

from app.core.dependencies import (
    get_db, 
    get_current_active_user, 
    get_user_portfolio,
    get_user_main_portfolio,
    get_pagination,
    get_timeframe_filter,
    rate_limit_standard
)
from app.models.database import User, Portfolio, Position, Order, Asset, PortfolioHistory
from app.models.schemas.portfolio import (
    PortfolioCreate,
    PortfolioUpdate,
    PortfolioResponse,
    PortfolioSummary,
    PortfolioHistory as PortfolioHistorySchema
)
from app.models.schemas.trading import PositionResponse, OrderResponse
from app.models.schemas.common import PaginatedResponse, SuccessResponse
from app.core.config import settings

logger = logging.getLogger(__name__)

# Create router
router = APIRouter()

# ================================
# PORTFOLIO CRUD OPERATIONS
# ================================

@router.get(
    "/",
    response_model=PaginatedResponse,
    summary="Get User Portfolios",
    description="Get all portfolios for the authenticated user",
    dependencies=[Depends(rate_limit_standard)]
)
async def get_user_portfolios(
    current_user: User = Depends(get_current_active_user),
    pagination: dict = Depends(get_pagination),
    db: AsyncSession = Depends(get_db)
) -> PaginatedResponse:
    """Get all portfolios for the current user"""
    
    try:
        # Count total portfolios
        count_query = select(func.count(Portfolio.id)).where(Portfolio.user_id == current_user.id)
        total_result = await db.execute(count_query)
        total = total_result.scalar()
        
        # Get portfolios with pagination
        portfolios_query = (
            select(Portfolio)
            .where(Portfolio.user_id == current_user.id)
            .order_by(desc(Portfolio.created_at))
            .offset(pagination["skip"])
            .limit(pagination["limit"])
        )
        
        result = await db.execute(portfolios_query)
        portfolios = result.scalars().all()
        
        # Convert to response models
        portfolio_responses = [PortfolioResponse.from_orm(portfolio) for portfolio in portfolios]
        
        return PaginatedResponse(
            items=portfolio_responses,
            total=total,
            skip=pagination["skip"],
            limit=pagination["limit"],
            has_next=(pagination["skip"] + pagination["limit"]) < total
        )
        
    except Exception as e:
        logger.error(f"Error fetching user portfolios: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to fetch portfolios"
        )

@router.post(
    "/",
    response_model=PortfolioResponse,
    status_code=status.HTTP_201_CREATED,
    summary="Create Portfolio",
    description="Create a new portfolio",
    dependencies=[Depends(rate_limit_standard)]
)
async def create_portfolio(
    portfolio_data: PortfolioCreate,
    current_user: User = Depends(get_current_active_user),
    db: AsyncSession = Depends(get_db)
) -> PortfolioResponse:
    """Create a new portfolio for the user"""
    
    try:
        # Check if portfolio name already exists for user
        existing_query = select(Portfolio).where(
            and_(Portfolio.user_id == current_user.id, Portfolio.name == portfolio_data.name)
        )
        existing_result = await db.execute(existing_query)
        if existing_result.scalar_one_or_none():
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Portfolio with this name already exists"
            )
        
        # Create new portfolio
        portfolio = Portfolio(
            user_id=current_user.id,
            name=portfolio_data.name,
            initial_balance=portfolio_data.initial_balance,
            current_balance=portfolio_data.initial_balance,
            currency=portfolio_data.currency,
            total_invested=Decimal('0'),
            total_profit_loss=Decimal('0')
        )
        
        db.add(portfolio)
        await db.commit()
        await db.refresh(portfolio)
        
        logger.info(f"Portfolio created: {portfolio.name} for user {current_user.username}")
        
        return PortfolioResponse.from_orm(portfolio)
        
    except HTTPException:
        raise
    except Exception as e:
        await db.rollback()
        logger.error(f"Error creating portfolio: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to create portfolio"
        )

@router.get(
    "/{portfolio_id}",
    response_model=PortfolioResponse,
    summary="Get Portfolio",
    description="Get specific portfolio details"
)
async def get_portfolio(
    portfolio: Portfolio = Depends(get_user_portfolio)
) -> PortfolioResponse:
    """Get specific portfolio details"""
    return PortfolioResponse.from_orm(portfolio)

@router.put(
    "/{portfolio_id}",
    response_model=PortfolioResponse,
    summary="Update Portfolio",
    description="Update portfolio information"
)
async def update_portfolio(
    portfolio_data: PortfolioUpdate,
    portfolio: Portfolio = Depends(get_user_portfolio),
    db: AsyncSession = Depends(get_db)
) -> PortfolioResponse:
    """Update portfolio information"""
    
    try:
        # Update only provided fields
        update_data = portfolio_data.dict(exclude_unset=True)
        
        # Check for name conflicts if name is being updated
        if "name" in update_data:
            existing_query = select(Portfolio).where(
                and_(
                    Portfolio.user_id == portfolio.user_id,
                    Portfolio.name == update_data["name"],
                    Portfolio.id != portfolio.id
                )
            )
            existing_result = await db.execute(existing_query)
            if existing_result.scalar_one_or_none():
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Portfolio with this name already exists"
                )
        
        # Update portfolio
        for field, value in update_data.items():
            setattr(portfolio, field, value)
        
        portfolio.updated_at = datetime.utcnow()
        
        await db.commit()
        await db.refresh(portfolio)
        
        logger.info(f"Portfolio updated: {portfolio.id}")
        
        return PortfolioResponse.from_orm(portfolio)
        
    except HTTPException:
        raise
    except Exception as e:
        await db.rollback()
        logger.error(f"Error updating portfolio: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to update portfolio"
        )

@router.delete(
    "/{portfolio_id}",
    response_model=SuccessResponse,
    summary="Delete Portfolio",
    description="Delete a portfolio (only if no active positions)"
)
async def delete_portfolio(
    portfolio: Portfolio = Depends(get_user_portfolio),
    db: AsyncSession = Depends(get_db)
) -> SuccessResponse:
    """Delete a portfolio"""
    
    try:
        # Check for active positions
        positions_query = select(func.count(Position.id)).where(
            and_(Position.portfolio_id == portfolio.id, Position.status == "open")
        )
        positions_result = await db.execute(positions_query)
        active_positions = positions_result.scalar()
        
        if active_positions > 0:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Cannot delete portfolio with {active_positions} active positions"
            )
        
        # Check for pending orders
        orders_query = select(func.count(Order.id)).where(
            and_(Order.portfolio_id == portfolio.id, Order.status == "pending")
        )
        orders_result = await db.execute(orders_query)
        pending_orders = orders_result.scalar()
        
        if pending_orders > 0:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Cannot delete portfolio with {pending_orders} pending orders"
            )
        
        # Soft delete by marking as inactive
        portfolio.is_active = False
        portfolio.name = f"deleted_{portfolio.id}_{portfolio.name}"
        
        await db.commit()
        
        logger.info(f"Portfolio deleted: {portfolio.id}")
        
        return SuccessResponse(
            message="Portfolio deleted successfully",
            data={"portfolio_id": str(portfolio.id)}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        await db.rollback()
        logger.error(f"Error deleting portfolio: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to delete portfolio"
        )

# ================================
# PORTFOLIO SUMMARY & ANALYTICS
# ================================

@router.get(
    "/{portfolio_id}/summary",
    response_model=PortfolioSummary,
    summary="Get Portfolio Summary",
    description="Get comprehensive portfolio summary with metrics"
)
async def get_portfolio_summary(
    portfolio: Portfolio = Depends(get_user_portfolio),
    db: AsyncSession = Depends(get_db)
) -> PortfolioSummary:
    """Get comprehensive portfolio summary with performance metrics"""
    
    try:
        # Get position count
        positions_query = select(func.count(Position.id)).where(
            and_(Position.portfolio_id == portfolio.id, Position.status == "open")
        )
        positions_result = await db.execute(positions_query)
        positions_count = positions_result.scalar()
        
        # Get active orders count
        orders_query = select(func.count(Order.id)).where(
            and_(Order.portfolio_id == portfolio.id, Order.status == "pending")
        )
        orders_result = await db.execute(orders_query)
        active_orders_count = orders_result.scalar()
        
        # Calculate performance metrics (simplified)
        performance_metrics = await calculate_portfolio_performance(portfolio.id, db)
        
        return PortfolioSummary(
            portfolio=PortfolioResponse.from_orm(portfolio),
            positions_count=positions_count,
            active_orders_count=active_orders_count,
            **performance_metrics
        )
        
    except Exception as e:
        logger.error(f"Error getting portfolio summary: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to get portfolio summary"
        )

@router.get(
    "/{portfolio_id}/performance",
    response_model=Dict[str, Any],
    summary="Get Portfolio Performance",
    description="Get detailed portfolio performance metrics and analytics"
)
async def get_portfolio_performance(
    portfolio: Portfolio = Depends(get_user_portfolio),
    timeframe: dict = Depends(get_timeframe_filter),
    db: AsyncSession = Depends(get_db)
) -> Dict[str, Any]:
    """Get detailed portfolio performance analytics"""
    
    try:
        # Set default timeframe if not provided
        end_date = timeframe.get("end_date", datetime.utcnow())
        start_date = timeframe.get("start_date", end_date - timedelta(days=30))
        
        # Get portfolio history for the timeframe
        history_query = (
            select(PortfolioHistory)
            .where(
                and_(
                    PortfolioHistory.portfolio_id == portfolio.id,
                    PortfolioHistory.time >= start_date,
                    PortfolioHistory.time <= end_date
                )
            )
            .order_by(PortfolioHistory.time)
        )
        
        history_result = await db.execute(history_query)
        history_data = history_result.scalars().all()
        
        if not history_data:
            return {
                "message": "No performance data available for the selected timeframe",
                "timeframe": {"start": start_date, "end": end_date},
                "metrics": {}
            }
        
        # Calculate performance metrics
        metrics = calculate_performance_metrics(history_data, portfolio.initial_balance)
        
        # Prepare chart data
        chart_data = [
            {
                "timestamp": record.time.isoformat(),
                "total_value": float(record.total_value),
                "unrealized_pnl": float(record.unrealized_pnl),
                "realized_pnl": float(record.realized_pnl),
                "daily_return": float(record.daily_return) if record.daily_return else 0
            }
            for record in history_data
        ]
        
        return {
            "portfolio_id": str(portfolio.id),
            "timeframe": {"start": start_date, "end": end_date},
            "metrics": metrics,
            "chart_data": chart_data
        }
        
    except Exception as e:
        logger.error(f"Error getting portfolio performance: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to get portfolio performance"
        )

# ================================
# PORTFOLIO POSITIONS
# ================================

@router.get(
    "/{portfolio_id}/positions",
    response_model=PaginatedResponse,
    summary="Get Portfolio Positions",
    description="Get all positions in the portfolio"
)
async def get_portfolio_positions(
    portfolio: Portfolio = Depends(get_user_portfolio),
    pagination: dict = Depends(get_pagination),
    status_filter: Optional[str] = Query(None, description="Filter by position status"),
    db: AsyncSession = Depends(get_db)
) -> PaginatedResponse:
    """Get all positions in the portfolio"""
    
    try:
        # Build query with filters
        query_filters = [Position.portfolio_id == portfolio.id]
        if status_filter:
            query_filters.append(Position.status == status_filter)
        
        # Count total positions
        count_query = select(func.count(Position.id)).where(and_(*query_filters))
        total_result = await db.execute(count_query)
        total = total_result.scalar()
        
        # Get positions with asset information
        positions_query = (
            select(Position)
            .options(selectinload(Position.asset))
            .where(and_(*query_filters))
            .order_by(desc(Position.opened_at))
            .offset(pagination["skip"])
            .limit(pagination["limit"])
        )
        
        result = await db.execute(positions_query)
        positions = result.scalars().all()
        
        # Convert to response models with asset symbol
        position_responses = []
        for position in positions:
            position_data = PositionResponse.from_orm(position)
            position_data.asset_symbol = position.asset.symbol
            position_responses.append(position_data)
        
        return PaginatedResponse(
            items=position_responses,
            total=total,
            skip=pagination["skip"],
            limit=pagination["limit"],
            has_next=(pagination["skip"] + pagination["limit"]) < total
        )
        
    except Exception as e:
        logger.error(f"Error getting portfolio positions: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to get portfolio positions"
        )

# ================================
# PORTFOLIO ORDERS
# ================================

@router.get(
    "/{portfolio_id}/orders",
    response_model=PaginatedResponse,
    summary="Get Portfolio Orders",
    description="Get all orders in the portfolio"
)
async def get_portfolio_orders(
    portfolio: Portfolio = Depends(get_user_portfolio),
    pagination: dict = Depends(get_pagination),
    status_filter: Optional[str] = Query(None, description="Filter by order status"),
    db: AsyncSession = Depends(get_db)
) -> PaginatedResponse:
    """Get all orders in the portfolio"""
    
    try:
        # Build query with filters
        query_filters = [Order.portfolio_id == portfolio.id]
        if status_filter:
            query_filters.append(Order.status == status_filter)
        
        # Count total orders
        count_query = select(func.count(Order.id)).where(and_(*query_filters))
        total_result = await db.execute(count_query)
        total = total_result.scalar()
        
        # Get orders with asset information
        orders_query = (
            select(Order)
            .options(selectinload(Order.asset))
            .where(and_(*query_filters))
            .order_by(desc(Order.created_at))
            .offset(pagination["skip"])
            .limit(pagination["limit"])
        )
        
        result = await db.execute(orders_query)
        orders = result.scalars().all()
        
        # Convert to response models with asset symbol
        order_responses = []
        for order in orders:
            order_data = OrderResponse.from_orm(order)
            order_data.asset_symbol = order.asset.symbol
            order_responses.append(order_data)
        
        return PaginatedResponse(
            items=order_responses,
            total=total,
            skip=pagination["skip"],
            limit=pagination["limit"],
            has_next=(pagination["skip"] + pagination["limit"]) < total
        )
        
    except Exception as e:
        logger.error(f"Error getting portfolio orders: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to get portfolio orders"
        )

# ================================
# PORTFOLIO HISTORY
# ================================

@router.get(
    "/{portfolio_id}/history",
    response_model=List[PortfolioHistorySchema],
    summary="Get Portfolio History",
    description="Get portfolio value history over time"
)
async def get_portfolio_history(
    portfolio: Portfolio = Depends(get_user_portfolio),
    timeframe: dict = Depends(get_timeframe_filter),
    interval: str = Query("1d", description="Data interval (1h, 1d, 1w)"),
    db: AsyncSession = Depends(get_db)
) -> List[PortfolioHistorySchema]:
    """Get portfolio value history over time"""
    
    try:
        # Set default timeframe if not provided
        end_date = timeframe.get("end_date", datetime.utcnow())
        start_date = timeframe.get("start_date", end_date - timedelta(days=30))
        
        # Determine time bucket based on interval
        interval_mapping = {
            "1h": "1 hour",
            "1d": "1 day", 
            "1w": "1 week"
        }
        time_bucket = interval_mapping.get(interval, "1 day")
        
        # Query portfolio history with time bucketing
        history_query = f"""
        SELECT 
            time_bucket('{time_bucket}', time) as bucket,
            last(total_value, time) as total_value,
            last(cash_balance, time) as cash_balance,
            last(invested_value, time) as invested_value,
            last(unrealized_pnl, time) as unrealized_pnl,
            last(realized_pnl, time) as realized_pnl,
            avg(daily_return) as daily_return
        FROM portfolio_history 
        WHERE portfolio_id = :portfolio_id 
        AND time >= :start_date 
        AND time <= :end_date
        GROUP BY bucket
        ORDER BY bucket



  
  


# ai-trading-bot/api/Dockerfile
# Multi-stage Dockerfile for FastAPI Trading Bot API

# ================================
# STAGE 1: Base Python Image
# ================================
FROM python:3.11-slim as base

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Create app user
RUN groupadd -r app && useradd -r -g app app

# Set work directory
WORKDIR /app

# ================================
# STAGE 2: Dependencies
# ================================
FROM base as dependencies

# Copy requirements files
COPY requirements.txt requirements-dev.txt ./

# Install Python dependencies
RUN pip install --upgrade pip && \
    pip install -r requirements.txt

# ================================
# STAGE 3: Development
# ================================
FROM dependencies as development

# Install development dependencies
RUN pip install -r requirements-dev.txt

# Copy application code
COPY . .

# Change ownership to app user
RUN chown -R app:app /app

# Switch to app user
USER app

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health/ping || exit 1

# Default command for development
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

# ================================
# STAGE 4: Production
# ================================
FROM dependencies as production

# Copy only application code (no dev dependencies)
COPY . .

# Change ownership to app user
RUN chown -R app:app /app

# Switch to app user
USER app

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health/ping || exit 1

# Production command with Gunicorn
CMD ["gunicorn", "app.main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000"]

---
# ai-trading-bot/api/requirements.txt
# FastAPI and ASGI server
fastapi==0.104.1
uvicorn[standard]==0.24.0
gunicorn==21.2.0

# Database
sqlalchemy[asyncio]==2.0.23
asyncpg==0.29.0
alembic==1.13.1

# Authentication & Security
passlib[bcrypt]==1.7.4
python-jose[cryptography]==3.3.0
python-multipart==0.0.6

# HTTP client
httpx==0.25.2
aiohttp==3.9.1

# Redis
redis[hiredis]==5.0.1

# Data validation
pydantic[email]==2.5.0
pydantic-settings==2.1.0

# Rate limiting
slowapi==0.1.9

# Monitoring & Logging
prometheus-client==0.19.0
structlog==23.2.0

# System metrics
psutil==5.9.6

# Date/Time utilities
python-dateutil==2.8.2

# Environment variables
python-dotenv==1.0.0

# Math and calculations
numpy==1.25.2
pandas==2.1.4

# Async utilities
asyncio-throttle==1.0.2

---
# ai-trading-bot/api/requirements-dev.txt
# Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
pytest-mock==3.12.0
httpx==0.25.2

# Code quality
black==23.11.0
isort==5.12.0
flake8==6.1.0
mypy==1.7.1
bandit==1.7.5

# Pre-commit hooks
pre-commit==3.6.0

# Development tools
ipython==8.17.2
ipdb==0.13.13

# Documentation
mkdocs==1.5.3
mkdocs-material==9.4.8

---
# ai-trading-bot/docker-compose.yml
# Docker Compose for AI Trading Bot Development Environment

version: '3.8'

services:
  # ================================
  # DATABASE SERVICES
  # ================================
  postgres:
    image: timescale/timescaledb:2.11.0-pg15
    container_name: trading_postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-trading_bot}
      POSTGRES_USER: ${POSTGRES_USER:-trading_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-trading_password}
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init:/docker-entrypoint-initdb.d
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-trading_user} -d ${POSTGRES_DB:-trading_bot}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - trading_network

  redis:
    image: redis:7.0-alpine
    container_name: trading_redis
    command: redis-server --requirepass ${REDIS_PASSWORD:-redis_password}
    volumes:
      - redis_data:/data
    ports:
      - "${REDIS_PORT:-6379}:6379"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - trading_network

  # ================================
  # FASTAPI APPLICATION
  # ================================
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
      target: development
    container_name: trading_api
    environment:
      # Database
      - DATABASE_URL=postgresql://trading_user:${POSTGRES_PASSWORD:-trading_password}@postgres:5432/${POSTGRES_DB:-trading_bot}
      
      # Redis
      - REDIS_URL=redis://:${REDIS_PASSWORD:-redis_password}@redis:6379/0
      
      # Application
      - ENVIRONMENT=development
      - DEBUG=true
      - LOG_LEVEL=INFO
      
      # Security
      - JWT_SECRET_KEY=${JWT_SECRET_KEY:-super_secret_jwt_key_change_in_production}
      - SECRET_KEY=${SECRET_KEY:-super_secret_app_key_change_in_production}
      
      # External APIs (optional for development)
      - BITPANDA_API_KEY=${BITPANDA_API_KEY:-}
      - BITPANDA_API_SECRET=${BITPANDA_API_SECRET:-}
      - BITPANDA_SANDBOX=true
      
      # AI APIs (optional for development)
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY:-}
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT:-}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:-}
      
      # Market Data APIs (optional for development)
      - ALPHA_VANTAGE_API_KEY=${ALPHA_VANTAGE_API_KEY:-}
      - COINGECKO_API_KEY=${COINGECKO_API_KEY:-}
      - NEWS_API_KEY=${NEWS_API_KEY:-}
      
      # CORS
      - CORS_ORIGINS=http://localhost:3000,http://localhost:8000
      
      # Features
      - ENABLE_AUTO_TRADING=false
      - ENABLE_PAPER_TRADING=true
      - ENABLE_AI_ANALYSIS=true
      - ENABLE_WEBSOCKET=true
      - ENABLE_RISK_ALERTS=true
    
    volumes:
      - ./api:/app
      - api_logs:/app/logs
    
    ports:
      - "${API_PORT:-8000}:8000"
    
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    
    restart: unless-stopped
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    networks:
      - trading_network
    
    # Development: enable hot reload
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload", "--log-level", "info"]

  # ================================
  # MONITORING SERVICES (Optional)
  # ================================
  
  # Uncomment for monitoring setup
  # prometheus:
  #   image: prom/prometheus:v2.40.0
  #   container_name: trading_prometheus
  #   command:
  #     - '--config.file=/etc/prometheus/prometheus.yml'
  #     - '--storage.tsdb.path=/prometheus'
  #     - '--web.console.libraries=/etc/prometheus/console_libraries'
  #     - '--web.console.templates=/etc/prometheus/consoles'
  #     - '--storage.tsdb.retention.time=200h'
  #     - '--web.enable-lifecycle'
  #   volumes:
  #     - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
  #     - prometheus_data:/prometheus
  #   ports:
  #     - "9090:9090"
  #   networks:
  #     - trading_network
  #   restart: unless-stopped

  # grafana:
  #   image: grafana/grafana:9.3.0
  #   container_name: trading_grafana
  #   environment:
  #     - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
  #     - GF_USERS_ALLOW_SIGN_UP=false
  #   volumes:
  #     - grafana_data:/var/lib/grafana
  #   ports:
  #     - "3001:3000"
  #   networks:
  #     - trading_network
  #   restart: unless-stopped

# ================================
# VOLUMES
# ================================
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  api_logs:
    driver: local
  # prometheus_data:
  #   driver: local
  # grafana_data:
  #   driver: local

# ================================
# NETWORKS
# ================================
networks:
  trading_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

---
# ai-trading-bot/.env.example
# Environment Variables for AI Trading Bot
# Copy this file to .env and fill in your values

# ================================
# DATABASE CONFIGURATION
# ================================
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=trading_user
POSTGRES_PASSWORD=trading_password_change_me
POSTGRES_DB=trading_bot
DATABASE_URL=postgresql://trading_user:trading_password_change_me@localhost:5432/trading_bot

# ================================
# REDIS CONFIGURATION
# ================================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=redis_password_change_me
REDIS_DB=0
REDIS_URL=redis://:redis_password_change_me@localhost:6379/0

# ================================
# APPLICATION SETTINGS
# ================================
ENVIRONMENT=development
DEBUG=true
LOG_LEVEL=INFO
SECRET_KEY=super_secret_app_key_change_in_production
JWT_SECRET_KEY=super_secret_jwt_key_change_in_production
JWT_ALGORITHM=HS256
JWT_EXPIRE_MINUTES=60

# ================================
# EXTERNAL API KEYS
# ================================

# Bitpanda API (for real trading)
BITPANDA_API_KEY=your_bitpanda_api_key_here
BITPANDA_API_SECRET=your_bitpanda_api_secret_here
BITPANDA_SANDBOX=true

# Azure OpenAI (for AI analysis)
AZURE_OPENAI_API_KEY=your_azure_openai_key_here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_MODEL=gpt-4
AZURE_OPENAI_VERSION=2024-02-01

# DeepSeek API
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_BASE_URL=https://api.deepseek.com

# Ollama (local AI)
OLLAMA_ENDPOINT=http://localhost:11434
OLLAMA_MODEL_GEMINI=gemma:7b
OLLAMA_MODEL_MISTRAL=mistral:7b

# Market Data APIs
ALPHA_VANTAGE_API_KEY=your_alpha_vantage_key_here
COINGECKO_API_KEY=your_coingecko_key_here
NEWS_API_KEY=your_news_api_key_here

# Social Media APIs
TWITTER_BEARER_TOKEN=your_twitter_bearer_token_here
REDDIT_CLIENT_ID=your_reddit_client_id_here
REDDIT_CLIENT_SECRET=your_reddit_client_secret_here

# ================================
# CORS & SECURITY
# ================================
CORS_ORIGINS=http://localhost:3000,http://localhost:8000
ALLOWED_HOSTS=localhost,127.0.0.1,0.0.0.0

# ================================
# RATE LIMITING
# ================================
RATE_LIMIT_PER_MINUTE=100
RATE_LIMIT_TRADING_PER_MINUTE=50
RATE_LIMIT_AI_PER_MINUTE=20

# ================================
# TRADING CONFIGURATION
# ================================
DEFAULT_RISK_TOLERANCE=0.05
MAX_PORTFOLIO_RISK=0.15
DEFAULT_STOP_LOSS=0.05
DEFAULT_TAKE_PROFIT=0.15
MIN_TRADE_AMOUNT=10.00
MAX_DAILY_TRADES=100
MAX_PORTFOLIO_POSITIONS=50

# ================================
# FEATURE FLAGS
# ================================
ENABLE_AUTO_TRADING=false
ENABLE_PAPER_TRADING=true
ENABLE_RISK_ALERTS=true
ENABLE_EMAIL_NOTIFICATIONS=false
ENABLE_WEBSOCKET=true
ENABLE_AI_ANALYSIS=true

# ================================
# MONITORING
# ================================
ENABLE_METRICS=true
METRICS_PORT=9090
GRAFANA_PASSWORD=admin

# ================================
# EMAIL CONFIGURATION (Optional)
# ================================
SMTP_TLS=true
SMTP_PORT=587
SMTP_HOST=smtp.gmail.com
SMTP_USER=your_email@gmail.com
SMTP_PASSWORD=your_app_password
EMAILS_FROM_EMAIL=noreply@ai-trading-bot.com
EMAILS_FROM_NAME=AI Trading Bot

# ================================
# DOCKER COMPOSE OVERRIDES
# ================================
API_PORT=8000

---
# ai-trading-bot/Makefile
# Makefile for AI Trading Bot Development

.PHONY: help setup build up down logs test clean lint format install

# Default target
help: ## Show this help message
	@echo "AI Trading Bot - Development Commands"
	@echo "====================================="
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-20s\033[0m %s\n", $$1, $$2}'

# Setup and Installation
setup: ## Initial project setup
	@echo "🚀 Setting up AI Trading Bot development environment..."
	cp .env.example .env
	@echo "📝 Please edit .env file with your configuration"
	make install
	make build

install: ## Install Python dependencies
	@echo "📦 Installing Python dependencies..."
	cd api && pip install -r requirements.txt -r requirements-dev.txt
	@echo "🔧 Setting up pre-commit hooks..."
	pre-commit install

# Docker Operations
build: ## Build all Docker images
	@echo "🔨 Building Docker images..."
	docker-compose build

up: ## Start all services
	@echo "🚀 Starting all services..."
	docker-compose up -d

down: ## Stop all services
	@echo "🛑 Stopping all services..."
	docker-compose down

restart: ## Restart all services
	@echo "🔄 Restarting all services..."
	docker-compose restart

logs: ## Show logs for all services
	@echo "📋 Showing logs..."
	docker-compose logs -f

logs-api: ## Show API logs only
	@echo "📋 Showing API logs..."
	docker-compose logs -f api

logs-db: ## Show database logs only
	@echo "📋 Showing database logs..."
	docker-compose logs -f postgres

# Development
dev: ## Start development environment
	@echo "💻 Starting development environment..."
	docker-compose up --build

dev-detached: ## Start development environment in background
	@echo "💻 Starting development environment (detached)..."
	docker-compose up -d --build

# Database Operations
db-migrate: ## Run database migrations
	@echo "🗄️ Running database migrations..."
	docker-compose exec api alembic upgrade head

db-reset: ## Reset database (WARNING: This will delete all data)
	@echo "⚠️ Resetting database..."
	@read -p "Are you sure? This will delete all data! [y/N] " -n 1 -r; \
	echo; \
	if [[ $$REPLY =~ ^[Yy]$$ ]]; then \
		docker-compose down -v; \
		docker-compose up -d postgres redis; \
		sleep 10; \
		make db-migrate; \
	fi

db-seed: ## Seed database with test data
	@echo "🌱 Seeding database..."
	docker-compose exec api python scripts/seed_data.py

db-shell: ## Access database shell
	@echo "🗄️ Opening database shell..."
	docker-compose exec postgres psql -U trading_user -d trading_bot

# Testing
test: ## Run all tests
	@echo "🧪 Running tests..."
	docker-compose exec api pytest tests/ -v --cov=app

test-unit: ## Run unit tests only
	@echo "🧪 Running unit tests..."
	docker-compose exec api pytest tests/unit/ -v

test-integration: ## Run integration tests only
	@echo "🧪 Running integration tests..."
	docker-compose exec api pytest tests/integration/ -v

test-coverage: ## Run tests with coverage report
	@echo "🧪 Running tests with coverage..."
	docker-compose exec api pytest tests/ --cov=app --cov-report=html
	@echo "📊 Coverage report generated in htmlcov/"

# Code Quality
lint: ## Run all linters
	@echo "🔍 Running linters..."
	docker-compose exec api flake8 app/
	docker-compose exec api mypy app/
	docker-compose exec api bandit -r app/

format: ## Format code
	@echo "✨ Formatting code..."
	docker-compose exec api black app/ tests/
	docker-compose exec api isort app/ tests/

format-check: ## Check code formatting
	@echo "🔍 Checking code format..."
	docker-compose exec api black --check app/ tests/
	docker-compose exec api isort --check-only app/ tests/

pre-commit: ## Run pre-commit hooks
	@echo "🔧 Running pre-commit hooks..."
	pre-commit run --all-files

# API Operations
api-shell: ## Access API container shell
	@echo "🐚 Opening API shell..."
	docker-compose exec api bash

api-python: ## Access Python shell in API container
	@echo "🐍 Opening Python shell..."
	docker-compose exec api python

api-docs: ## Open API documentation
	@echo "📚 API documentation available at:"
	@echo "http://localhost:8000/docs"

# Health Checks
health: ## Check service health
	@echo "🏥 Checking service health..."
	@curl -f http://localhost:8000/health || echo "❌ API not responding"
	@curl -f http://localhost:8000/health/ready || echo "❌ API not ready"

ping: ## Ping API service
	@echo "🏓 Pinging API..."
	@curl -f http://localhost:8000/health/ping || echo "❌ API not responding"

# Monitoring
metrics: ## View metrics
	@echo "📊 Metrics available at:"
	@echo "http://localhost:8000/health/metrics"
	@curl -s http://localhost:8000/health/metrics | head -20

# Cleanup
clean: ## Clean up Docker resources
	@echo "🧹 Cleaning up..."
	docker-compose down -v --remove-orphans
	docker system prune -f

clean-all: ## Clean up everything (images, volumes, etc.)
	@echo "🧹 Deep cleaning..."
	@read -p "This will remove all Docker images and volumes. Continue? [y/N] " -n 1 -r; \
	echo; \
	if [[ $$REPLY =~ ^[Yy]$$ ]]; then \
		docker-compose down -v --remove-orphans; \
		docker system prune -af; \
		docker volume prune -f; \
	fi

# Security
security-scan: ## Run security scans
	@echo "🔒 Running security scans..."
	docker-compose exec api bandit -r app/ -f json -o bandit-report.json
	@echo "📋 Security report saved to bandit-report.json"

# Backup
backup-db: ## Backup database
	@echo "💾 Backing up database..."
	docker-compose exec postgres pg_dump -U trading_user trading_bot > backup_$(shell date +%Y%m%d_%H%M%S).sql
	@echo "✅ Database backup created"

# Environment
env-check: ## Check environment variables
	@echo "🔍 Checking environment variables..."
	@docker-compose exec api python -c "from app.core.config import settings; print('✅ Configuration loaded successfully')"

env-example: ## Update .env.example from current .env
	@echo "📝 Updating .env.example..."
	@cp .env .env.example.tmp
	@sed 's/=.*/=/' .env.example.tmp > .env.example
	@rm .env.example.tmp
	@echo "✅ .env.example updated"

# Load Testing
load-test: ## Run basic load test
	@echo "⚡ Running load test..."
	@echo "Install httpie first: pip install httpie"
	@for i in {1..10}; do \
		echo "Request $$i:"; \
		time curl -s http://localhost:8000/health/ping > /dev/null; \
	done

# Development Shortcuts
quick-start: build up ## Quick start (build and run)
	@echo "🚀 Quick start complete!"
	@echo "📚 API Docs: http://localhost:8000/docs"
	@echo "🏥 Health Check: http://localhost:8000/health"

full-restart: down clean build up ## Full restart with cleanup
	@echo "🔄 Full restart complete!"

# Status
status: ## Show service status
	@echo "📊 Service Status:"
	@docker-compose ps

# Performance
performance: ## Show performance metrics
	@echo "⚡ Performance Metrics:"
	@docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"





 # ai-trading-bot/api/app/core/logging.py
"""
Advanced Logging System with Structured Logging
JSON formatting, multiple handlers, and performance monitoring
"""

import json
import logging
import logging.config
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict

import structlog
from pythonjsonlogger import jsonlogger

from app.core.config import settings


# ================================
# CUSTOM LOG FORMATTERS
# ================================

class CustomJSONFormatter(jsonlogger.JsonFormatter):
    """Custom JSON formatter with additional fields"""
    
    def add_fields(self, log_record: Dict[str, Any], record: logging.LogRecord, message_dict: Dict[str, Any]) -> None:
        super().add_fields(log_record, record, message_dict)
        
        # Add timestamp in ISO format
        log_record['timestamp'] = datetime.utcnow().isoformat()
        
        # Add service information
        log_record['service'] = 'ai-trading-bot-api'
        log_record['version'] = settings.PROJECT_VERSION
        log_record['environment'] = settings.ENVIRONMENT
        
        # Add level name
        log_record['level'] = record.levelname
        
        # Add logger name
        log_record['logger'] = record.name
        
        # Add process/thread info for debugging
        log_record['process_id'] = record.process
        log_record['thread_id'] = record.thread
        
        # Add filename and line number for debugging
        if settings.DEBUG:
            log_record['file'] = record.filename
            log_record['line'] = record.lineno
            log_record['function'] = record.funcName


class ColoredFormatter(logging.Formatter):
    """Colored formatter for console output"""
    
    # Color codes
    COLORS = {
        'DEBUG': '\033[36m',      # Cyan
        'INFO': '\033[32m',       # Green
        'WARNING': '\033[33m',    # Yellow
        'ERROR': '\033[31m',      # Red
        'CRITICAL': '\033[35m',   # Magenta
        'RESET': '\033[0m'        # Reset
    }
    
    def format(self, record: logging.LogRecord) -> str:
        # Add color to level name
        level_color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        record.levelname = f"{level_color}{record.levelname}{self.COLORS['RESET']}"
        
        # Format timestamp
        timestamp = datetime.fromtimestamp(record.created).strftime('%Y-%m-%d %H:%M:%S')
        
        # Create formatted message
        message = super().format(record)
        
        return f"{self.COLORS['RESET']}{timestamp} | {message}{self.COLORS['RESET']}"


# ================================
# PERFORMANCE LOGGING
# ================================

class PerformanceFilter(logging.Filter):
    """Filter to add performance metrics to log records"""
    
    def filter(self, record: logging.LogRecord) -> bool:
        # Add memory usage info
        try:
            import psutil
            process = psutil.Process()
            record.memory_mb = round(process.memory_info().rss / 1024 / 1024, 2)
            record.cpu_percent = process.cpu_percent()
        except ImportError:
            record.memory_mb = 0
            record.cpu_percent = 0
        
        return True


class RequestContextFilter(logging.Filter):
    """Filter to add request context to log records"""
    
    def filter(self, record: logging.LogRecord) -> bool:
        # Add request context if available
        import contextvars
        
        try:
            # These would be set by middleware
            record.request_id = contextvars.copy_context().get('request_id', 'unknown')
            record.user_id = contextvars.copy_context().get('user_id', 'anonymous')
            record.endpoint = contextvars.copy_context().get('endpoint', 'unknown')
        except Exception:
            record.request_id = 'unknown'
            record.user_id = 'anonymous'
            record.endpoint = 'unknown'
        
        return True


# ================================
# LOGGING CONFIGURATION
# ================================

def get_logging_config() -> Dict[str, Any]:
    """Get logging configuration dictionary"""
    
    # Ensure logs directory exists
    settings.LOGS_DIR.mkdir(exist_ok=True)
    
    config = {
        'version': 1,
        'disable_existing_loggers': False,
        
        'formatters': {
            'json': {
                '()': CustomJSONFormatter,
                'format': '%(timestamp)s %(level)s %(name)s %(message)s'
            },
            'colored': {
                '()': ColoredFormatter,
                'format': '%(levelname)s | %(name)s | %(message)s'
            },
            'detailed': {
                'format': '%(asctime)s | %(levelname)-8s | %(name)-20s | %(funcName)-15s:%(lineno)-3d | %(message)s',
                'datefmt': '%Y-%m-%d %H:%M:%S'
            },
            'simple': {
                'format': '%(levelname)s | %(name)s | %(message)s'
            }
        },
        
        'filters': {
            'performance': {
                '()': PerformanceFilter,
            },
            'request_context': {
                '()': RequestContextFilter,
            }
        },
        
        'handlers': {
            # Console handler with colors (development)
            'console': {
                'class': 'logging.StreamHandler',
                'level': 'DEBUG' if settings.DEBUG else 'INFO',
                'formatter': 'colored' if settings.DEBUG else 'simple',
                'stream': sys.stdout,
                'filters': ['request_context']
            },
            
            # File handler for all logs
            'file_all': {
                'class': 'logging.handlers.RotatingFileHandler',
                'level': 'DEBUG',
                'formatter': 'json',
                'filename': str(settings.LOGS_DIR / 'app.log'),
                'maxBytes': 10_000_000,  # 10MB
                'backupCount': 5,
                'filters': ['performance', 'request_context']
            },
            
            # File handler for errors only
            'file_error': {
                'class': 'logging.handlers.RotatingFileHandler',
                'level': 'ERROR',
                'formatter': 'json',
                'filename': str(settings.LOGS_DIR / 'errors.log'),
                'maxBytes': 5_000_000,  # 5MB
                'backupCount': 10,
                'filters': ['performance', 'request_context']
            },
            
            # File handler for trading operations
            'file_trading': {
                'class': 'logging.handlers.RotatingFileHandler',
                'level': 'INFO',
                'formatter': 'json',
                'filename': str(settings.LOGS_DIR / 'trading.log'),
                'maxBytes': 20_000_000,  # 20MB
                'backupCount': 10,
                'filters': ['performance', 'request_context']
            },
            
            # File handler for security events
            'file_security': {
                'class': 'logging.handlers.RotatingFileHandler',
                'level': 'WARNING',
                'formatter': 'json',
                'filename': str(settings.LOGS_DIR / 'security.log'),
                'maxBytes': 5_000_000,  # 5MB
                'backupCount': 20,
                'filters': ['performance', 'request_context']
            }
        },
        
        'loggers': {
            # Root logger
            '': {
                'level': settings.LOG_LEVEL,
                'handlers': ['console', 'file_all', 'file_error']
            },
            
            # Application loggers
            'app': {
                'level': 'DEBUG' if settings.DEBUG else 'INFO',
                'handlers': ['console', 'file_all'],
                'propagate': False
            },
            
            # Trading specific logger
            'app.trading': {
                'level': 'INFO',
                'handlers': ['console', 'file_trading', 'file_all'],
                'propagate': False
            },
            
            # Security logger
            'app.security': {
                'level': 'WARNING',
                'handlers': ['console', 'file_security', 'file_all'],
                'propagate': False
            },
            
            # Database logger
            'sqlalchemy': {
                'level': 'WARNING',
                'handlers': ['file_all'],
                'propagate': False
            },
            
            # SQLAlchemy engine logger (for SQL queries in debug)
            'sqlalchemy.engine': {
                'level': 'INFO' if settings.DEBUG else 'WARNING',
                'handlers': ['console'] if settings.DEBUG else ['file_all'],
                'propagate': False
            },
            
            # FastAPI/Uvicorn loggers
            'uvicorn': {
                'level': 'INFO',
                'handlers': ['console', 'file_all'],
                'propagate': False
            },
            
            'uvicorn.access': {
                'level': 'INFO',
                'handlers': ['file_all'],
                'propagate': False
            },
            
            # External libraries
            'httpx': {
                'level': 'WARNING',
                'handlers': ['file_all'],
                'propagate': False
            },
            
            'aiohttp': {
                'level': 'WARNING',
                'handlers': ['file_all'],
                'propagate': False
            }
        }
    }
    
    return config


def setup_logging(logger_name: str = None) -> logging.Logger:
    """
    Setup logging configuration and return logger
    
    Args:
        logger_name: Name of the logger to return. If None, returns root logger.
    
    Returns:
        Configured logger instance
    """
    
    # Apply logging configuration
    config = get_logging_config()
    logging.config.dictConfig(config)
    
    # Get logger
    logger = logging.getLogger(logger_name or __name__)
    
    # Log startup information
    if logger_name is None:
        logger.info(
            "Logging system initialized",
            extra={
                'environment': settings.ENVIRONMENT,
                'log_level': settings.LOG_LEVEL,
                'debug_mode': settings.DEBUG,
                'logs_directory': str(settings.LOGS_DIR)
            }
        )
    
    return logger


# ================================
# STRUCTURED LOGGING WITH STRUCTLOG
# ================================

def configure_structlog() -> None:
    """Configure structlog for structured logging"""
    
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )


# ================================
# PERFORMANCE MONITORING
# ================================

class LoggingTimer:
    """Context manager for logging execution time"""
    
    def __init__(self, logger: logging.Logger, operation: str, level: int = logging.INFO):
        self.logger = logger
        self.operation = operation
        self.level = level
        self.start_time = None
    
    def __enter__(self):
        self.start_time = time.time()
        self.logger.log(self.level, f"Starting {self.operation}")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        execution_time = time.time() - self.start_time
        
        if exc_type is None:
            self.logger.log(
                self.level,
                f"Completed {self.operation}",
                extra={'execution_time': round(execution_time, 3)}
            )
        else:
            self.logger.error(
                f"Failed {self.operation}",
                extra={
                    'execution_time': round(execution_time, 3),
                    'error_type': exc_type.__name__,
                    'error_message': str(exc_val)
                },
                exc_info=True
            )


def log_performance(operation: str, level: int = logging.INFO):
    """Decorator for logging function performance"""
    
    def decorator(func):
        def wrapper(*args, **kwargs):
            logger = logging.getLogger(func.__module__)
            
            with LoggingTimer(logger, f"{func.__name__}:{operation}", level):
                return func(*args, **kwargs)
        
        return wrapper
    return decorator


# ================================
# TRADING SPECIFIC LOGGERS
# ================================

class TradingLogger:
    """Specialized logger for trading operations"""
    
    def __init__(self):
        self.logger = logging.getLogger('app.trading')
    
    def order_created(self, order_id: str, symbol: str, order_type: str, quantity: float, price: float = None):
        """Log order creation"""
        self.logger.info(
            f"Order created: {order_type} {quantity} {symbol}",
            extra={
                'event': 'order_created',
                'order_id': order_id,
                'symbol': symbol,
                'order_type': order_type,
                'quantity': quantity,
                'price': price
            }
        )
    
    def order_executed(self, order_id: str, symbol: str, executed_price: float, executed_quantity: float):
        """Log order execution"""
        self.logger.info(
            f"Order executed: {executed_quantity} {symbol} at {executed_price}",
            extra={
                'event': 'order_executed',
                'order_id': order_id,
                'symbol': symbol,
                'executed_price': executed_price,
                'executed_quantity': executed_quantity
            }
        )
    
    def position_opened(self, position_id: str, symbol: str, quantity: float, price: float):
        """Log position opening"""
        self.logger.info(
            f"Position opened: {quantity} {symbol} at {price}",
            extra={
                'event': 'position_opened',
                'position_id': position_id,
                'symbol': symbol,
                'quantity': quantity,
                'price': price
            }
        )
    
    def position_closed(self, position_id: str, symbol: str, pnl: float):
        """Log position closing"""
        self.logger.info(
            f"Position closed: {symbol} with P&L {pnl}",
            extra={
                'event': 'position_closed',
                'position_id': position_id,
                'symbol': symbol,
                'pnl': pnl
            }
        )
    
    def risk_alert(self, alert_type: str, message: str, severity: str = 'medium'):
        """Log risk management alerts"""
        self.logger.warning(
            f"Risk Alert [{severity.upper()}]: {message}",
            extra={
                'event': 'risk_alert',
                'alert_type': alert_type,
                'severity': severity,
                'message': message
            }
        )


class SecurityLogger:
    """Specialized logger for security events"""
    
    def __init__(self):
        self.logger = logging.getLogger('app.security')
    
    def login_attempt(self, username: str, success: bool, ip_address: str = None):
        """Log login attempts"""
        level = logging.INFO if success else logging.WARNING
        
        self.logger.log(
            level,
            f"Login {'successful' if success else 'failed'} for user: {username}",
            extra={
                'event': 'login_attempt',
                'username': username,
                'success': success,
                'ip_address': ip_address
            }
        )
    
    def failed_authentication(self, username: str, reason: str, ip_address: str = None):
        """Log failed authentication"""
        self.logger.warning(
            f"Authentication failed for {username}: {reason}",
            extra={
                'event': 'auth_failed',
                'username': username,
                'reason': reason,
                'ip_address': ip_address
            }
        )
    
    def account_locked(self, username: str, ip_address: str = None):
        """Log account lockouts"""
        self.logger.error(
            f"Account locked: {username}",
            extra={
                'event': 'account_locked',
                'username': username,
                'ip_address': ip_address
            }
        )
    
    def suspicious_activity(self, activity: str, details: Dict[str, Any] = None):
        """Log suspicious activities"""
        self.logger.error(
            f"Suspicious activity detected: {activity}",
            extra={
                'event': 'suspicious_activity',
                'activity': activity,
                'details': details or {}
            }
        )


# ================================
# LOG ANALYSIS HELPERS
# ================================

def get_recent_logs(log_file: str = 'app.log', lines: int = 100) -> List[Dict[str, Any]]:
    """Get recent log entries as parsed JSON objects"""
    
    log_path = settings.LOGS_DIR / log_file
    recent_logs = []
    
    try:
        with open(log_path, 'r') as f:
            # Read last N lines
            log_lines = f.readlines()
            recent_lines = log_lines[-lines:] if len(log_lines) > lines else log_lines
            
            # Parse JSON log entries
            for line in recent_lines:
                try:
                    log_entry = json.loads(line.strip())
                    recent_logs.append(log_entry)
                except json.JSONDecodeError:
                    # Skip non-JSON lines
                    continue
                    
    except FileNotFoundError:
        pass
    
    return recent_logs


def get_error_summary(hours: int = 24) -> Dict[str, int]:
    """Get error summary for the last N hours"""
    
    from datetime import datetime, timedelta
    
    cutoff_time = datetime.utcnow() - timedelta(hours=hours)
    recent_logs = get_recent_logs('errors.log', lines=1000)
    
    error_summary = {}
    
    for log_entry in recent_logs:
        try:
            log_time = datetime.fromisoformat(log_entry.get('timestamp', '').replace('Z', '+00:00'))
            if log_time > cutoff_time:
                error_type = log_entry.get('error_type', 'Unknown')
                error_summary[error_type] = error_summary.get(error_type, 0) + 1
        except (ValueError, TypeError):
            continue
    
    return error_summary


# ================================
# SINGLETON INSTANCES
# ================================

# Create singleton instances for easy import
trading_logger = TradingLogger()
security_logger = SecurityLogger()

# ================================
# EXPORTS
# ================================

__all__ = [
    'setup_logging',
    'configure_structlog',
    'LoggingTimer',
    'log_performance',
    'TradingLogger',
    'SecurityLogger',
    'trading_logger',
    'security_logger',
    'get_recent_logs',
    'get_error_summary'
]




# ai-trading-bot/api/alembic.ini
# Alembic Configuration File

[alembic]
# Path to migration scripts
script_location = alembic

# Template used to generate migration file names
file_template = %%(year)d%%(month).2d%%(day).2d_%%(hour).2d%%(minute).2d_%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
prepend_sys_path = .

# Timezone to use when rendering the date within the migration file
# as well as the filename.
timezone = UTC

# Max length of characters to apply to the "slug" field
truncate_slug_length = 40

# Set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
revision_environment = false

# Set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
sourceless = false

# Version location specification
version_locations = %(here)s/alembic/versions

# Version path separator; As mentioned above, this is the character used to split
version_path_separator = :

# Set to 'true' to search source files recursively
recursive_version_locations = false

# The output encoding used when revision files
# are written from script.py.mako
output_encoding = utf-8

[post_write_hooks]
# Post-write hooks define scripts or Python functions that are run
# on newly generated revision scripts.
hooks = black,isort

black.type = console_scripts
black.entrypoint = black
black.options = REVISION_SCRIPT_FILENAME

isort.type = console_scripts
isort.entrypoint = isort
isort.options = REVISION_SCRIPT_FILENAME

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S

---
# ai-trading-bot/api/alembic/env.py
"""
Alembic Environment Configuration
Database migration environment setup with async SQLAlchemy support
"""

import asyncio
import os
from logging.config import fileConfig
from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config

from alembic import context

# Import your models here
from app.models.database import Base
from app.core.config import settings

# This is the Alembic Config object
config = context.config

# Interpret the config file for Python logging
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Add your model's MetaData object here for 'autogenerate' support
target_metadata = Base.metadata

def get_database_url():
    """Get database URL from environment"""
    return str(settings.DATABASE_URL).replace('postgresql://', 'postgresql+asyncpg://')

def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.
    """
    url = get_database_url()
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        compare_type=True,
        compare_server_default=True,
        include_schemas=True,
    )

    with context.begin_transaction():
        context.run_migrations()

def do_run_migrations(connection: Connection) -> None:
    """Run migrations with connection"""
    context.configure(
        connection=connection,
        target_metadata=target_metadata,
        compare_type=True,
        compare_server_default=True,
        include_schemas=True,
    )

    with context.begin_transaction():
        context.run_migrations()

async def run_async_migrations() -> None:
    """Run migrations in async mode"""
    configuration = config.get_section(config.config_ini_section)
    configuration["sqlalchemy.url"] = get_database_url()

    connectable = async_engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()

def run_migrations_online() -> None:
    """Run migrations in 'online' mode"""
    asyncio.run(run_async_migrations())

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

---
# ai-trading-bot/api/alembic/script.py.mako
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# Revision identifiers
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade database schema"""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade database schema"""
    ${downgrades if downgrades else "pass"}

---
# ai-trading-bot/api/alembic/versions/001_initial_migration.py
"""Initial database schema

Revision ID: 001
Revises: 
Create Date: 2024-01-01 00:00:00.000000

"""
from typing import Sequence, Union
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# Revision identifiers
revision: str = '001'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Create initial database schema"""
    
    # Enable TimescaleDB extension
    op.execute("CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE")
    
    # Create custom types
    op.execute("""
        CREATE TYPE asset_type_enum AS ENUM (
            'crypto', 'stock', 'etf', 'commodity', 'forex'
        )
    """)
    
    op.execute("""
        CREATE TYPE order_type_enum AS ENUM (
            'buy', 'sell', 'stop_loss', 'take_profit', 'market', 'limit'
        )
    """)
    
    op.execute("""
        CREATE TYPE order_status_enum AS ENUM (
            'pending', 'executed', 'cancelled', 'failed', 'partial'
        )
    """)
    
    op.execute("""
        CREATE TYPE position_status_enum AS ENUM (
            'open', 'closed', 'partial'
        )
    """)
    
    op.execute("""
        CREATE TYPE analysis_type_enum AS ENUM (
            'fundamental', 'technical', 'sentiment', 'consensus'
        )
    """)
    
    op.execute("""
        CREATE TYPE recommendation_enum AS ENUM (
            'BUY', 'SELL', 'HOLD'
        )
    """)
    
    op.execute("""
        CREATE TYPE alert_type_enum AS ENUM (
            'drawdown', 'concentration', 'volatility', 'stop_loss', 'margin_call', 'position_size'
        )
    """)
    
    op.execute("""
        CREATE TYPE alert_severity_enum AS ENUM (
            'low', 'medium', 'high', 'critical'
        )
    """)
    
    # Users table
    op.create_table('users',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        sa.Column('updated_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        
        # Basic user info
        sa.Column('username', sa.String(50), nullable=False, unique=True),
        sa.Column('email', sa.String(255), nullable=False, unique=True),
        sa.Column('password_hash', sa.String(255), nullable=False),
        
        # Account status
        sa.Column('is_active', sa.Boolean(), nullable=False, server_default='true'),
        sa.Column('is_verified', sa.Boolean(), nullable=False, server_default='false'),
        sa.Column('is_superuser', sa.Boolean(), nullable=False, server_default='false'),
        
        # Authentication tracking
        sa.Column('last_login', sa.DateTime(), nullable=True),
        sa.Column('failed_login_attempts', sa.Integer(), nullable=False, server_default='0'),
        sa.Column('locked_until', sa.DateTime(), nullable=True),
        
        # Trading configuration
        sa.Column('risk_tolerance', sa.Numeric(3, 2), nullable=False, server_default='0.05'),
        sa.Column('max_portfolio_risk', sa.Numeric(3, 2), nullable=False, server_default='0.15'),
        sa.Column('auto_trading_enabled', sa.Boolean(), nullable=False, server_default='false'),
        sa.Column('paper_trading_mode', sa.Boolean(), nullable=False, server_default='true'),
        
        # Austrian tax settings
        sa.Column('tax_residence', sa.String(2), nullable=False, server_default='AT'),
        sa.Column('tax_id', sa.String(50), nullable=True),
        
        # Preferences
        sa.Column('preferred_currency', sa.String(3), nullable=False, server_default='EUR'),
        sa.Column('timezone', sa.String(50), nullable=False, server_default='Europe/Vienna'),
    )
    
    # Assets table
    op.create_table('assets',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        sa.Column('updated_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        
        # Asset information
        sa.Column('symbol', sa.String(20), nullable=False, unique=True),
        sa.Column('name', sa.String(100), nullable=False),
        sa.Column('asset_type', sa.Enum(name='asset_type_enum'), nullable=False),
        sa.Column('exchange', sa.String(50), nullable=True),
        
        # Asset status
        sa.Column('is_active', sa.Boolean(), nullable=False, server_default='true'),
        sa.Column('is_tradeable', sa.Boolean(), nullable=False, server_default='true'),
        
        # Asset metadata
        sa.Column('sector', sa.String(50), nullable=True),
        sa.Column('market_cap', sa.BigInteger(), nullable=True),
        sa.Column('description', sa.String(500), nullable=True),
        
        # External identifiers
        sa.Column('external_id', sa.String(100), nullable=True),
        sa.Column('isin', sa.String(12), nullable=True),
    )
    
    # Portfolios table
    op.create_table('portfolios',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        sa.Column('updated_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        
        # Portfolio ownership
        sa.Column('user_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('users.id', ondelete='CASCADE'), nullable=False),
        sa.Column('name', sa.String(100), nullable=False, server_default='Main Portfolio'),
        
        # Portfolio balances
        sa.Column('initial_balance', sa.Numeric(15, 2), nullable=False),
        sa.Column('current_balance', sa.Numeric(15, 2), nullable=False),
        sa.Column('total_invested', sa.Numeric(15, 2), nullable=False, server_default='0'),
        sa.Column('total_profit_loss', sa.Numeric(15, 2), nullable=False, server_default='0'),
        
        # Portfolio settings
        sa.Column('currency', sa.String(3), nullable=False, server_default='EUR'),
        sa.Column('is_active', sa.Boolean(), nullable=False, server_default='true'),
        
        sa.UniqueConstraint('user_id', 'name', name='uq_user_portfolio_name'),
    )
    
    # Positions table
    op.create_table('positions',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        sa.Column('updated_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        
        # Position ownership
        sa.Column('portfolio_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('portfolios.id', ondelete='CASCADE'), nullable=False),
        sa.Column('asset_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('assets.id'), nullable=False),
        
        # Position details
        sa.Column('quantity', sa.Numeric(20, 8), nullable=False),
        sa.Column('avg_buy_price', sa.Numeric(15, 8), nullable=False),
        sa.Column('current_price', sa.Numeric(15, 8), nullable=True),
        
        # P&L calculation
        sa.Column('unrealized_pnl', sa.Numeric(15, 2), nullable=False, server_default='0'),
        sa.Column('realized_pnl', sa.Numeric(15, 2), nullable=False, server_default='0'),
        
        # Position management
        sa.Column('status', sa.Enum(name='position_status_enum'), nullable=False, server_default='open'),
        sa.Column('opened_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        sa.Column('closed_at', sa.DateTime(), nullable=True),
        
        # Risk management
        sa.Column('stop_loss_price', sa.Numeric(15, 8), nullable=True),
        sa.Column('take_profit_price', sa.Numeric(15, 8), nullable=True),
    )
    
    # Orders table
    op.create_table('orders',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        sa.Column('updated_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        
        # Order ownership
        sa.Column('portfolio_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('portfolios.id', ondelete='CASCADE'), nullable=False),
        sa.Column('position_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('positions.id'), nullable=True),
        sa.Column('asset_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('assets.id'), nullable=False),
        
        # Order details
        sa.Column('order_type', sa.Enum(name='order_type_enum'), nullable=False),
        sa.Column('quantity', sa.Numeric(20, 8), nullable=False),
        sa.Column('price', sa.Numeric(15, 8), nullable=True),
        sa.Column('stop_price', sa.Numeric(15, 8), nullable=True),
        
        # Order execution
        sa.Column('status', sa.Enum(name='order_status_enum'), nullable=False, server_default='pending'),
        sa.Column('executed_quantity', sa.Numeric(20, 8), nullable=False, server_default='0'),
        sa.Column('executed_price', sa.Numeric(15, 8), nullable=True),
        
        # Timestamps
        sa.Column('executed_at', sa.DateTime(), nullable=True),
        sa.Column('cancelled_at', sa.DateTime(), nullable=True),
        sa.Column('expires_at', sa.DateTime(), nullable=True),
        
        # External reference
        sa.Column('external_order_id', sa.String(100), nullable=True),
        
        # Fees and costs
        sa.Column('fee_amount', sa.Numeric(15, 8), nullable=False, server_default='0'),
        sa.Column('fee_currency', sa.String(10), nullable=True),
        
        # Order metadata
        sa.Column('notes', sa.String(500), nullable=True),
        sa.Column('source', sa.String(50), nullable=False, server_default='manual'),
    )
    
    # AI Analyses table
    op.create_table('ai_analyses',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        sa.Column('updated_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        
        # Analysis target
        sa.Column('asset_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('assets.id'), nullable=False),
        
        # Analysis details
        sa.Column('analysis_type', sa.Enum(name='analysis_type_enum'), nullable=False),
        sa.Column('ai_model', sa.String(50), nullable=False),
        
        # Analysis results
        sa.Column('recommendation', sa.Enum(name='recommendation_enum'), nullable=False),
        sa.Column('confidence_score', sa.Numeric(5, 4), nullable=False),
        sa.Column('target_price', sa.Numeric(15, 8), nullable=True),
        sa.Column('reasoning', sa.Text(), nullable=True),
        
        # Key indicators (JSON)
        sa.Column('indicators', postgresql.JSONB(), nullable=True),
        
        # Analysis metadata
        sa.Column('expires_at', sa.DateTime(), nullable=True),
        sa.Column('market_conditions', sa.String(100), nullable=True),
        
        # Performance tracking
        sa.Column('prediction_accuracy', sa.Numeric(5, 4), nullable=True),
    )
    
    # Risk Alerts table
    op.create_table('risk_alerts',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        sa.Column('updated_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        
        # Alert ownership
        sa.Column('user_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('users.id', ondelete='CASCADE'), nullable=False),
        sa.Column('portfolio_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('portfolios.id', ondelete='CASCADE'), nullable=True),
        
        # Alert details
        sa.Column('alert_type', sa.Enum(name='alert_type_enum'), nullable=False),
        sa.Column('severity', sa.Enum(name='alert_severity_enum'), nullable=False),
        sa.Column('message', sa.Text(), nullable=False),
        
        # Alert data
        sa.Column('current_value', sa.Numeric(15, 8), nullable=True),
        sa.Column('threshold_value', sa.Numeric(15, 8), nullable=True),
        
        # Alert status
        sa.Column('is_active', sa.Boolean(), nullable=False, server_default='true'),
        sa.Column('acknowledged_at', sa.DateTime(), nullable=True),
        sa.Column('resolved_at', sa.DateTime(), nullable=True),
    )
    
    # System Config table
    op.create_table('system_config',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('created_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        sa.Column('updated_at', sa.DateTime(), nullable=False, server_default=sa.text('now()')),
        
        # Configuration
        sa.Column('key', sa.String(100), nullable=False, unique=True),
        sa.Column('value', sa.Text(), nullable=False),
        sa.Column('description', sa.Text(), nullable=True),
        
        # Metadata
        sa.Column('updated_by', postgresql.UUID(as_uuid=True), sa.ForeignKey('users.id'), nullable=True),
    )
    
    # ================================
    # TIME-SERIES TABLES (TimescaleDB)
    # ================================
    
    # Market Data table
    op.create_table('market_data',
        sa.Column('time', sa.DateTime(), nullable=False),
        sa.Column('asset_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('assets.id'), nullable=False),
        sa.Column('timeframe', sa.String(10), nullable=False),
        
        # OHLCV data
        sa.Column('open_price', sa.Numeric(15, 8), nullable=False),
        sa.Column('high_price', sa.Numeric(15, 8), nullable=False),
        sa.Column('low_price', sa.Numeric(15, 8), nullable=False),
        sa.Column('close_price', sa.Numeric(15, 8), nullable=False),
        sa.Column('volume', sa.Numeric(20, 8), nullable=False),
        
        # Additional metrics
        sa.Column('volume_quote





      # ai-trading-bot/scripts/seed_data.py
"""
Database Seeding Script
Initialize database with test data for development
"""

import asyncio
import logging
from datetime import datetime, timedelta
from decimal import Decimal

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import get_session_context
from app.core.auth import password_manager
from app.models.database import (
    User, Portfolio, Asset, Position, Order, AIAnalysis, SystemConfig,
    AssetType, OrderType, OrderStatus, PositionStatus, AnalysisType, Recommendation
)

logger = logging.getLogger(__name__)

async def seed_users():
    """Seed test users"""
    print("🌱 Seeding users...")
    
    async with get_session_context() as db:
        # Check if admin user exists
        admin_query = select(User).where(User.username == "admin")
        admin_result = await db.execute(admin_query)
        admin_user = admin_result.scalar_one_or_none()
        
        if not admin_user:
            # Create admin user
            admin_user = User(
                username="admin",
                email="admin@trading-bot.com",
                password_hash=password_manager.hash_password("admin123!"),
                is_superuser=True,
                is_verified=True,
                is_active=True,
                preferred_currency="EUR",
                timezone="Europe/Vienna"
            )
            db.add(admin_user)
            print("✅ Created admin user (admin/admin123!)")
        
        # Create test user
        test_query = select(User).where(User.username == "testuser")
        test_result = await db.execute(test_query)
        test_user = test_result.scalar_one_or_none()
        
        if not test_user:
            test_user = User(
                username="testuser",
                email="test@trading-bot.com",
                password_hash=password_manager.hash_password("test123!"),
                is_superuser=False,
                is_verified=True,
                is_active=True,
                preferred_currency="EUR",
                timezone="Europe/Vienna",
                paper_trading_mode=True
            )
            db.add(test_user)
            print("✅ Created test user (testuser/test123!)")
        
        await db.commit()
        return admin_user, test_user

async def seed_assets():
    """Seed tradeable assets"""
    print("🌱 Seeding assets...")
    
    assets_data = [
        # Cryptocurrencies
        {"symbol": "BTC", "name": "Bitcoin", "asset_type": AssetType.CRYPTO, "exchange": "bitpanda"},
        {"symbol": "ETH", "name": "Ethereum", "asset_type": AssetType.CRYPTO, "exchange": "bitpanda"},
        {"symbol": "ADA", "name": "Cardano", "asset_type": AssetType.CRYPTO, "exchange": "bitpanda"},
        {"symbol": "DOT", "name": "Polkadot", "asset_type": AssetType.CRYPTO, "exchange": "bitpanda"},
        {"symbol": "SOL", "name": "Solana", "asset_type": AssetType.CRYPTO, "exchange": "bitpanda"},
        
        # Stocks
        {"symbol": "AAPL", "name": "Apple Inc.", "asset_type": AssetType.STOCK, "exchange": "nasdaq", "sector": "Technology"},
        {"symbol": "GOOGL", "name": "Alphabet Inc.", "asset_type": AssetType.STOCK, "exchange": "nasdaq", "sector": "Technology"},
        {"symbol": "MSFT", "name": "Microsoft Corporation", "asset_type": AssetType.STOCK, "exchange": "nasdaq", "sector": "Technology"},
        {"symbol": "TSLA", "name": "Tesla Inc.", "asset_type": AssetType.STOCK, "exchange": "nasdaq", "sector": "Automotive"},
        {"symbol": "NVDA", "name": "NVIDIA Corporation", "asset_type": AssetType.STOCK, "exchange": "nasdaq", "sector": "Technology"},
        
        # ETFs
        {"symbol": "SPY", "name": "SPDR S&P 500 ETF", "asset_type": AssetType.ETF, "exchange": "nyse"},
        {"symbol": "QQQ", "name": "Invesco QQQ Trust", "asset_type": AssetType.ETF, "exchange": "nasdaq"},
        {"symbol": "VTI", "name": "Vanguard Total Stock Market ETF", "asset_type": AssetType.ETF, "exchange": "nyse"},
        
        # Commodities
        {"symbol": "GOLD", "name": "Gold", "asset_type": AssetType.COMMODITY, "exchange": "lbma"},
        {"symbol": "SILVER", "name": "Silver", "asset_type": AssetType.COMMODITY, "exchange": "lbma"},
    ]
    
    async with get_session_context() as db:
        created_assets = []
        for asset_data in assets_data:
            # Check if asset exists
            existing_query = select(Asset).where(Asset.symbol == asset_data["symbol"])
            existing_result = await db.execute(existing_query)
            if not existing_result.scalar_one_or_none():
                asset = Asset(**asset_data)
                db.add(asset)
                created_assets.append(asset_data["symbol"])
        
        await db.commit()
        print(f"✅ Created {len(created_assets)} assets: {', '.join(created_assets)}")

async def seed_portfolios_and_positions():
    """Seed test portfolios and positions"""
    print("🌱 Seeding portfolios and positions...")
    
    async with get_session_context() as db:
        # Get test user
        user_query = select(User).where(User.username == "testuser")
        user_result = await db.execute(user_query)
        test_user = user_result.scalar_one_or_none()
        
        if not test_user:
            print("❌ Test user not found")
            return
        
        # Check if portfolio exists
        portfolio_query = select(Portfolio).where(
            Portfolio.user_id == test_user.id,
            Portfolio.name == "Main Portfolio"
        )
        portfolio_result = await db.execute(portfolio_query)
        portfolio = portfolio_result.scalar_one_or_none()
        
        if not portfolio:
            # Create main portfolio
            portfolio = Portfolio(
                user_id=test_user.id,
                name="Main Portfolio",
                initial_balance=Decimal('10000.00'),
                current_balance=Decimal('8500.00'),
                total_invested=Decimal('1500.00'),
                currency="EUR"
            )
            db.add(portfolio)
            await db.commit()
            await db.refresh(portfolio)
            print("✅ Created main portfolio with 10,000 EUR")
        
        # Get some assets for positions
        assets_query = select(Asset).where(Asset.symbol.in_(["BTC", "ETH", "AAPL", "GOOGL"]))
        assets_result = await db.execute(assets_query)
        assets = assets_result.scalars().all()
        
        # Create sample positions
        sample_positions = [
            {"asset": "BTC", "quantity": Decimal('0.05'), "avg_price": Decimal('45000.00')},
            {"asset": "ETH", "quantity": Decimal('0.5'), "avg_price": Decimal('3000.00')},
            {"asset": "AAPL", "quantity": Decimal('10'), "avg_price": Decimal('150.00')},
        ]
        
        created_positions = []
        for pos_data in sample_positions:
            asset = next((a for a in assets if a.symbol == pos_data["asset"]), None)
            if asset:
                # Check if position exists
                existing_pos_query = select(Position).where(
                    Position.portfolio_id == portfolio.id,
                    Position.asset_id == asset.id,
                    Position.status == PositionStatus.OPEN
                )
                existing_pos_result = await db.execute(existing_pos_query)
                if not existing_pos_result.scalar_one_or_none():
                    current_price = pos_data["avg_price"] * Decimal('1.05')  # 5% gain
                    position = Position(
                        portfolio_id=portfolio.id,
                        asset_id=asset.id,
                        quantity=pos_data["quantity"],
                        avg_buy_price=pos_data["avg_price"],
                        current_price=current_price,
                        unrealized_pnl=(current_price - pos_data["avg_price"]) * pos_data["quantity"],
                        status=PositionStatus.OPEN,
                        opened_at=datetime.utcnow() - timedelta(days=7)
                    )
                    db.add(position)
                    created_positions.append(pos_data["asset"])
        
        await db.commit()
        print(f"✅ Created {len(created_positions)} positions: {', '.join(created_positions)}")

async def seed_system_config():
    """Seed system configuration"""
    print("🌱 Seeding system configuration...")
    
    config_data = [
        {"key": "max_daily_trades", "value": "100", "description": "Maximum number of trades per day per user"},
        {"key": "min_trade_amount", "value": "10.00", "description": "Minimum trade amount in EUR"},
        {"key": "max_position_size", "value": "0.20", "description": "Maximum position size as % of portfolio"},
        {"key": "stop_loss_default", "value": "0.05", "description": "Default stop loss percentage"},
        {"key": "take_profit_default", "value": "0.15", "description": "Default take profit percentage"},
        {"key": "ai_analysis_enabled", "value": "true", "description": "Enable AI analysis features"},
        {"key": "risk_alerts_enabled", "value": "true", "description": "Enable risk management alerts"},
        {"key": "paper_trading_default", "value": "true", "description": "Enable paper trading mode by default"},
    ]
    
    async with get_session_context() as db:
        created_configs = []
        for config in config_data:
            # Check if config exists
            existing_query = select(SystemConfig).where(SystemConfig.key == config["key"])
            existing_result = await db.execute(existing_query)
            if not existing_result.scalar_one_or_none():
                system_config = SystemConfig(**config)
                db.add(system_config)
                created_configs.append(config["key"])
        
        await db.commit()
        print(f"✅ Created {len(created_configs)} system configs")

async def seed_sample_orders():
    """Seed sample orders for demonstration"""
    print("🌱 Seeding sample orders...")
    
    async with get_session_context() as db:
        # Get test user's portfolio
        user_query = select(User).where(User.username == "testuser")
        user_result = await db.execute(user_query)
        test_user = user_result.scalar_one_or_none()
        
        if not test_user:
            return
        
        portfolio_query = select(Portfolio).where(Portfolio.user_id == test_user.id)
        portfolio_result = await db.execute(portfolio_query)
        portfolio = portfolio_result.scalar_one_or_none()
        
        if not portfolio:
            return
        
        # Get ETH asset for sample order
        eth_query = select(Asset).where(Asset.symbol == "ETH")
        eth_result = await db.execute(eth_query)
        eth_asset = eth_result.scalar_one_or_none()
        
        if eth_asset:
            # Create a pending buy order
            pending_order = Order(
                portfolio_id=portfolio.id,
                asset_id=eth_asset.id,
                order_type=OrderType.BUY,
                quantity=Decimal('0.1'),
                price=Decimal('2800.00'),
                status=OrderStatus.PENDING,
                source="manual"
            )
            db.add(pending_order)
            
            # Create an executed sell order
            executed_order = Order(
                portfolio_id=portfolio.id,
                asset_id=eth_asset.id,
                order_type=OrderType.SELL,
                quantity=Decimal('0.05'),
                price=Decimal('3100.00'),
                status=OrderStatus.EXECUTED,
                executed_quantity=Decimal('0.05'),
                executed_price=Decimal('3100.00'),
                executed_at=datetime.utcnow() - timedelta(hours=2),
                source="manual"
            )
            db.add(executed_order)
            
            await db.commit()
            print("✅ Created sample orders (1 pending, 1 executed)")

async def main():
    """Main seeding function"""
    print("🚀 Starting database seeding...")
    
    try:
        await seed_users()
        await seed_assets()
        await seed_portfolios_and_positions()
        await seed_system_config()
        await seed_sample_orders()
        
        print("\n🎉 Database seeding completed successfully!")
        print("\n📋 Test Accounts Created:")
        print("   Admin: admin / admin123!")
        print("   User:  testuser / test123!")
        print("\n🔗 Access the API at: http://localhost:8000/docs")
        
    except Exception as e:
        print(f"❌ Seeding failed: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())

---
# ai-trading-bot/scripts/start_dev.py
"""
Development Startup Script
Complete development environment setup and validation
"""

import asyncio
import subprocess
import sys
import time
import logging
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent / "api"
sys.path.insert(0, str(project_root))

from app.core.database import check_database_health
from app.core.config import settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def run_command(command: str, cwd: Path = None) -> tuple[int, str, str]:
    """Run shell command and return result"""
    print(f"🔧 Running: {command}")
    
    try:
        result = subprocess.run(
            command.split(),
            cwd=cwd,
            capture_output=True,
            text=True,
            timeout=60
        )
        return result.returncode, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return 1, "", "Command timed out"
    except Exception as e:
        return 1, "", str(e)

def check_docker():
    """Check if Docker is running"""
    print("🐳 Checking Docker...")
    
    code, stdout, stderr = run_command("docker --version")
    if code != 0:
        print("❌ Docker is not installed or not running")
        return False
    
    print(f"✅ Docker: {stdout.strip()}")
    
    # Check if Docker daemon is running
    code, stdout, stderr = run_command("docker ps")
    if code != 0:
        print("❌ Docker daemon is not running")
        return False
    
    return True

def check_docker_compose():
    """Check if Docker Compose is available"""
    print("🔧 Checking Docker Compose...")
    
    code, stdout, stderr = run_command("docker-compose --version")
    if code != 0:
        print("❌ Docker Compose is not installed")
        return False
    
    print(f"✅ Docker Compose: {stdout.strip()}")
    return True

def start_services():
    """Start all Docker services"""
    print("🚀 Starting Docker services...")
    
    project_root = Path(__file__).parent.parent
    
    # Stop any existing services
    run_command("docker-compose down", cwd=project_root)
    
    # Start services
    code, stdout, stderr = run_command("docker-compose up -d --build", cwd=project_root)
    
    if code != 0:
        print(f"❌ Failed to start services: {stderr}")
        return False
    
    print("✅ Services started successfully")
    return True

def wait_for_services():
    """Wait for services to be ready"""
    print("⏳ Waiting for services to be ready...")
    
    max_retries = 30
    retry_count = 0
    
    while retry_count < max_retries:
        try:
            # Check API health
            import requests
            response = requests.get("http://localhost:8000/health/ping", timeout=5)
            if response.status_code == 200:
                print("✅ API is ready")
                return True
        except Exception:
            pass
        
        retry_count += 1
        print(f"⏳ Waiting... ({retry_count}/{max_retries})")
        time.sleep(2)
    
    print("❌ Services did not start within timeout")
    return False

async def run_migrations():
    """Run database migrations"""
    print("🗄️ Running database migrations...")
    
    project_root = Path(__file__).parent.parent
    
    code, stdout, stderr = run_command(
        "docker-compose exec -T api alembic upgrade head",
        cwd=project_root
    )
    
    if code != 0:
        print(f"❌ Migration failed: {stderr}")
        return False
    
    print("✅ Database migrations completed")
    return True

async def seed_database():
    """Seed database with test data"""
    print("🌱 Seeding database...")
    
    project_root = Path(__file__).parent.parent
    
    code, stdout, stderr = run_command(
        "docker-compose exec -T api python scripts/seed_data.py",
        cwd=project_root
    )
    
    if code != 0:
        print(f"❌ Database seeding failed: {stderr}")
        return False
    
    print("✅ Database seeded successfully")
    return True

def show_endpoints():
    """Show available endpoints"""
    print("\n🔗 Available Endpoints:")
    print("   📚 API Documentation: http://localhost:8000/docs")
    print("   🏥 Health Check:      http://localhost:8000/health")
    print("   📊 Metrics:          http://localhost:8000/health/metrics")
    print("   🔍 API Root:         http://localhost:8000/")

def show_test_accounts():
    """Show test accounts"""
    print("\n👤 Test Accounts:")
    print("   Admin: admin / admin123!")
    print("   User:  testuser / test123!")

async def run_basic_tests():
    """Run basic API tests"""
    print("🧪 Running basic tests...")
    
    try:
        import requests
        
        # Test health endpoint
        response = requests.get("http://localhost:8000/health/ping")
        assert response.status_code == 200
        print("✅ Health check passed")
        
        # Test API info
        response = requests.get("http://localhost:8000/info")
        assert response.status_code == 200
        print("✅ API info endpoint passed")
        
        # Test login
        login_data = {
            "username_or_email": "testuser",
            "password": "test123!"
        }
        response = requests.post("http://localhost:8000/auth/login", json=login_data)
        assert response.status_code == 200
        print("✅ User login test passed")
        
        return True
        
    except Exception as e:
        print(f"❌ Tests failed: {e}")
        return False

async def main():
    """Main startup function"""
    print("🚀 AI Trading Bot - Development Environment Setup")
    print("=" * 50)
    
    # Check prerequisites
    if not check_docker():
        sys.exit(1)
    
    if not check_docker_compose():
        sys.exit(1)
    
    # Start services
    if not start_services():
        sys.exit(1)
    
    # Wait for services
    if not wait_for_services():
        print("❌ Showing service logs:")
        run_command("docker-compose logs api")
        sys.exit(1)
    
    # Run migrations
    if not await run_migrations():
        sys.exit(1)
    
    # Seed database
    if not await seed_database():
        sys.exit(1)
    
    # Run tests
    if not await run_basic_tests():
        print("⚠️ Some tests failed, but services are running")
    
    # Show information
    show_endpoints()
    show_test_accounts()
    
    print("\n🎉 Development environment is ready!")
    print("📋 Use 'docker-compose logs -f api' to view live logs")
    print("🛑 Use 'docker-compose down' to stop services")

if __name__ == "__main__":
    asyncio.run(main())

---
# ai-trading-bot/scripts/test_api.py
"""
API Testing Script
Comprehensive API endpoint testing
"""

import asyncio
import json
import sys
from pathlib import Path

import httpx

# Test configuration
BASE_URL = "http://localhost:8000"
TEST_USER = {"username": "testuser", "password": "test123!"}
ADMIN_USER = {"username": "admin", "password": "admin123!"}

class APITester:
    def __init__(self):
        self.client = httpx.AsyncClient(base_url=BASE_URL, timeout=30.0)
        self.user_token = None
        self.admin_token = None
        self.test_results = []
    
    async def __aenter__(self):
        return self
    



  



